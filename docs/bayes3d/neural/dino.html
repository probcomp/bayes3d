<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>bayes3d.neural.dino API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>bayes3d.neural.dino</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import argparse
import torch
import torchvision.transforms
from torch import nn
from torchvision import transforms
import torch.nn.modules.utils as nn_utils
import math
import timm
import types
from pathlib import Path
from typing import Union, List, Tuple
from PIL import Image
import numpy as np


class ViTExtractor:
    &#34;&#34;&#34; This class facilitates extraction of features, descriptors, and saliency maps from a ViT.

    We use the following notation in the documentation of the module&#39;s methods:
    B - batch size
    h - number of heads. usually takes place of the channel dimension in pytorch&#39;s convention BxCxHxW
    p - patch size of the ViT. either 8 or 16.
    t - number of tokens. equals the number of patches + 1, e.g. HW / p**2 + 1. Where H and W are the height and width
    of the input image.
    d - the embedding dimension in the ViT.
    &#34;&#34;&#34;

    def __init__(self, model_type: str = &#39;dino_vits8&#39;, stride: int = 4, model: nn.Module = None, device: str = &#39;cuda&#39;):
        &#34;&#34;&#34;
        :param model_type: A string specifying the type of model to extract from.
                          [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 |
                          vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]
        :param stride: stride of first convolution layer. small stride -&gt; higher resolution.
        :param model: Optional parameter. The nn.Module to extract from instead of creating a new one in ViTExtractor.
                      should be compatible with model_type.
        &#34;&#34;&#34;
        self.model_type = model_type
        self.device = device
        if model is not None:
            self.model = model
        else:
            self.model = ViTExtractor.create_model(model_type)

        self.model = ViTExtractor.patch_vit_resolution(self.model, stride=stride)
        self.model.eval()
        self.model.to(self.device)
        self.p = self.model.patch_embed.patch_size
        self.stride = self.model.patch_embed.proj.stride

        self.mean = (0.485, 0.456, 0.406) if &#34;dino&#34; in self.model_type else (0.5, 0.5, 0.5)
        self.std = (0.229, 0.224, 0.225) if &#34;dino&#34; in self.model_type else (0.5, 0.5, 0.5)

        self._feats = []
        self.hook_handlers = []
        self.load_size = None
        self.num_patches = None

    @staticmethod
    def create_model(model_type: str) -&gt; nn.Module:
        &#34;&#34;&#34;
        :param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |
                           dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |
                           vit_base_patch16_224]
        :return: the model
        &#34;&#34;&#34;
        if &#39;dino&#39; in model_type:
            model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type)
        else:  # model from timm -- load weights from timm to dino model (enables working on arbitrary size images).
            temp_model = timm.create_model(model_type, pretrained=True)
            model_type_dict = {
                &#39;vit_small_patch16_224&#39;: &#39;dino_vits16&#39;,
                &#39;vit_small_patch8_224&#39;: &#39;dino_vits8&#39;,
                &#39;vit_base_patch16_224&#39;: &#39;dino_vitb16&#39;,
                &#39;vit_base_patch8_224&#39;: &#39;dino_vitb8&#39;
            }
            model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type_dict[model_type])
            temp_state_dict = temp_model.state_dict()
            del temp_state_dict[&#39;head.weight&#39;]
            del temp_state_dict[&#39;head.bias&#39;]
            model.load_state_dict(temp_state_dict)
        return model

    @staticmethod
    def _fix_pos_enc(patch_size: int, stride_hw: Tuple[int, int]):
        &#34;&#34;&#34;
        Creates a method for position encoding interpolation.
        :param patch_size: patch size of the model.
        :param stride_hw: A tuple containing the new height and width stride respectively.
        :return: the interpolation method
        &#34;&#34;&#34;
        def interpolate_pos_encoding(self, x: torch.Tensor, w: int, h: int) -&gt; torch.Tensor:
            npatch = x.shape[1] - 1
            N = self.pos_embed.shape[1] - 1
            if npatch == N and w == h:
                return self.pos_embed
            class_pos_embed = self.pos_embed[:, 0]
            patch_pos_embed = self.pos_embed[:, 1:]
            dim = x.shape[-1]
            # compute number of tokens taking stride into account
            w0 = 1 + (w - patch_size) // stride_hw[1]
            h0 = 1 + (h - patch_size) // stride_hw[0]
            assert (w0 * h0 == npatch), f&#34;&#34;&#34;got wrong grid size for {h}x{w} with patch_size {patch_size} and 
                                            stride {stride_hw} got {h0}x{w0}={h0 * w0} expecting {npatch}&#34;&#34;&#34;
            # we add a small number to avoid floating point error in the interpolation
            # see discussion at https://github.com/facebookresearch/dino/issues/8
            w0, h0 = w0 + 0.1, h0 + 0.1
            patch_pos_embed = nn.functional.interpolate(
                patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
                scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
                mode=&#39;bicubic&#39;,
                align_corners=False, recompute_scale_factor=False
            )
            assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
            patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
            return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

        return interpolate_pos_encoding

    @staticmethod
    def patch_vit_resolution(model: nn.Module, stride: int) -&gt; nn.Module:
        &#34;&#34;&#34;
        change resolution of model output by changing the stride of the patch extraction.
        :param model: the model to change resolution for.
        :param stride: the new stride parameter.
        :return: the adjusted model
        &#34;&#34;&#34;
        patch_size = model.patch_embed.patch_size
        if stride == patch_size:  # nothing to do
            return model

        stride = nn_utils._pair(stride)
        assert all([(patch_size // s_) * s_ == patch_size for s_ in
                    stride]), f&#39;stride {stride} should divide patch_size {patch_size}&#39;

        # fix the stride
        model.patch_embed.proj.stride = stride
        # fix the positional encoding code
        model.interpolate_pos_encoding = types.MethodType(ViTExtractor._fix_pos_enc(patch_size, stride), model)
        return model

    def preprocess(self, image_path: Union[str, Path],
                   load_size: Union[int, Tuple[int, int]] = None) -&gt; Tuple[torch.Tensor, Image.Image]:
        &#34;&#34;&#34;
        Preprocesses an image before extraction.
        :param image_path: path to image to be extracted.
        :param load_size: optional. Size to resize image before the rest of preprocessing.
        :return: a tuple containing:
                    (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.
                    (2) the pil image in relevant dimensions
        &#34;&#34;&#34;
        pil_image = Image.open(image_path).convert(&#39;RGB&#39;)
        if load_size is not None:
            pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.LANCZOS)(pil_image)
        prep = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std)
        ])
        prep_img = prep(pil_image)[None, ...]
        return prep_img, pil_image

    def _get_hook(self, facet: str):
        &#34;&#34;&#34;
        generate a hook method for a specific block and facet.
        &#34;&#34;&#34;
        if facet in [&#39;attn&#39;, &#39;token&#39;]:
            def _hook(model, input, output):
                self._feats.append(output)
            return _hook

        if facet == &#39;query&#39;:
            facet_idx = 0
        elif facet == &#39;key&#39;:
            facet_idx = 1
        elif facet == &#39;value&#39;:
            facet_idx = 2
        else:
            raise TypeError(f&#34;{facet} is not a supported facet.&#34;)

        def _inner_hook(module, input, output):
            input = input[0]
            B, N, C = input.shape
            qkv = module.qkv(input).reshape(B, N, 3, module.num_heads, C // module.num_heads).permute(2, 0, 3, 1, 4)
            self._feats.append(qkv[facet_idx]) #Bxhxtxd
        return _inner_hook

    def _register_hooks(self, layers: List[int], facet: str) -&gt; None:
        &#34;&#34;&#34;
        register hook to extract features.
        :param layers: layers from which to extract features.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39; | &#39;attn&#39;]
        &#34;&#34;&#34;
        for block_idx, block in enumerate(self.model.blocks):
            if block_idx in layers:
                if facet == &#39;token&#39;:
                    self.hook_handlers.append(block.register_forward_hook(self._get_hook(facet)))
                elif facet == &#39;attn&#39;:
                    self.hook_handlers.append(block.attn.attn_drop.register_forward_hook(self._get_hook(facet)))
                elif facet in [&#39;key&#39;, &#39;query&#39;, &#39;value&#39;]:
                    self.hook_handlers.append(block.attn.register_forward_hook(self._get_hook(facet)))
                else:
                    raise TypeError(f&#34;{facet} is not a supported facet.&#34;)

    def _unregister_hooks(self) -&gt; None:
        &#34;&#34;&#34;
        unregisters the hooks. should be called after feature extraction.
        &#34;&#34;&#34;
        for handle in self.hook_handlers:
            handle.remove()
        self.hook_handlers = []

    def _extract_features(self, batch: torch.Tensor, layers: List[int] = 11, facet: str = &#39;key&#39;) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;
        extract features from the model
        :param batch: batch to extract features for. Has shape BxCxHxW.
        :param layers: layer to extract. A number between 0 to 11.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39; | &#39;attn&#39;]
        :return : tensor of features.
                  if facet is &#39;key&#39; | &#39;query&#39; | &#39;value&#39; has shape Bxhxtxd
                  if facet is &#39;attn&#39; has shape Bxhxtxt
                  if facet is &#39;token&#39; has shape Bxtxd
        &#34;&#34;&#34;
        B, C, H, W = batch.shape
        self._feats = []
        self._register_hooks(layers, facet)
        _ = self.model(batch)
        self._unregister_hooks()
        self.load_size = (H, W)
        self.num_patches = (1 + (H - self.p) // self.stride[0], 1 + (W - self.p) // self.stride[1])
        return self._feats

    def _log_bin(self, x: torch.Tensor, hierarchy: int = 2) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        create a log-binned descriptor.
        :param x: tensor of features. Has shape Bxhxtxd.
        :param hierarchy: how many bin hierarchies to use.
        &#34;&#34;&#34;
        B = x.shape[0]
        num_bins = 1 + 8 * hierarchy

        bin_x = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1)  # Bx(t-1)x(dxh)
        bin_x = bin_x.permute(0, 2, 1)
        bin_x = bin_x.reshape(B, bin_x.shape[1], self.num_patches[0], self.num_patches[1])
        # Bx(dxh)xnum_patches[0]xnum_patches[1]
        sub_desc_dim = bin_x.shape[1]

        avg_pools = []
        # compute bins of all sizes for all spatial locations.
        for k in range(0, hierarchy):
            # avg pooling with kernel 3**kx3**k
            win_size = 3 ** k
            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)
            avg_pools.append(avg_pool(bin_x))

        bin_x = torch.zeros((B, sub_desc_dim * num_bins, self.num_patches[0], self.num_patches[1])).to(self.device)
        for y in range(self.num_patches[0]):
            for x in range(self.num_patches[1]):
                part_idx = 0
                # fill all bins for a spatial location (y, x)
                for k in range(0, hierarchy):
                    kernel_size = 3 ** k
                    for i in range(y - kernel_size, y + kernel_size + 1, kernel_size):
                        for j in range(x - kernel_size, x + kernel_size + 1, kernel_size):
                            if i == y and j == x and k != 0:
                                continue
                            if 0 &lt;= i &lt; self.num_patches[0] and 0 &lt;= j &lt; self.num_patches[1]:
                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][
                                                                                                           :, :, i, j]
                            else:  # handle padding in a more delicate way than zero padding
                                temp_i = max(0, min(i, self.num_patches[0] - 1))
                                temp_j = max(0, min(j, self.num_patches[1] - 1))
                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][
                                                                                                           :, :, temp_i,
                                                                                                           temp_j]
                            part_idx += 1
        bin_x = bin_x.flatten(start_dim=-2, end_dim=-1).permute(0, 2, 1).unsqueeze(dim=1)
        # Bx1x(t-1)x(dxh)
        return bin_x

    def extract_descriptors(self, batch: torch.Tensor, layer: int = 11, facet: str = &#39;key&#39;,
                            bin: bool = False, include_cls: bool = False) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        extract descriptors from the model
        :param batch: batch to extract descriptors for. Has shape BxCxHxW.
        :param layers: layer to extract. A number between 0 to 11.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;]
        :param bin: apply log binning to the descriptor. default is False.
        :return: tensor of descriptors. Bx1xtxd&#39; where d&#39; is the dimension of the descriptors.
        &#34;&#34;&#34;
        assert facet in [&#39;key&#39;, &#39;query&#39;, &#39;value&#39;, &#39;token&#39;], f&#34;&#34;&#34;{facet} is not a supported facet for descriptors. 
                                                             choose from [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;] &#34;&#34;&#34;
        self._extract_features(batch, [layer], facet)
        x = self._feats[0]
        if facet == &#39;token&#39;:
            x.unsqueeze_(dim=1) #Bx1xtxd
        if not include_cls:
            x = x[:, :, 1:, :]  # remove cls token
        else:
            assert not bin, &#34;bin = True and include_cls = True are not supported together, set one of them False.&#34;
        if not bin:
            desc = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1).unsqueeze(dim=1)  # Bx1xtx(dxh)
        else:
            desc = self._log_bin(x)
        return desc

    def extract_saliency_maps(self, batch: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer
        in of the CLS token. All values are then normalized to range between 0 and 1.
        :param batch: batch to extract saliency maps for. Has shape BxCxHxW.
        :return: a tensor of saliency maps. has shape Bxt-1
        &#34;&#34;&#34;
        assert self.model_type == &#34;dino_vits8&#34;, f&#34;saliency maps are supported only for dino_vits model_type.&#34;
        self._extract_features(batch, [11], &#39;attn&#39;)
        head_idxs = [0, 2, 4, 5]
        curr_feats = self._feats[0] #Bxhxtxt
        cls_attn_map = curr_feats[:, head_idxs, 0, 1:].mean(dim=1) #Bx(t-1)
        temp_mins, temp_maxs = cls_attn_map.min(dim=1)[0], cls_attn_map.max(dim=1)[0]
        cls_attn_maps = (cls_attn_map - temp_mins) / (temp_maxs - temp_mins)  # normalize to range [0,1]
        return cls_attn_maps

&#34;&#34;&#34; taken from https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse&#34;&#34;&#34;
def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in (&#39;yes&#39;, &#39;true&#39;, &#39;t&#39;, &#39;y&#39;, &#39;1&#39;):
        return True
    elif v.lower() in (&#39;no&#39;, &#39;false&#39;, &#39;f&#39;, &#39;n&#39;, &#39;0&#39;):
        return False
    else:
        raise argparse.ArgumentTypeError(&#39;Boolean value expected.&#39;)

if __name__ == &#34;__main__&#34;:
    parser = argparse.ArgumentParser(description=&#39;Facilitate ViT Descriptor extraction.&#39;)
    parser.add_argument(&#39;--image_path&#39;, type=str, required=True, help=&#39;path of the extracted image.&#39;)
    parser.add_argument(&#39;--output_path&#39;, type=str, required=True, help=&#39;path to file containing extracted descriptors.&#39;)
    parser.add_argument(&#39;--load_size&#39;, default=224, type=int, help=&#39;load size of the input image.&#39;)
    parser.add_argument(&#39;--stride&#39;, default=4, type=int, help=&#34;&#34;&#34;stride of first convolution layer. 
                                                              small stride -&gt; higher resolution.&#34;&#34;&#34;)
    parser.add_argument(&#39;--model_type&#39;, default=&#39;dino_vits8&#39;, type=str,
                        help=&#34;&#34;&#34;type of model to extract. 
                        Choose from [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 | 
                        vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]&#34;&#34;&#34;)
    parser.add_argument(&#39;--facet&#39;, default=&#39;key&#39;, type=str, help=&#34;&#34;&#34;facet to create descriptors from. 
                                                                    options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;]&#34;&#34;&#34;)
    parser.add_argument(&#39;--layer&#39;, default=11, type=int, help=&#34;layer to create descriptors from.&#34;)
    parser.add_argument(&#39;--bin&#39;, default=&#39;False&#39;, type=str2bool, help=&#34;create a binned descriptor if True.&#34;)

    args = parser.parse_args()

    with torch.no_grad():
        device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;
        extractor = ViTExtractor(args.model_type, args.stride, device=device)
        image_batch, image_pil = extractor.preprocess(args.image_path, args.load_size)
        print(f&#34;Image {args.image_path} is preprocessed to tensor of size {image_batch.shape}.&#34;)
        descriptors = extractor.extract_descriptors(image_batch.to(device), args.layer, args.facet, args.bin)
        print(f&#34;Descriptors are of size: {descriptors.shape}&#34;)
        torch.save(descriptors, args.output_path)
        print(f&#34;Descriptors saved to: {args.output_path}&#34;)


def upsample_feat_vec(feat, target_shape):
    return torch.nn.functional.interpolate(
        feat, target_shape, mode=&#34;bilinear&#34;, align_corners=True
    )


class VITFeatureExtractor(torch.nn.Module):
    def __init__(
        self,
        model_type=&#34;dino_vits8&#34;,
        stride=4,
        device=&#34;cuda:0&#34;,
        load_size=224,
        upsample=False,
        **kwargs,
    ):

        super().__init__()
        self.extractor = ViTExtractor(model_type, stride, device=device)
        self.load_size = load_size
        self.input_image_transform = self.get_input_image_transform()
        if upsample == True:
            if &#34;desired_height&#34; in kwargs.keys():
                self.desired_height = kwargs[&#34;desired_height&#34;]
                if &#34;desired_width&#34; in kwargs.keys():
                    self.desired_width = kwargs[&#34;desired_width&#34;]
                    self.upsample = True
                else:
                    warnings.warn(
                        &#34;Ignoring upsample arguments as they are incomplete. &#34;
                        &#34;Missing `desired_width`.&#34;
                    )
            else:
                warnings.warn(
                    &#34;Ignoring upsample arguments as they are incomplete. &#34;
                    &#34;Missing `desired_height`.&#34;
                )
        else:
            self.upsample = False
        # Layer to extract feature maps from
        self.layer_idx_to_extract_from = 11
        if &#34;layer&#34; in kwargs.keys():
            self.layer_idx_to_extract_from = kwargs[&#34;layer&#34;]
        # Type of attention component to create descriptors from
        self.facet = &#34;key&#34;
        if &#34;facet&#34; in kwargs.keys():
            self.facet = kwargs[&#34;facet&#34;]
        # Whether or not to create a binned descriptor
        self.binned = False
        if &#34;binned&#34; in kwargs.keys():
            self.binned = kwargs[&#34;binned&#34;]

    def get_input_image_transform(self):
        _NORM_MEAN = [0.485, 0.456, 0.406]
        _NORM_STD = [0.229, 0.224, 0.225]
        return torchvision.transforms.Compose(
            [torchvision.transforms.Normalize(mean=_NORM_MEAN, std=_NORM_STD)]
        )

    def forward(self, img, apply_default_input_transform=True):
        img = torchvision.transforms.functional.resize(img, self.load_size)
        if apply_default_input_transform:
            # Default input image transfoms
            img = self.input_image_transform(img)
        feat = self.extractor.extract_descriptors(
            img, self.layer_idx_to_extract_from, self.facet, self.binned
        )
        feat = feat.reshape(
            self.extractor.num_patches[0],
            self.extractor.num_patches[1],
            feat.shape[-1],
        )
        feat = feat.permute(2, 0, 1)
        feat = feat.unsqueeze(0)
        if self.upsample:
            feat = upsample_feat_vec(feat, [self.desired_height, self.desired_width])
        return feat

class Dino(object):
    def __init__(self, height, width):
        self.model = VITFeatureExtractor(
            upsample= True,
            desired_height=height,
            desired_width=width,
        )

    def get_embeddings(self, img):
        img = torch.from_numpy(np.array(img)).float() / 255.0
        img = img[..., :3]  # drop alpha channel, if present
        img = img.cuda()
        img = img.permute(2, 0, 1)  # C, H, W
        img = img.unsqueeze(0)  # 1, C, H, W
        img = img.cuda()
        # img_feat -&gt; (1, 512, H // 2, W // 2)
        img_feat = self.model.forward(img, apply_default_input_transform=False)
        img_feat_norm = torch.nn.functional.normalize(img_feat, dim=1)
        return img_feat_norm.cpu().detach().numpy()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="bayes3d.neural.dino.str2bool"><code class="name flex">
<span>def <span class="ident">str2bool</span></span>(<span>v)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in (&#39;yes&#39;, &#39;true&#39;, &#39;t&#39;, &#39;y&#39;, &#39;1&#39;):
        return True
    elif v.lower() in (&#39;no&#39;, &#39;false&#39;, &#39;f&#39;, &#39;n&#39;, &#39;0&#39;):
        return False
    else:
        raise argparse.ArgumentTypeError(&#39;Boolean value expected.&#39;)</code></pre>
</details>
</dd>
<dt id="bayes3d.neural.dino.upsample_feat_vec"><code class="name flex">
<span>def <span class="ident">upsample_feat_vec</span></span>(<span>feat, target_shape)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upsample_feat_vec(feat, target_shape):
    return torch.nn.functional.interpolate(
        feat, target_shape, mode=&#34;bilinear&#34;, align_corners=True
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="bayes3d.neural.dino.Dino"><code class="flex name class">
<span>class <span class="ident">Dino</span></span>
<span>(</span><span>height, width)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dino(object):
    def __init__(self, height, width):
        self.model = VITFeatureExtractor(
            upsample= True,
            desired_height=height,
            desired_width=width,
        )

    def get_embeddings(self, img):
        img = torch.from_numpy(np.array(img)).float() / 255.0
        img = img[..., :3]  # drop alpha channel, if present
        img = img.cuda()
        img = img.permute(2, 0, 1)  # C, H, W
        img = img.unsqueeze(0)  # 1, C, H, W
        img = img.cuda()
        # img_feat -&gt; (1, 512, H // 2, W // 2)
        img_feat = self.model.forward(img, apply_default_input_transform=False)
        img_feat_norm = torch.nn.functional.normalize(img_feat, dim=1)
        return img_feat_norm.cpu().detach().numpy()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bayes3d.neural.dino.Dino.get_embeddings"><code class="name flex">
<span>def <span class="ident">get_embeddings</span></span>(<span>self, img)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embeddings(self, img):
    img = torch.from_numpy(np.array(img)).float() / 255.0
    img = img[..., :3]  # drop alpha channel, if present
    img = img.cuda()
    img = img.permute(2, 0, 1)  # C, H, W
    img = img.unsqueeze(0)  # 1, C, H, W
    img = img.cuda()
    # img_feat -&gt; (1, 512, H // 2, W // 2)
    img_feat = self.model.forward(img, apply_default_input_transform=False)
    img_feat_norm = torch.nn.functional.normalize(img_feat, dim=1)
    return img_feat_norm.cpu().detach().numpy()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bayes3d.neural.dino.VITFeatureExtractor"><code class="flex name class">
<span>class <span class="ident">VITFeatureExtractor</span></span>
<span>(</span><span>model_type='dino_vits8', stride=4, device='cuda:0', load_size=224, upsample=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VITFeatureExtractor(torch.nn.Module):
    def __init__(
        self,
        model_type=&#34;dino_vits8&#34;,
        stride=4,
        device=&#34;cuda:0&#34;,
        load_size=224,
        upsample=False,
        **kwargs,
    ):

        super().__init__()
        self.extractor = ViTExtractor(model_type, stride, device=device)
        self.load_size = load_size
        self.input_image_transform = self.get_input_image_transform()
        if upsample == True:
            if &#34;desired_height&#34; in kwargs.keys():
                self.desired_height = kwargs[&#34;desired_height&#34;]
                if &#34;desired_width&#34; in kwargs.keys():
                    self.desired_width = kwargs[&#34;desired_width&#34;]
                    self.upsample = True
                else:
                    warnings.warn(
                        &#34;Ignoring upsample arguments as they are incomplete. &#34;
                        &#34;Missing `desired_width`.&#34;
                    )
            else:
                warnings.warn(
                    &#34;Ignoring upsample arguments as they are incomplete. &#34;
                    &#34;Missing `desired_height`.&#34;
                )
        else:
            self.upsample = False
        # Layer to extract feature maps from
        self.layer_idx_to_extract_from = 11
        if &#34;layer&#34; in kwargs.keys():
            self.layer_idx_to_extract_from = kwargs[&#34;layer&#34;]
        # Type of attention component to create descriptors from
        self.facet = &#34;key&#34;
        if &#34;facet&#34; in kwargs.keys():
            self.facet = kwargs[&#34;facet&#34;]
        # Whether or not to create a binned descriptor
        self.binned = False
        if &#34;binned&#34; in kwargs.keys():
            self.binned = kwargs[&#34;binned&#34;]

    def get_input_image_transform(self):
        _NORM_MEAN = [0.485, 0.456, 0.406]
        _NORM_STD = [0.229, 0.224, 0.225]
        return torchvision.transforms.Compose(
            [torchvision.transforms.Normalize(mean=_NORM_MEAN, std=_NORM_STD)]
        )

    def forward(self, img, apply_default_input_transform=True):
        img = torchvision.transforms.functional.resize(img, self.load_size)
        if apply_default_input_transform:
            # Default input image transfoms
            img = self.input_image_transform(img)
        feat = self.extractor.extract_descriptors(
            img, self.layer_idx_to_extract_from, self.facet, self.binned
        )
        feat = feat.reshape(
            self.extractor.num_patches[0],
            self.extractor.num_patches[1],
            feat.shape[-1],
        )
        feat = feat.permute(2, 0, 1)
        feat = feat.unsqueeze(0)
        if self.upsample:
            feat = upsample_feat_vec(feat, [self.desired_height, self.desired_width])
        return feat</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="bayes3d.neural.dino.VITFeatureExtractor.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bayes3d.neural.dino.VITFeatureExtractor.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="bayes3d.neural.dino.VITFeatureExtractor.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bayes3d.neural.dino.VITFeatureExtractor.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, img, apply_default_input_transform=True) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, img, apply_default_input_transform=True):
    img = torchvision.transforms.functional.resize(img, self.load_size)
    if apply_default_input_transform:
        # Default input image transfoms
        img = self.input_image_transform(img)
    feat = self.extractor.extract_descriptors(
        img, self.layer_idx_to_extract_from, self.facet, self.binned
    )
    feat = feat.reshape(
        self.extractor.num_patches[0],
        self.extractor.num_patches[1],
        feat.shape[-1],
    )
    feat = feat.permute(2, 0, 1)
    feat = feat.unsqueeze(0)
    if self.upsample:
        feat = upsample_feat_vec(feat, [self.desired_height, self.desired_width])
    return feat</code></pre>
</details>
</dd>
<dt id="bayes3d.neural.dino.VITFeatureExtractor.get_input_image_transform"><code class="name flex">
<span>def <span class="ident">get_input_image_transform</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_image_transform(self):
    _NORM_MEAN = [0.485, 0.456, 0.406]
    _NORM_STD = [0.229, 0.224, 0.225]
    return torchvision.transforms.Compose(
        [torchvision.transforms.Normalize(mean=_NORM_MEAN, std=_NORM_STD)]
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bayes3d.neural.dino.ViTExtractor"><code class="flex name class">
<span>class <span class="ident">ViTExtractor</span></span>
<span>(</span><span>model_type: str = 'dino_vits8', stride: int = 4, model: torch.nn.modules.module.Module = None, device: str = 'cuda')</span>
</code></dt>
<dd>
<div class="desc"><p>This class facilitates extraction of features, descriptors, and saliency maps from a ViT.</p>
<p>We use the following notation in the documentation of the module's methods:
B - batch size
h - number of heads. usually takes place of the channel dimension in pytorch's convention BxCxHxW
p - patch size of the ViT. either 8 or 16.
t - number of tokens. equals the number of patches + 1, e.g. HW / p**2 + 1. Where H and W are the height and width
of the input image.
d - the embedding dimension in the ViT.</p>
<p>:param model_type: A string specifying the type of model to extract from.
[dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 |
vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]
:param stride: stride of first convolution layer. small stride -&gt; higher resolution.
:param model: Optional parameter. The nn.Module to extract from instead of creating a new one in ViTExtractor.
should be compatible with model_type.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ViTExtractor:
    &#34;&#34;&#34; This class facilitates extraction of features, descriptors, and saliency maps from a ViT.

    We use the following notation in the documentation of the module&#39;s methods:
    B - batch size
    h - number of heads. usually takes place of the channel dimension in pytorch&#39;s convention BxCxHxW
    p - patch size of the ViT. either 8 or 16.
    t - number of tokens. equals the number of patches + 1, e.g. HW / p**2 + 1. Where H and W are the height and width
    of the input image.
    d - the embedding dimension in the ViT.
    &#34;&#34;&#34;

    def __init__(self, model_type: str = &#39;dino_vits8&#39;, stride: int = 4, model: nn.Module = None, device: str = &#39;cuda&#39;):
        &#34;&#34;&#34;
        :param model_type: A string specifying the type of model to extract from.
                          [dino_vits8 | dino_vits16 | dino_vitb8 | dino_vitb16 | vit_small_patch8_224 |
                          vit_small_patch16_224 | vit_base_patch8_224 | vit_base_patch16_224]
        :param stride: stride of first convolution layer. small stride -&gt; higher resolution.
        :param model: Optional parameter. The nn.Module to extract from instead of creating a new one in ViTExtractor.
                      should be compatible with model_type.
        &#34;&#34;&#34;
        self.model_type = model_type
        self.device = device
        if model is not None:
            self.model = model
        else:
            self.model = ViTExtractor.create_model(model_type)

        self.model = ViTExtractor.patch_vit_resolution(self.model, stride=stride)
        self.model.eval()
        self.model.to(self.device)
        self.p = self.model.patch_embed.patch_size
        self.stride = self.model.patch_embed.proj.stride

        self.mean = (0.485, 0.456, 0.406) if &#34;dino&#34; in self.model_type else (0.5, 0.5, 0.5)
        self.std = (0.229, 0.224, 0.225) if &#34;dino&#34; in self.model_type else (0.5, 0.5, 0.5)

        self._feats = []
        self.hook_handlers = []
        self.load_size = None
        self.num_patches = None

    @staticmethod
    def create_model(model_type: str) -&gt; nn.Module:
        &#34;&#34;&#34;
        :param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |
                           dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |
                           vit_base_patch16_224]
        :return: the model
        &#34;&#34;&#34;
        if &#39;dino&#39; in model_type:
            model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type)
        else:  # model from timm -- load weights from timm to dino model (enables working on arbitrary size images).
            temp_model = timm.create_model(model_type, pretrained=True)
            model_type_dict = {
                &#39;vit_small_patch16_224&#39;: &#39;dino_vits16&#39;,
                &#39;vit_small_patch8_224&#39;: &#39;dino_vits8&#39;,
                &#39;vit_base_patch16_224&#39;: &#39;dino_vitb16&#39;,
                &#39;vit_base_patch8_224&#39;: &#39;dino_vitb8&#39;
            }
            model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type_dict[model_type])
            temp_state_dict = temp_model.state_dict()
            del temp_state_dict[&#39;head.weight&#39;]
            del temp_state_dict[&#39;head.bias&#39;]
            model.load_state_dict(temp_state_dict)
        return model

    @staticmethod
    def _fix_pos_enc(patch_size: int, stride_hw: Tuple[int, int]):
        &#34;&#34;&#34;
        Creates a method for position encoding interpolation.
        :param patch_size: patch size of the model.
        :param stride_hw: A tuple containing the new height and width stride respectively.
        :return: the interpolation method
        &#34;&#34;&#34;
        def interpolate_pos_encoding(self, x: torch.Tensor, w: int, h: int) -&gt; torch.Tensor:
            npatch = x.shape[1] - 1
            N = self.pos_embed.shape[1] - 1
            if npatch == N and w == h:
                return self.pos_embed
            class_pos_embed = self.pos_embed[:, 0]
            patch_pos_embed = self.pos_embed[:, 1:]
            dim = x.shape[-1]
            # compute number of tokens taking stride into account
            w0 = 1 + (w - patch_size) // stride_hw[1]
            h0 = 1 + (h - patch_size) // stride_hw[0]
            assert (w0 * h0 == npatch), f&#34;&#34;&#34;got wrong grid size for {h}x{w} with patch_size {patch_size} and 
                                            stride {stride_hw} got {h0}x{w0}={h0 * w0} expecting {npatch}&#34;&#34;&#34;
            # we add a small number to avoid floating point error in the interpolation
            # see discussion at https://github.com/facebookresearch/dino/issues/8
            w0, h0 = w0 + 0.1, h0 + 0.1
            patch_pos_embed = nn.functional.interpolate(
                patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),
                scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),
                mode=&#39;bicubic&#39;,
                align_corners=False, recompute_scale_factor=False
            )
            assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]
            patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)
            return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)

        return interpolate_pos_encoding

    @staticmethod
    def patch_vit_resolution(model: nn.Module, stride: int) -&gt; nn.Module:
        &#34;&#34;&#34;
        change resolution of model output by changing the stride of the patch extraction.
        :param model: the model to change resolution for.
        :param stride: the new stride parameter.
        :return: the adjusted model
        &#34;&#34;&#34;
        patch_size = model.patch_embed.patch_size
        if stride == patch_size:  # nothing to do
            return model

        stride = nn_utils._pair(stride)
        assert all([(patch_size // s_) * s_ == patch_size for s_ in
                    stride]), f&#39;stride {stride} should divide patch_size {patch_size}&#39;

        # fix the stride
        model.patch_embed.proj.stride = stride
        # fix the positional encoding code
        model.interpolate_pos_encoding = types.MethodType(ViTExtractor._fix_pos_enc(patch_size, stride), model)
        return model

    def preprocess(self, image_path: Union[str, Path],
                   load_size: Union[int, Tuple[int, int]] = None) -&gt; Tuple[torch.Tensor, Image.Image]:
        &#34;&#34;&#34;
        Preprocesses an image before extraction.
        :param image_path: path to image to be extracted.
        :param load_size: optional. Size to resize image before the rest of preprocessing.
        :return: a tuple containing:
                    (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.
                    (2) the pil image in relevant dimensions
        &#34;&#34;&#34;
        pil_image = Image.open(image_path).convert(&#39;RGB&#39;)
        if load_size is not None:
            pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.LANCZOS)(pil_image)
        prep = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=self.mean, std=self.std)
        ])
        prep_img = prep(pil_image)[None, ...]
        return prep_img, pil_image

    def _get_hook(self, facet: str):
        &#34;&#34;&#34;
        generate a hook method for a specific block and facet.
        &#34;&#34;&#34;
        if facet in [&#39;attn&#39;, &#39;token&#39;]:
            def _hook(model, input, output):
                self._feats.append(output)
            return _hook

        if facet == &#39;query&#39;:
            facet_idx = 0
        elif facet == &#39;key&#39;:
            facet_idx = 1
        elif facet == &#39;value&#39;:
            facet_idx = 2
        else:
            raise TypeError(f&#34;{facet} is not a supported facet.&#34;)

        def _inner_hook(module, input, output):
            input = input[0]
            B, N, C = input.shape
            qkv = module.qkv(input).reshape(B, N, 3, module.num_heads, C // module.num_heads).permute(2, 0, 3, 1, 4)
            self._feats.append(qkv[facet_idx]) #Bxhxtxd
        return _inner_hook

    def _register_hooks(self, layers: List[int], facet: str) -&gt; None:
        &#34;&#34;&#34;
        register hook to extract features.
        :param layers: layers from which to extract features.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39; | &#39;attn&#39;]
        &#34;&#34;&#34;
        for block_idx, block in enumerate(self.model.blocks):
            if block_idx in layers:
                if facet == &#39;token&#39;:
                    self.hook_handlers.append(block.register_forward_hook(self._get_hook(facet)))
                elif facet == &#39;attn&#39;:
                    self.hook_handlers.append(block.attn.attn_drop.register_forward_hook(self._get_hook(facet)))
                elif facet in [&#39;key&#39;, &#39;query&#39;, &#39;value&#39;]:
                    self.hook_handlers.append(block.attn.register_forward_hook(self._get_hook(facet)))
                else:
                    raise TypeError(f&#34;{facet} is not a supported facet.&#34;)

    def _unregister_hooks(self) -&gt; None:
        &#34;&#34;&#34;
        unregisters the hooks. should be called after feature extraction.
        &#34;&#34;&#34;
        for handle in self.hook_handlers:
            handle.remove()
        self.hook_handlers = []

    def _extract_features(self, batch: torch.Tensor, layers: List[int] = 11, facet: str = &#39;key&#39;) -&gt; List[torch.Tensor]:
        &#34;&#34;&#34;
        extract features from the model
        :param batch: batch to extract features for. Has shape BxCxHxW.
        :param layers: layer to extract. A number between 0 to 11.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39; | &#39;attn&#39;]
        :return : tensor of features.
                  if facet is &#39;key&#39; | &#39;query&#39; | &#39;value&#39; has shape Bxhxtxd
                  if facet is &#39;attn&#39; has shape Bxhxtxt
                  if facet is &#39;token&#39; has shape Bxtxd
        &#34;&#34;&#34;
        B, C, H, W = batch.shape
        self._feats = []
        self._register_hooks(layers, facet)
        _ = self.model(batch)
        self._unregister_hooks()
        self.load_size = (H, W)
        self.num_patches = (1 + (H - self.p) // self.stride[0], 1 + (W - self.p) // self.stride[1])
        return self._feats

    def _log_bin(self, x: torch.Tensor, hierarchy: int = 2) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        create a log-binned descriptor.
        :param x: tensor of features. Has shape Bxhxtxd.
        :param hierarchy: how many bin hierarchies to use.
        &#34;&#34;&#34;
        B = x.shape[0]
        num_bins = 1 + 8 * hierarchy

        bin_x = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1)  # Bx(t-1)x(dxh)
        bin_x = bin_x.permute(0, 2, 1)
        bin_x = bin_x.reshape(B, bin_x.shape[1], self.num_patches[0], self.num_patches[1])
        # Bx(dxh)xnum_patches[0]xnum_patches[1]
        sub_desc_dim = bin_x.shape[1]

        avg_pools = []
        # compute bins of all sizes for all spatial locations.
        for k in range(0, hierarchy):
            # avg pooling with kernel 3**kx3**k
            win_size = 3 ** k
            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)
            avg_pools.append(avg_pool(bin_x))

        bin_x = torch.zeros((B, sub_desc_dim * num_bins, self.num_patches[0], self.num_patches[1])).to(self.device)
        for y in range(self.num_patches[0]):
            for x in range(self.num_patches[1]):
                part_idx = 0
                # fill all bins for a spatial location (y, x)
                for k in range(0, hierarchy):
                    kernel_size = 3 ** k
                    for i in range(y - kernel_size, y + kernel_size + 1, kernel_size):
                        for j in range(x - kernel_size, x + kernel_size + 1, kernel_size):
                            if i == y and j == x and k != 0:
                                continue
                            if 0 &lt;= i &lt; self.num_patches[0] and 0 &lt;= j &lt; self.num_patches[1]:
                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][
                                                                                                           :, :, i, j]
                            else:  # handle padding in a more delicate way than zero padding
                                temp_i = max(0, min(i, self.num_patches[0] - 1))
                                temp_j = max(0, min(j, self.num_patches[1] - 1))
                                bin_x[:, part_idx * sub_desc_dim: (part_idx + 1) * sub_desc_dim, y, x] = avg_pools[k][
                                                                                                           :, :, temp_i,
                                                                                                           temp_j]
                            part_idx += 1
        bin_x = bin_x.flatten(start_dim=-2, end_dim=-1).permute(0, 2, 1).unsqueeze(dim=1)
        # Bx1x(t-1)x(dxh)
        return bin_x

    def extract_descriptors(self, batch: torch.Tensor, layer: int = 11, facet: str = &#39;key&#39;,
                            bin: bool = False, include_cls: bool = False) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        extract descriptors from the model
        :param batch: batch to extract descriptors for. Has shape BxCxHxW.
        :param layers: layer to extract. A number between 0 to 11.
        :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;]
        :param bin: apply log binning to the descriptor. default is False.
        :return: tensor of descriptors. Bx1xtxd&#39; where d&#39; is the dimension of the descriptors.
        &#34;&#34;&#34;
        assert facet in [&#39;key&#39;, &#39;query&#39;, &#39;value&#39;, &#39;token&#39;], f&#34;&#34;&#34;{facet} is not a supported facet for descriptors. 
                                                             choose from [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;] &#34;&#34;&#34;
        self._extract_features(batch, [layer], facet)
        x = self._feats[0]
        if facet == &#39;token&#39;:
            x.unsqueeze_(dim=1) #Bx1xtxd
        if not include_cls:
            x = x[:, :, 1:, :]  # remove cls token
        else:
            assert not bin, &#34;bin = True and include_cls = True are not supported together, set one of them False.&#34;
        if not bin:
            desc = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1).unsqueeze(dim=1)  # Bx1xtx(dxh)
        else:
            desc = self._log_bin(x)
        return desc

    def extract_saliency_maps(self, batch: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer
        in of the CLS token. All values are then normalized to range between 0 and 1.
        :param batch: batch to extract saliency maps for. Has shape BxCxHxW.
        :return: a tensor of saliency maps. has shape Bxt-1
        &#34;&#34;&#34;
        assert self.model_type == &#34;dino_vits8&#34;, f&#34;saliency maps are supported only for dino_vits model_type.&#34;
        self._extract_features(batch, [11], &#39;attn&#39;)
        head_idxs = [0, 2, 4, 5]
        curr_feats = self._feats[0] #Bxhxtxt
        cls_attn_map = curr_feats[:, head_idxs, 0, 1:].mean(dim=1) #Bx(t-1)
        temp_mins, temp_maxs = cls_attn_map.min(dim=1)[0], cls_attn_map.max(dim=1)[0]
        cls_attn_maps = (cls_attn_map - temp_mins) / (temp_maxs - temp_mins)  # normalize to range [0,1]
        return cls_attn_maps</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="bayes3d.neural.dino.ViTExtractor.create_model"><code class="name flex">
<span>def <span class="ident">create_model</span></span>(<span>model_type: str) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>:param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |
dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |
vit_base_patch16_224]
:return: the model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create_model(model_type: str) -&gt; nn.Module:
    &#34;&#34;&#34;
    :param model_type: a string specifying which model to load. [dino_vits8 | dino_vits16 | dino_vitb8 |
                       dino_vitb16 | vit_small_patch8_224 | vit_small_patch16_224 | vit_base_patch8_224 |
                       vit_base_patch16_224]
    :return: the model
    &#34;&#34;&#34;
    if &#39;dino&#39; in model_type:
        model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type)
    else:  # model from timm -- load weights from timm to dino model (enables working on arbitrary size images).
        temp_model = timm.create_model(model_type, pretrained=True)
        model_type_dict = {
            &#39;vit_small_patch16_224&#39;: &#39;dino_vits16&#39;,
            &#39;vit_small_patch8_224&#39;: &#39;dino_vits8&#39;,
            &#39;vit_base_patch16_224&#39;: &#39;dino_vitb16&#39;,
            &#39;vit_base_patch8_224&#39;: &#39;dino_vitb8&#39;
        }
        model = torch.hub.load(&#39;facebookresearch/dino:main&#39;, model_type_dict[model_type])
        temp_state_dict = temp_model.state_dict()
        del temp_state_dict[&#39;head.weight&#39;]
        del temp_state_dict[&#39;head.bias&#39;]
        model.load_state_dict(temp_state_dict)
    return model</code></pre>
</details>
</dd>
<dt id="bayes3d.neural.dino.ViTExtractor.patch_vit_resolution"><code class="name flex">
<span>def <span class="ident">patch_vit_resolution</span></span>(<span>model: torch.nn.modules.module.Module, stride: int) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<div class="desc"><p>change resolution of model output by changing the stride of the patch extraction.
:param model: the model to change resolution for.
:param stride: the new stride parameter.
:return: the adjusted model</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def patch_vit_resolution(model: nn.Module, stride: int) -&gt; nn.Module:
    &#34;&#34;&#34;
    change resolution of model output by changing the stride of the patch extraction.
    :param model: the model to change resolution for.
    :param stride: the new stride parameter.
    :return: the adjusted model
    &#34;&#34;&#34;
    patch_size = model.patch_embed.patch_size
    if stride == patch_size:  # nothing to do
        return model

    stride = nn_utils._pair(stride)
    assert all([(patch_size // s_) * s_ == patch_size for s_ in
                stride]), f&#39;stride {stride} should divide patch_size {patch_size}&#39;

    # fix the stride
    model.patch_embed.proj.stride = stride
    # fix the positional encoding code
    model.interpolate_pos_encoding = types.MethodType(ViTExtractor._fix_pos_enc(patch_size, stride), model)
    return model</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="bayes3d.neural.dino.ViTExtractor.extract_descriptors"><code class="name flex">
<span>def <span class="ident">extract_descriptors</span></span>(<span>self, batch: torch.Tensor, layer: int = 11, facet: str = 'key', bin: bool = False, include_cls: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>extract descriptors from the model
:param batch: batch to extract descriptors for. Has shape BxCxHxW.
:param layers: layer to extract. A number between 0 to 11.
:param facet: facet to extract. One of the following options: ['key' | 'query' | 'value' | 'token']
:param bin: apply log binning to the descriptor. default is False.
:return: tensor of descriptors. Bx1xtxd' where d' is the dimension of the descriptors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_descriptors(self, batch: torch.Tensor, layer: int = 11, facet: str = &#39;key&#39;,
                        bin: bool = False, include_cls: bool = False) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    extract descriptors from the model
    :param batch: batch to extract descriptors for. Has shape BxCxHxW.
    :param layers: layer to extract. A number between 0 to 11.
    :param facet: facet to extract. One of the following options: [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;]
    :param bin: apply log binning to the descriptor. default is False.
    :return: tensor of descriptors. Bx1xtxd&#39; where d&#39; is the dimension of the descriptors.
    &#34;&#34;&#34;
    assert facet in [&#39;key&#39;, &#39;query&#39;, &#39;value&#39;, &#39;token&#39;], f&#34;&#34;&#34;{facet} is not a supported facet for descriptors. 
                                                         choose from [&#39;key&#39; | &#39;query&#39; | &#39;value&#39; | &#39;token&#39;] &#34;&#34;&#34;
    self._extract_features(batch, [layer], facet)
    x = self._feats[0]
    if facet == &#39;token&#39;:
        x.unsqueeze_(dim=1) #Bx1xtxd
    if not include_cls:
        x = x[:, :, 1:, :]  # remove cls token
    else:
        assert not bin, &#34;bin = True and include_cls = True are not supported together, set one of them False.&#34;
    if not bin:
        desc = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1).unsqueeze(dim=1)  # Bx1xtx(dxh)
    else:
        desc = self._log_bin(x)
    return desc</code></pre>
</details>
</dd>
<dt id="bayes3d.neural.dino.ViTExtractor.extract_saliency_maps"><code class="name flex">
<span>def <span class="ident">extract_saliency_maps</span></span>(<span>self, batch: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer
in of the CLS token. All values are then normalized to range between 0 and 1.
:param batch: batch to extract saliency maps for. Has shape BxCxHxW.
:return: a tensor of saliency maps. has shape Bxt-1</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_saliency_maps(self, batch: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer
    in of the CLS token. All values are then normalized to range between 0 and 1.
    :param batch: batch to extract saliency maps for. Has shape BxCxHxW.
    :return: a tensor of saliency maps. has shape Bxt-1
    &#34;&#34;&#34;
    assert self.model_type == &#34;dino_vits8&#34;, f&#34;saliency maps are supported only for dino_vits model_type.&#34;
    self._extract_features(batch, [11], &#39;attn&#39;)
    head_idxs = [0, 2, 4, 5]
    curr_feats = self._feats[0] #Bxhxtxt
    cls_attn_map = curr_feats[:, head_idxs, 0, 1:].mean(dim=1) #Bx(t-1)
    temp_mins, temp_maxs = cls_attn_map.min(dim=1)[0], cls_attn_map.max(dim=1)[0]
    cls_attn_maps = (cls_attn_map - temp_mins) / (temp_maxs - temp_mins)  # normalize to range [0,1]
    return cls_attn_maps</code></pre>
</details>
</dd>
<dt id="bayes3d.neural.dino.ViTExtractor.preprocess"><code class="name flex">
<span>def <span class="ident">preprocess</span></span>(<span>self, image_path: Union[str, pathlib.Path], load_size: Union[int, Tuple[int, int]] = None) ‑> Tuple[torch.Tensor, PIL.Image.Image]</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocesses an image before extraction.
:param image_path: path to image to be extracted.
:param load_size: optional. Size to resize image before the rest of preprocessing.
:return: a tuple containing:
(1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.
(2) the pil image in relevant dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess(self, image_path: Union[str, Path],
               load_size: Union[int, Tuple[int, int]] = None) -&gt; Tuple[torch.Tensor, Image.Image]:
    &#34;&#34;&#34;
    Preprocesses an image before extraction.
    :param image_path: path to image to be extracted.
    :param load_size: optional. Size to resize image before the rest of preprocessing.
    :return: a tuple containing:
                (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.
                (2) the pil image in relevant dimensions
    &#34;&#34;&#34;
    pil_image = Image.open(image_path).convert(&#39;RGB&#39;)
    if load_size is not None:
        pil_image = transforms.Resize(load_size, interpolation=transforms.InterpolationMode.LANCZOS)(pil_image)
    prep = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=self.mean, std=self.std)
    ])
    prep_img = prep(pil_image)[None, ...]
    return prep_img, pil_image</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="bayes3d.neural" href="index.html">bayes3d.neural</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="bayes3d.neural.dino.str2bool" href="#bayes3d.neural.dino.str2bool">str2bool</a></code></li>
<li><code><a title="bayes3d.neural.dino.upsample_feat_vec" href="#bayes3d.neural.dino.upsample_feat_vec">upsample_feat_vec</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="bayes3d.neural.dino.Dino" href="#bayes3d.neural.dino.Dino">Dino</a></code></h4>
<ul class="">
<li><code><a title="bayes3d.neural.dino.Dino.get_embeddings" href="#bayes3d.neural.dino.Dino.get_embeddings">get_embeddings</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bayes3d.neural.dino.VITFeatureExtractor" href="#bayes3d.neural.dino.VITFeatureExtractor">VITFeatureExtractor</a></code></h4>
<ul class="">
<li><code><a title="bayes3d.neural.dino.VITFeatureExtractor.call_super_init" href="#bayes3d.neural.dino.VITFeatureExtractor.call_super_init">call_super_init</a></code></li>
<li><code><a title="bayes3d.neural.dino.VITFeatureExtractor.dump_patches" href="#bayes3d.neural.dino.VITFeatureExtractor.dump_patches">dump_patches</a></code></li>
<li><code><a title="bayes3d.neural.dino.VITFeatureExtractor.forward" href="#bayes3d.neural.dino.VITFeatureExtractor.forward">forward</a></code></li>
<li><code><a title="bayes3d.neural.dino.VITFeatureExtractor.get_input_image_transform" href="#bayes3d.neural.dino.VITFeatureExtractor.get_input_image_transform">get_input_image_transform</a></code></li>
<li><code><a title="bayes3d.neural.dino.VITFeatureExtractor.training" href="#bayes3d.neural.dino.VITFeatureExtractor.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bayes3d.neural.dino.ViTExtractor" href="#bayes3d.neural.dino.ViTExtractor">ViTExtractor</a></code></h4>
<ul class="">
<li><code><a title="bayes3d.neural.dino.ViTExtractor.create_model" href="#bayes3d.neural.dino.ViTExtractor.create_model">create_model</a></code></li>
<li><code><a title="bayes3d.neural.dino.ViTExtractor.extract_descriptors" href="#bayes3d.neural.dino.ViTExtractor.extract_descriptors">extract_descriptors</a></code></li>
<li><code><a title="bayes3d.neural.dino.ViTExtractor.extract_saliency_maps" href="#bayes3d.neural.dino.ViTExtractor.extract_saliency_maps">extract_saliency_maps</a></code></li>
<li><code><a title="bayes3d.neural.dino.ViTExtractor.patch_vit_resolution" href="#bayes3d.neural.dino.ViTExtractor.patch_vit_resolution">patch_vit_resolution</a></code></li>
<li><code><a title="bayes3d.neural.dino.ViTExtractor.preprocess" href="#bayes3d.neural.dino.ViTExtractor.preprocess">preprocess</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>