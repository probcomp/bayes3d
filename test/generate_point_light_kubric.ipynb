{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import bayes3d as b\n",
    "import trimesh\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "from bayes3d._rendering.photorealistic_renderers.kubric_interface import render_many\n",
    "import png2avi as p2a\n",
    "import jax\n",
    "import matplotlib\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_ind = 57 #54\n",
    "im_ind = 1\n",
    "\n",
    "# --- creating the ycb dir from the working directory\n",
    "bop_ycb_dir = os.path.join(b.utils.get_assets_dir(), \"bop/ycbv\")\n",
    "rgbd, gt_ids, gt_poses, masks = b.utils.ycb_loader.get_test_img(str(scene_ind), str(im_ind), bop_ycb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing frame buffer size to (width, height, depth) = (640, 480, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E rasterize_gl.cpp:121] OpenGL version reported as 4.6\n"
     ]
    }
   ],
   "source": [
    "intrinsics = b.Intrinsics(\n",
    "    rgbd.intrinsics.height, rgbd.intrinsics.width,\n",
    "    rgbd.intrinsics.fx, rgbd.intrinsics.fx,\n",
    "    rgbd.intrinsics.width/2, rgbd.intrinsics.height/2,\n",
    "    rgbd.intrinsics.near, 10.0 #rgbd.intrinsics.far\n",
    ")\n",
    "\n",
    "\n",
    "b.setup_renderer(intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "mesh_paths = []\n",
    "offset_poses = []\n",
    "heights = []\n",
    "names = []\n",
    "model_dir = os.path.join(b.utils.get_assets_dir(), \"ycb_video_models/models\")\n",
    "for i in tqdm(gt_ids):\n",
    "    mesh_path = os.path.join(model_dir, b.utils.ycb_loader.MODEL_NAMES[i],\"textured.obj\")\n",
    "    m, pose = b.utils.mesh.center_mesh(trimesh.load(mesh_path), return_pose=True)\n",
    "    m = trimesh.load(mesh_path)\n",
    "    bbox, _ = b.utils.aabb(m.vertices)\n",
    "    heights.append(bbox[2]) # get z axis\n",
    "    names.append(b.utils.ycb_loader.MODEL_NAMES[i])\n",
    "    offset_poses.append(pose)\n",
    "    b.RENDERER.add_mesh_from_file(mesh_path, center_mesh=False)\n",
    "    mesh_paths.append(\n",
    "        mesh_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Heights:\n",
      "005_tomato_soup_can: 0.101815\n",
      "021_bleach_cleanser: 0.250586\n",
      "040_large_marker: 0.018885002\n",
      "052_extra_large_clamp: 0.036225\n",
      "061_foam_brick: 0.051204003\n"
     ]
    }
   ],
   "source": [
    "# print heights\n",
    "print('Object Heights:')\n",
    "for i, name in enumerate(names):\n",
    "    print(name + ': ' + str(heights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = []\n",
    "for i in range(len(gt_ids)):\n",
    "    poses.append(\n",
    "        gt_poses[i] @ b.t3d.inverse_pose(offset_poses[i])\n",
    "    )\n",
    "poses = jnp.array(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the hardcoded object has to be upright - how to detect this automatically?\n",
    "centered_item = 0 # hardcoded to be a number\n",
    "\n",
    "center_obj_basis = gt_poses[centered_item]\n",
    "\n",
    "obj_poses = jnp.einsum('jk,ikl->ijl', b.t3d.inverse_pose(center_obj_basis),poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_pc = []\n",
    "\n",
    "for i in range(len(b.RENDERER.meshes)):\n",
    "    scene_pc.append(b.t3d.apply_transform(b.RENDERER.meshes[i].vertices, obj_poses[i]))\n",
    "\n",
    "scene_pc = np.concatenate(scene_pc)\n",
    "\n",
    "bbox, center = b.utils.aabb(scene_pc)\n",
    "minz = center[2,3]-bbox[2]/2\n",
    "scene_pc_shift = b.t3d.apply_transform(scene_pc, b.t3d.inverse_pose(b.t3d.transform_from_pos(jnp.array([0,0,minz]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mesh = b.utils.make_cuboid_mesh([1,1,0.01])\n",
    "\n",
    "max_edge = min(b.utils.aabb(table_mesh.vertices)[0])\n",
    "\n",
    "b.RENDERER.add_mesh(trimesh.Trimesh(*trimesh.remesh.subdivide_to_size(table_mesh.vertices, table_mesh.faces, max_edge))) # need to remesh to proper scale\n",
    "frames = 20\n",
    "\n",
    "dome_pose = np.eye(4)\n",
    "\n",
    "vids = 1 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering mesh with translation [-0.0013725  0.002524  -0.017171 ]\n"
     ]
    }
   ],
   "source": [
    "## Add scaled wood block for photorealistic kubric render\n",
    "mesh_path = os.path.join(model_dir, b.utils.ycb_loader.MODEL_NAMES[15],\"textured.obj\")\n",
    "mesh_paths.append(mesh_path)\n",
    "b.RENDERER.add_mesh_from_file(mesh_path, center_mesh=True)\n",
    "\n",
    "flat_wood_ind = len(b.RENDERER.meshes) - 1\n",
    "bbox, _ = b.utils.aabb(b.RENDERER.meshes[flat_wood_ind].vertices)\n",
    "\n",
    "tabletop_scaling_factor = [15.0, 15.0, 1.0/10]\n",
    "b.RENDERER.meshes[flat_wood_ind].vertices = b.RENDERER.meshes[flat_wood_ind].vertices @ np.diag(tabletop_scaling_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_obj_poses = []\n",
    "for i in range(len(obj_poses)):\n",
    "    translated_obj_poses.append(b.t3d.inverse_pose(b.t3d.transform_from_pos(jnp.array([0,0,minz]))) @ obj_poses[i])\n",
    "\n",
    "translated_obj_poses.append(dome_pose) # wood block\n",
    "translated_obj_poses.append(dome_pose) # cuboid mesh\n",
    "\n",
    "translated_obj_poses = jnp.array(translated_obj_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "\n",
    "def cart2sph(x,y,z):\n",
    "    azimuth = np.arctan2(y,x)\n",
    "    elevation = np.arctan2(z,np.sqrt(x**2 + y**2))\n",
    "    r = np.sqrt(x**2 + y**2 + z**2)\n",
    "    return azimuth, elevation, r\n",
    "\n",
    "def sph2cart(azimuth,elevation,r):\n",
    "    x = r * np.cos(elevation) * np.cos(azimuth)\n",
    "    y = r * np.cos(elevation) * np.sin(azimuth)\n",
    "    z = r * np.sin(elevation)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def sample_point_in_half_sphere_shell(inner_radius, outer_radius, z_offset_min, rng_linear):\n",
    "    while True:\n",
    "        point = rng_linear.uniform(inner_radius, outer_radius, 3)\n",
    "        if np.linalg.norm(point) > inner_radius and np.linalg.norm(point) < outer_radius and point[2] > z_offset_min:\n",
    "            return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera view rng is FIXED TRAJECTORY in this function\n",
    "def get_linear_camera_motion(\n",
    "    movement_speed: float,\n",
    "    inner_radius: float = 0.5,\n",
    "    outer_radius: float = 1,\n",
    "    z_offset_min: float = 0.1,\n",
    "    z_offset_max: float = 1,\n",
    "    frames = 10,\n",
    "    rng_linear = np.random.RandomState(12345)\n",
    "):\n",
    "    \"\"\"Sample a linear path which starts and ends within a half-sphere shell.\"\"\"\n",
    "\n",
    "    while True:\n",
    "        camera_start = np.array(sample_point_in_half_sphere_shell(inner_radius, outer_radius, z_offset_min, rng_linear))\n",
    "        direction = rng_linear.rand(3) - 0.5\n",
    "        movement = direction / np.linalg.norm(direction) * movement_speed\n",
    "        camera_end = camera_start + movement\n",
    "\n",
    "        #check values\n",
    "        print('camera start: '+str(camera_start))\n",
    "        print('camera end: ' + str(camera_end))\n",
    "\n",
    "        if (inner_radius <= np.linalg.norm(camera_end) <= outer_radius and\n",
    "            camera_end[2] > z_offset_min and camera_end[2] < z_offset_max):\n",
    "            break\n",
    "    \n",
    "    camera_positions = []\n",
    "\n",
    "    for frame in range(frames):\n",
    "        interp = (frame * 1.0) / frames\n",
    "        pos_interp = camera_start + interp*(camera_end - camera_start)\n",
    "        camera_positions.append(pos_interp)\n",
    "    \n",
    "    return camera_positions\n",
    "\n",
    "\n",
    "# camera view rng is FIXED TRAJECTORY in this function\n",
    "def get_spherical_camera_motion(\n",
    "    movement_speed: float,\n",
    "    inner_radius: float = 0.75,\n",
    "    outer_radius: float = 1.5,\n",
    "    z_offset_min: float = 0.1,\n",
    "    z_offset_max: float = 1,\n",
    "    frames = 10,\n",
    "    rng_sphere = np.random.RandomState(12345)\n",
    "):\n",
    "    \"\"\"Sample a spherical path which starts and ends within a half-sphere shell.\"\"\"\n",
    "\n",
    "    while True:\n",
    "        camera_start = np.array(sample_point_in_half_sphere_shell(inner_radius, outer_radius, z_offset_min, rng_sphere))\n",
    "        \n",
    "\n",
    "        # movement speed is defined as unit time for the whole trajectory\n",
    "        # frame interpolation is done in next block\n",
    "        length = movement_speed * 1.0 \n",
    "\n",
    "        lambda1, phi1, r = cart2sph(*camera_start)\n",
    "        angle = length/r\n",
    "\n",
    "        # Great Circle Formula:\n",
    "        # angle = arrcos(sin(phi1)*sin(phi2)+cos(phi1)*cos(phi2)*cos(del_lambda))\n",
    "        phi2 = rng_sphere.uniform(0, np.pi/2)\n",
    "        del_lambda = np.arccos((np.cos(angle) - np.sin(phi1)*np.sin(phi2))/(np.cos(phi1)*np.cos(phi2)))\n",
    "        #lambda2 = rng_sphere.choice([lambda1+del_lambda, lambda1-del_lambda])\n",
    "        lambda2 = lambda1+del_lambda#, lambda1-del_lambda\n",
    "\n",
    "        camera_end = np.array(sph2cart(lambda2, phi2, r))\n",
    "\n",
    "        #check values\n",
    "        print('camera start spherical: '+str(cart2sph(*camera_start)))\n",
    "        print('camera end spherical: ' + str(cart2sph(*camera_end)))\n",
    "\n",
    "        if (inner_radius <= np.linalg.norm(camera_end) <= outer_radius and\n",
    "            camera_end[2] > z_offset_min and camera_end[2] < z_offset_max):\n",
    "            # return camera_start, camera_end\n",
    "            break\n",
    "    \n",
    "    camera_positions = []\n",
    "\n",
    "    for frame in range(frames):\n",
    "        # linear interpolate lambda and phi angles, arc length not preserved\n",
    "        interp = (frame * 1.0) / frames\n",
    "\n",
    "        lambda1, phi1, r = cart2sph(*camera_start)\n",
    "        lambda2, phi2, r = cart2sph(*camera_end)\n",
    "\n",
    "        # angles are hacks\n",
    "        phi_interp = phi1 + interp*(phi2-phi1)\n",
    "        lambda_interp = lambda1 + interp*(lambda2-lambda1)\n",
    "\n",
    "        camera_positions.append(sph2cart(lambda_interp, phi_interp, r))\n",
    "    \n",
    "    return camera_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera start spherical: (0.8818168874962262, 0.5837006085865413, 1.4522511806183331)\n",
      "camera end spherical: (2.627531573991499, 1.561139088918255, 1.4522511806183334)\n",
      "camera start spherical: (0.8275364423854072, 0.5417016766200865, 1.4748556125311647)\n",
      "camera end spherical: (2.0687754060963206, 0.7952232357731807, 1.4748556125311647)\n",
      "camera start spherical: (0.7458156776835946, 0.6804105894880996, 1.4390357388821051)\n",
      "camera end spherical: (2.1176923551502123, 0.752424406356249, 1.4390357388821051)\n",
      "camera start spherical: (0.8990955221758858, 0.5774551236678905, 1.492415412567042)\n",
      "camera end spherical: (2.1756235806013002, 0.8569314113486358, 1.4924154125670421)\n",
      "camera start spherical: (0.7381099915836054, 0.6363950160183693, 1.489247697498801)\n",
      "camera end spherical: (2.0328690240587735, 0.7662421704173467, 1.489247697498801)\n",
      "camera start spherical: (0.8383103832656439, 0.6474744166068999, 1.4847831913467897)\n",
      "camera end spherical: (2.09728319379078, 0.6740347510851519, 1.48478319134679)\n",
      "camera start spherical: (0.8032959149474951, 0.6631912829199049, 1.4452734427730773)\n",
      "camera end spherical: (2.5143363805644885, 1.237420482062824, 1.4452734427730773)\n",
      "camera start spherical: (0.8460920346402644, 0.6245943677684949, 1.4248350940736627)\n",
      "camera end spherical: (2.224964892669453, 0.8220372551097868, 1.424835094073663)\n",
      "camera start spherical: (0.7258046350965754, 0.6101618019685882, 1.4134628634336355)\n",
      "camera end spherical: (2.5408881345363925, 1.3859959958575268, 1.4134628634336357)\n",
      "camera start spherical: (0.7896934695274253, 0.7030495529474343, 1.4375627487249845)\n",
      "camera end spherical: (2.048016851298648, 0.5578454390840976, 1.4375627487249842)\n",
      "camera start spherical: (0.7352512540928363, 0.657984741489984, 1.4692976051784994)\n",
      "camera end spherical: (2.9504727663787804, 1.4674665648045948, 1.4692976051784992)\n",
      "camera start spherical: (0.8691187447450488, 0.6155004357553014, 1.4967865335943018)\n",
      "camera end spherical: (2.4188468108298515, 1.4293114717721451, 1.4967865335943016)\n",
      "camera start spherical: (0.7800502361591819, 0.6130440251993039, 1.4396291670812105)\n",
      "camera end spherical: (2.486575222111309, 1.3731418130820052, 1.4396291670812105)\n",
      "camera start spherical: (0.8006494689876343, 0.5697031588770153, 1.4152503090758157)\n",
      "camera end spherical: (1.8863490952699018, 0.2836506131562051, 1.4152503090758162)\n",
      "camera start spherical: (0.8191284737733756, 0.6000361093074248, 1.468127781398546)\n",
      "camera end spherical: (2.3225036740724474, 1.2961516167692293, 1.4681277813985463)\n",
      "camera start spherical: (0.8833023282662803, 0.5770922528974067, 1.4205562415129434)\n",
      "camera end spherical: (1.8095570359912843, 0.05184439689399289, 1.420556241512944)\n"
     ]
    }
   ],
   "source": [
    "max_camera_movement = 1.5\n",
    "\n",
    "\n",
    "# interpolate the camera position between these two points\n",
    "# while keeping it focused on the center of chosen object\n",
    "\n",
    "positions = []\n",
    "orientations = []\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "# look_ind = rng.choice(len(translated_obj_poses))\n",
    "\n",
    "look_ind = np.argmax(np.array(heights))\n",
    "look_point = translated_obj_poses[look_ind][0:3,3] # look at the center of the tallest object\n",
    "\n",
    "\n",
    "camera_trajectory_pos = get_spherical_camera_motion(\n",
    "    movement_speed=rng.uniform(low=max_camera_movement/2.0, high=max_camera_movement), # low was 0\n",
    "    inner_radius=0.75,\n",
    "    outer_radius=1.5, # is outer radius too limiting?\n",
    "    z_offset_min=0.1,\n",
    "    z_offset_max=0.4,\n",
    "    frames=frames,\n",
    "    rng_sphere=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_poses = []\n",
    "up = np.array([0,0,1])\n",
    "\n",
    "for pos in camera_trajectory_pos:\n",
    "    camera_poses.append(b.t3d.transform_from_pos_target_up(np.array(pos), look_point, up))\n",
    "\n",
    "cam_poses = jnp.array(camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiframe_poses = []\n",
    "\n",
    "for c in range(len(cam_poses)):\n",
    "    frame_poses = []\n",
    "    for p in range(len(translated_obj_poses)):\n",
    "        frame_poses.append(b.t3d.inverse_pose(cam_poses[c]) @ translated_obj_poses[p])\n",
    "    multiframe_poses.append(frame_poses)\n",
    "\n",
    "multiframe_poses = jnp.array(multiframe_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_indices = [0,1,2,3,4,5]\n",
    "# depth_im = b.RENDERER.render_many(multiframe_poses[:,render_indices,:,:], jnp.array(render_indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Point Light Renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_subsample_proportion = 25\n",
    "dots = 250 #500\n",
    "lifetime = 5 #keep 1-1/5 of the dots after every frame update\n",
    "point_rad = 5\n",
    "\n",
    "# Subsample dots in point cloud\n",
    "choices = rng.choice(np.arange(len(scene_pc_shift)), size = len(scene_pc_shift)//scene_subsample_proportion, replace=False)\n",
    "scene_pc_subsample = scene_pc_shift[choices,:]\n",
    "scene_pc_table_shift = np.concatenate((scene_pc_subsample, b.RENDERER.meshes[-2].vertices), axis=0)\n",
    "\n",
    "# Resample fraction of dots at each frame in video according to lifetime\n",
    "\n",
    "pc = scene_pc_table_shift\n",
    "pc_subsample_start = pc[jax.random.choice(jax.random.PRNGKey(10), jnp.arange(pc.shape[0]), shape=(dots,) )] #want 1000 dots total\n",
    "pc_replacements = pc[jax.random.choice(jax.random.PRNGKey(0), jnp.arange(pc.shape[0]), shape=(frames,dots//lifetime) )]\n",
    "\n",
    "pc_subsamples = jnp.zeros((frames,*pc_subsample_start.shape))\n",
    "pc_subsamples = pc_subsamples.at[0,...].set(pc_subsample_start)\n",
    "\n",
    "for i in range(1,frames):\n",
    "    pc_subsamples = pc_subsamples.at[i,...].set(pc_subsamples[i-1,...])\n",
    "    sampled_indices = jax.random.choice(jax.random.PRNGKey(i), jnp.arange(dots), shape=(dots//lifetime,) )\n",
    "    pc_subsamples = pc_subsamples.at[i,sampled_indices,...].set(pc_replacements[i,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utlity and rendering functions\n",
    "\n",
    "def circles(flips_xy, radius):\n",
    "    centers = jnp.array((flips_xy>0).nonzero(size=5000,fill_value=jnp.inf))\n",
    "    x,y = jnp.meshgrid(jnp.arange(flips_xy.shape[1]),jnp.arange(flips_xy.shape[0]))\n",
    "    xymesh = jnp.array([y,x])\n",
    "    distances_to_keypoints = (\n",
    "        jnp.linalg.norm(xymesh[:, :,:,None] - centers[:,None, None,:],\n",
    "        axis=0\n",
    "    ))\n",
    "    index_of_nearest_keypoint = distances_to_keypoints.argmin(2)\n",
    "    distance_to_nearest_keypoints = distances_to_keypoints.min(2)\n",
    "    DISTANCE_THRESHOLD = radius\n",
    "    valid_match_mask = (distance_to_nearest_keypoints < DISTANCE_THRESHOLD)[...,None]\n",
    "    return valid_match_mask\n",
    "\n",
    "def render_point_light(poses, cam_pose, pc_to_render, key):\n",
    "    pc_in_camera_frame = b.t3d.apply_transform(pc_to_render, b.t3d.inverse_pose(cam_pose))\n",
    "    img = b.render_point_cloud(pc_in_camera_frame, intrinsics)\n",
    "\n",
    "    # idx needs to be less hacky\n",
    "    rendered_image = point_cloud_img = b.RENDERER.render(poses,  jnp.array([0,1,2,3,4,5]))[:,:,:3] # this needs to be extended to multiple objects\n",
    "\n",
    "    \n",
    "    mask = (rendered_image[:,:,2] < intrinsics.far)\n",
    "    \n",
    "    matches = (jnp.abs(img[:,:,2] - rendered_image[:,:,2]) < 0.05)\n",
    "    #matches = (jnp.abs(img[:,:,2] - rendered_image[:,:,2]) < 0.5) ???\n",
    "    \n",
    "    #turn down the flips to make less noisy\n",
    "    flips = (jax.random.uniform(key,shape=matches.shape) < 0.0005)\n",
    "    \n",
    "    final_no_noise = circles(mask * matches,point_rad)\n",
    "    final_with_noise = circles(mask * matches + (1.0 - mask) * flips, point_rad)\n",
    "\n",
    "    return final_no_noise, final_with_noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test block to check correctness of depth and point cloud renders\n",
    "# im = b.RENDERER.render(multiframe_poses[0][0:6],  jnp.array([0,1,2,3,4,5]))[:,:,:3]\n",
    "# pcim = b.render_point_cloud(b.t3d.apply_transform(pc_subsamples[0], b.t3d.inverse_pose(cam_poses[0])), intrinsics, pixel_smudge=5)\n",
    "# plt.imshow(pcim[:,:,2])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(100)\n",
    "keys = jax.random.split(key, multiframe_poses.shape[0])\n",
    "\n",
    "# last item in mesh list is photorealistic wood block, can't be used for point light stimulus\n",
    "b.RENDERER.meshes = b.RENDERER.meshes[:6] \n",
    "\n",
    "render_point_light_parallel_jit = jax.jit(jax.vmap(render_point_light, in_axes=(0,0,0,0)))\n",
    "images_no_noise, images = render_point_light_parallel_jit(multiframe_poses[:,:6,:,:], cam_poses, pc_subsamples, keys)\n",
    "\n",
    "# write video to GIF\n",
    "\n",
    "stim_path = './stimuli/scene'+str(scene_ind).zfill(2)\n",
    "stim_gt_path = './stimuli_gt/scene'+str(scene_ind).zfill(2)\n",
    "\n",
    "# if not os.path.exists(stim_path):\n",
    "#     os.mkdir(stim_path)\n",
    "\n",
    "if not os.path.exists(stim_gt_path):\n",
    "    os.makedirs(stim_gt_path)\n",
    "    \n",
    "\n",
    "# viz = [b.get_depth_image(1.0 - point_light_image * 1.0, cmap=matplotlib.colormaps['Greys']) for point_light_image in images ]\n",
    "# b.make_gif_from_pil_images(viz, stim_path+\"/vec\"+str(s).zfill(3)+\".gif\")\n",
    "# # viz[0].save('out_frame_m.png')\n",
    "# # viz = [b.get_depth_image(1.0 - point_light_image * 1.0, cmap=matplotlib.colormaps['Greys']) for point_light_image in images_no_noise ]\n",
    "# # b.make_gif_from_pil_images(viz, \"./stimuli/obj\"+str(IDX).zfill(2)+\"vec\"+str(s).zfill(3)+\"out_clean.gif\")\n",
    "\n",
    "traj = 1\n",
    "fps = 8\n",
    "\n",
    "static = jnp.repeat(images[0,...][jnp.newaxis,...], frames, axis=0)\n",
    "viz = [b.get_depth_image(1.0 - point_light_image * 1.0, cmap=matplotlib.colormaps['Greys']) for point_light_image in jnp.concatenate((static, images_no_noise, images),axis=2)]\n",
    "b.make_gif_from_pil_images(viz, stim_gt_path+\"/vec\"+str(traj).zfill(3)+\".gif\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/004_sugar_box/textured.obj', '/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/007_tuna_fish_can/textured.obj', '/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/010_potted_meat_can/textured.obj', '/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/024_bowl/textured.obj', '/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/036_wood_block/textured.obj']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mesh_paths)\n",
    "len(mesh_paths)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface to photorealistic renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes3d._rendering.photorealistic_renderers.kubric_interface_background import render_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      "/data/vision/billf/scratch/esli/bayes3d/bayes3d/_rendering\n",
      "sudo docker run --rm --interactive --user $(id -u):$(id -g) --volume /data/vision/billf/scratch/esli/bayes3d/bayes3d/_rendering:/data/vision/billf/scratch/esli/bayes3d/bayes3d/_rendering --volume /tmp:/tmp   --volume /data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/003_cracker_box:/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/003_cracker_box  --volume /data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/004_sugar_box:/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/004_sugar_box  --volume /data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/021_bleach_cleanser:/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/021_bleach_cleanser  --volume /data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/035_power_drill:/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/035_power_drill  --volume /data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/051_large_clamp:/data/vision/billf/scratch/esli/bayes3d/assets/ycb_video_models/models/051_large_clamp  kubricdockerhub/kubruntu /usr/bin/python3 /data/vision/billf/scratch/esli/bayes3d/bayes3d/_rendering/photorealistic_renderers/_kubric_exec_parallel_nishad.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\n",
      "sudo: a password is required\n"
     ]
    }
   ],
   "source": [
    "kubric_pose_idx = [0,1,2,3,4,6] # this is always the number of objects in scene plus last element in mesh list\n",
    "rgbds = render_many(mesh_paths, multiframe_poses[:,kubric_pose_idx,:,:], intrinsics, tabletop_scaling_factor = tabletop_scaling_factor)\n",
    "\n",
    "vid = 1\n",
    "im_dir = \"ku_scene_vids_linear_\"+str(scene_ind)+\"/frames\"+str(vid)+\"/images\"\n",
    "\n",
    "if not os.path.exists(im_dir):\n",
    "    os.makedirs(im_dir)\n",
    "\n",
    "for frame, rgbd in enumerate(rgbds):\n",
    "    b.get_rgb_image(rgbd.rgb).save(im_dir+\"/image{:03d}.png\".format(frame))\n",
    "\n",
    "\n",
    "p2a.save(image_folder = im_dir, video_name = 'ku_scene_vids_linear_'+str(scene_ind)+'/linear'+str(vid)+'.avi', fps=fps)\n",
    "\n",
    "# write camera position and quaternions\n",
    "\n",
    "cp = []\n",
    "co = []\n",
    "\n",
    "for c_ind in range(len(cam_poses)):\n",
    "    cp.append(np.array(cam_poses[c_ind,:3,3]))\n",
    "    co.append(np.array(b.t3d.rotation_matrix_to_quaternion(cam_poses[c_ind,:3,:3])))\n",
    "\n",
    "gt_poses = np.concatenate((np.array(cp), np.array(co)),axis=1)\n",
    "np.savetxt('ku_scene_vids_linear_'+str(scene_ind)+'/cam_pos_ori'+str(vid)+'.txt', gt_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth poses seem to come from manual optimization, which explains why there are weird offsets in object poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
