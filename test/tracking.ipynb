{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arijit's Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jun 12 2023 15:17:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kschuang/Documents/mcs/bayes3d/test this is the current working directory\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import bayes3d as b\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle\n",
    "import PIL.Image\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "print(os.getcwd(), \"this is the current working directory\")\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "    height, width, _ = frames[0].shape\n",
    "    dpi = 70\n",
    "    orig_backend = matplotlib.get_backend()\n",
    "    matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "    matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "    ax.set_axis_off()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    im = ax.imshow(frames[0])\n",
    "    def update(frame):\n",
    "      im.set_data(frame)\n",
    "      return [im]\n",
    "    interval = 1000/framerate\n",
    "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                   interval=interval, blit=True, repeat=True)\n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "def object_pose_in_camera_frame(object_id, view_matrix):\n",
    "    object_pos, object_orn = p.getBasePositionAndOrientation(object_id) # world frame\n",
    "    world2cam = np.array(view_matrix).reshape([4,4]).T # world --> cam \n",
    "    object_transform_matrix = np.eye(4)\n",
    "    object_transform_matrix[:3, :3] = np.reshape(p.getMatrixFromQuaternion(object_orn), (3, 3))\n",
    "    object_transform_matrix[:3, 3] = object_pos\n",
    "    return world2cam @ object_transform_matrix\n",
    "\n",
    "def get_camera_pose(view_matrix):\n",
    "    # cam2world\n",
    "    world2cam = np.array(view_matrix).reshape([4,4]).T\n",
    "    cam2world  = np.linalg.inv(world2cam)\n",
    "    return cam2world\n",
    "\n",
    "def object_pose_in_camera_frame(object_pose, view_matrix):\n",
    "    world2cam = np.array(view_matrix).reshape([4,4]).T # world --> cam \n",
    "    return world2cam @ object_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pybullet Scene Sim Recreating Arijit's Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  1.11022302e-16 -1.00000000e+00]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.11022302e-16]]\n"
     ]
    }
   ],
   "source": [
    "import bayes3d.pybullet_sim as pyb \n",
    "\n",
    "scene = pyb.Scene()\n",
    "scene.set_gravity([0, 0, -9.8])\n",
    "scene.set_timestep(1/240)\n",
    "scene.set_downsampling(4) \n",
    "\n",
    "occ1_meshscale = [0.0667,0.0667,0.0667]\n",
    "path_to_obj = \"../assets/sample_objs/plane.obj\"\n",
    "base_position = [1,-1,1]\n",
    "base_orientation = [0.7071068, 0, 0, 0.7071068]\n",
    "base_orientation = np.array(p.getMatrixFromQuaternion(base_orientation)).reshape(3,3)\n",
    "occ1 = pyb.make_body_from_obj(path_to_obj, base_position, orientation=base_orientation,scale = occ1_meshscale, id = \"occluder\")\n",
    "occ1.set_mass(0)\n",
    "scene.add_body(occ1)\n",
    "\n",
    "box_mass = 1\n",
    "path_to_box = \"../assets/sample_objs/cube.obj\"\n",
    "box_position = [-3.25, 0, 0.501]\n",
    "box_start_velocity = [6, 0, 6]\n",
    "mesh_scale = [0.5,0.5,0.5]\n",
    "box = pyb.make_body_from_obj(path_to_box, box_position, scale = mesh_scale, restitution=1, id = \"box\")\n",
    "box.set_velocity(box_start_velocity)\n",
    "scene.add_body(box)\n",
    "\n",
    "bull = scene.simulate(360)\n",
    "bull.create_gif(\"tracking.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "box_poses = bull.get_body_poses()[\"box\"]\n",
    "plane_pos, plane_ori = bull.floor_pos_ori\n",
    "plane_ori = p.getMatrixFromQuaternion(plane_ori)\n",
    "plane_cam_pose = np.eye(4)\n",
    "plane_cam_pose[:3, 3] = plane_pos\n",
    "plane_cam_pose[:3, :3] = np.array(plane_ori).reshape(3,3)\n",
    "view_matrix = bull.viewMatrix \n",
    "view_matrix = np.array(view_matrix).reshape([4,4]).T\n",
    "box_poses = [pose @ view_matrix for pose in box_poses]\n",
    "plane_cam_poses = plane_cam_pose @ view_matrix\n",
    "occ1_pose = bull.get_body_poses()['occluder'][0] @ view_matrix\n",
    "cam_pose = get_camera_pose(view_matrix) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Information for Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dict = {\n",
    "    'box': box_poses, # box poses in camera view \n",
    "    'plane': plane_cam_pose, # plane pose in camera view \n",
    "    'occ1' : occ1_pose, # occluder pose in camera view\n",
    "    'occ1_meshscale' : occ1_meshscale,\n",
    "    'cam_pose' : cam_pose, # camera pose in world view\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[ 0.99999994  0.          0.          1.        ]\n",
      " [ 0.          0.87789559 -0.47885215  5.26498222]\n",
      " [ 0.          0.47885212  0.87789553  1.00000018]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[[ 9.99999940e-01  0.00000000e+00  0.00000000e+00 -3.25000000e+00]\n",
      " [ 0.00000000e+00  4.78852123e-01  8.77895534e-01  1.78813934e-07]\n",
      " [ 0.00000000e+00 -8.77895594e-01  4.78852153e-01 -5.76398222e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(box_poses[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayes3d as b\n",
    "import bayes3d.transforms_3d as t3d\n",
    "import numpy as np \n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "from jax.debug import print as jprint\n",
    "import physics_priors as pp\n",
    "import importlib\n",
    "importlib.reload(pp)\n",
    "import time\n",
    "import PIL.Image\n",
    "from math import sqrt\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m intrinsics \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mIntrinsics(\n\u001b[1;32m      2\u001b[0m     height\u001b[39m=\u001b[39m\u001b[39m360\u001b[39m,\n\u001b[1;32m      3\u001b[0m     width\u001b[39m=\u001b[39m\u001b[39m480\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     near\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, far\u001b[39m=\u001b[39m\u001b[39m10.0\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m b\u001b[39m.\u001b[39;49msetup_renderer(intrinsics)\n",
      "File \u001b[0;32m~/Documents/mcs/bayes3d/bayes3d/renderer.py:321\u001b[0m, in \u001b[0;36msetup_renderer\u001b[0;34m(intrinsics, num_layers)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_renderer\u001b[39m(intrinsics, num_layers\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[0;32m--> 321\u001b[0m     b\u001b[39m.\u001b[39mRENDERER \u001b[39m=\u001b[39m _Renderer(intrinsics, num_layers\u001b[39m=\u001b[39;49mnum_layers)\n",
      "File \u001b[0;32m~/Documents/mcs/bayes3d/bayes3d/renderer.py:327\u001b[0m, in \u001b[0;36m_Renderer.__init__\u001b[0;34m(self, intrinsics, num_layers)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, intrinsics, num_layers\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintrinsics \u001b[39m=\u001b[39m intrinsics\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrenderer_env \u001b[39m=\u001b[39m dr\u001b[39m.\u001b[39;49mRasterizeGLContext(intrinsics\u001b[39m.\u001b[39;49mheight, intrinsics\u001b[39m.\u001b[39;49mwidth, output_db\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(bayes3d\u001b[39m.\u001b[39mcamera\u001b[39m.\u001b[39mopen_gl_projection_matrix(\n\u001b[1;32m    329\u001b[0m         intrinsics\u001b[39m.\u001b[39mheight, intrinsics\u001b[39m.\u001b[39mwidth, \n\u001b[1;32m    330\u001b[0m         intrinsics\u001b[39m.\u001b[39mfx, intrinsics\u001b[39m.\u001b[39mfy, \n\u001b[1;32m    331\u001b[0m         intrinsics\u001b[39m.\u001b[39mcx, intrinsics\u001b[39m.\u001b[39mcy, \n\u001b[1;32m    332\u001b[0m         intrinsics\u001b[39m.\u001b[39mnear, intrinsics\u001b[39m.\u001b[39mfar\n\u001b[1;32m    333\u001b[0m     )\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    335\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m CUSTOM_CALLS:\n",
      "File \u001b[0;32m~/Documents/mcs/bayes3d/bayes3d/nvdiffrast/common/ops.py:183\u001b[0m, in \u001b[0;36mRasterizeGLContext.__init__\u001b[0;34m(self, height, width, output_db, mode, device)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     cuda_device_idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device()\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/bayes3d/lib/python3.8/site-packages/torch/cuda/__init__.py:674\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcurrent_device\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    673\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     _lazy_init()\n\u001b[1;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m~/miniconda3/envs/bayes3d/lib/python3.8/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "intrinsics = b.Intrinsics(\n",
    "    height=360,\n",
    "    width=480,\n",
    "    fx=180*sqrt(3), fy=180*sqrt(3),\n",
    "    cx=240.0, cy=180.0,\n",
    "    near=0.1, far=10.0\n",
    ")\n",
    "b.setup_renderer(intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_poses = np.load('scene_npzs/scene_demo.npz')\n",
    "# Find number of timesteps\n",
    "N_tsteps = loaded_poses['box'].shape[0]\n",
    "\n",
    "# load occluder poses into (N,4,4)\n",
    "occ1_meshscale = loaded_poses['occ1_meshscale']\n",
    "# occ1_meshscale = [0.0667,0.0667,0.0667]\n",
    "occ1_pose = loaded_poses['occ1']\n",
    "occ1_pose[1:3] *= -1\n",
    "occ1_poses = jnp.tile(jnp.array(occ1_pose), (N_tsteps,1,1))\n",
    "\n",
    "# occ2_meshscale = loaded_poses['occ2_meshscale']\n",
    "# occ2_pose = loaded_poses['occ2']\n",
    "# occ2_pose[1:3] *= -1\n",
    "# occ2_poses = jnp.tile(jnp.array(occ2_pose), (N_tsteps,1,1))\n",
    "\n",
    "# get object poses\n",
    "gt_poses = loaded_poses['box']\n",
    "gt_poses[:,1:3,:] *= -1 # CV2 convention\n",
    "gt_poses = jnp.array(gt_poses)\n",
    "\n",
    "# combine the poses\n",
    "# total_gt_poses = jnp.stack([gt_poses, occ1_poses, occ2_poses], axis = 1)\n",
    "total_gt_poses = jnp.stack([gt_poses, occ1_poses], axis = 1)\n",
    "\n",
    "# cv2 convention of cam pose\n",
    "world2cam = np.linalg.inv(loaded_poses['cam_pose'])\n",
    "world2cam[1:3] *= -1\n",
    "cam_pose = jnp.linalg.inv(jnp.array(world2cam)) \n",
    "\n",
    "b.RENDERER.add_mesh_from_file(\"../assets/sample_objs/cube.obj\",mesh_name=\"cube_0\", scaling_factor=[0.5,0.5,0.5])\n",
    "b.RENDERER.add_mesh_from_file(\"../assets/sample_objs/plane.obj\",mesh_name=\"occluder1\", scaling_factor=occ1_meshscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_images = jax.vmap(b.RENDERER.render_single_object, in_axes=(0, None))(gt_poses, jnp.int32(0))\n",
    "pose_estimate = gt_poses[0]\n",
    "\n",
    "# gt_images = b.RENDERER.render_multiobject_parallel(total_gt_poses, [0,1,2])\n",
    "\n",
    "gt_images = b.RENDERER.render_many(total_gt_poses,jnp.array([0,1]))\n",
    "\n",
    "# gt_images = jax.vmap(b.RENDERER.render, in_axes=(0, None))(gt_poses[:,None, ...], jnp.array([0]))\n",
    "depths = [b.viz.get_depth_image(gt_images[i,:,:,2]) for i in range(gt_images.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dy, dz = 0.2, 0.2, 0.2\n",
    "translation_deltas = b.utils.make_translation_grid_enumeration(-dx, -dy, -dz, dx, dy, dz, 5,5,5)\n",
    "\n",
    "\n",
    "dx, dy, dz = 2, 2, 2\n",
    "gridding = [\n",
    "    b.utils.make_translation_grid_enumeration(\n",
    "        -dx, -dy, -dz, dx, dy, dz, 5,5,5\n",
    "    ),\n",
    "    b.utils.make_translation_grid_enumeration(\n",
    "        -dx/2.0, -dy/2, -dz/2, dx/2, dy/2, dz/2, 5,5,5\n",
    "    ),\n",
    "    b.utils.make_translation_grid_enumeration(\n",
    "        -dx/10.0, -dy/10, -dz/10, dx/10, dy/10, dz/10, 5,5,5\n",
    "    ),\n",
    "]\n",
    "\n",
    "key = jax.random.PRNGKey(314)\n",
    "rotation_deltas = jax.vmap(lambda key: b.distributions.gaussian_vmf_zero_mean(key, 0.00001, 800.0))(\n",
    "    jax.random.split(key, 100)\n",
    ")\n",
    "\n",
    "# len_proposals = gridding[0].shape[0]\n",
    "len_proposals = translation_deltas.shape[0]\n",
    "\n",
    "occ1_poses_trans = jnp.tile(occ1_pose, (len_proposals, 1, 1))\n",
    "# occ2_poses_trans = jnp.tile(occ2_pose, (len_proposals, 1, 1))\n",
    "\n",
    "occ1_poses_rot = jnp.tile(occ1_pose, (rotation_deltas.shape[0], 1, 1))\n",
    "# occ2_poses_rot = jnp.tile(occ2_pose, (rotation_deltas.shape[0], 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pose_estimate(memory, gt_image):\n",
    "\n",
    "    pose_memory, T = memory\n",
    "    old_pose_estimate = pose_memory[T,...]\n",
    "    prev_pose = pose_memory[T-1,...]\n",
    "\n",
    "    threedp3_weight = 1\n",
    "    proposals = jnp.einsum(\"ij,ajk->aik\", old_pose_estimate, translation_deltas)\n",
    "    rendered_images = b.RENDERER.render_many(jnp.stack([proposals, occ1_poses_trans],axis = 1), jnp.array([0,1]))\n",
    "    threedp3_scores = threedp3_weight * b.threedp3_likelihood_parallel(gt_image, rendered_images, 0.001, 0.1, 10**3, 3)\n",
    "    unique_best_3dp3_score = jnp.sum(threedp3_scores == threedp3_scores[jnp.argmax(threedp3_scores)]) == 1\n",
    "\n",
    "    physics_weight = jax.lax.cond(unique_best_3dp3_score, lambda _ : 5000, lambda _ : 10000, None)\n",
    "    # physics_weight = 0\n",
    "\n",
    "    physics_estimated_pose = p.physics_prior_v1_jit(old_pose_estimate, prev_pose, jnp.array([1,1,1]), cam_pose, world2cam)\n",
    "\n",
    "    physics_scores = jax.lax.cond(jnp.greater(T, 1), \n",
    "    lambda _ : physics_weight * p.physics_prior_parallel_jit(proposals, physics_estimated_pose), \n",
    "    lambda _ : jnp.zeros(threedp3_scores.shape[0]), \n",
    "    None)\n",
    "\n",
    "    scores = threedp3_scores + physics_scores\n",
    "\n",
    "    pose_estimate = proposals[jnp.argmax(scores)]\n",
    "    pose_memory = pose_memory.at[T+1,...].set(pose_estimate)\n",
    "\n",
    "    pose_world = cam_pose @ pose_estimate\n",
    "    gt_pose_world = cam_pose @ gt_poses[T-1]\n",
    "    jprint(\"{}: {}, {}\", T, unique_best_3dp3_score, pose_world[:3,3])\n",
    "    jprint(\"{}: GT, {}\\n\", T, gt_pose_world[:3,3])\n",
    "\n",
    "    # proposals = jnp.einsum(\"ij,ajk->aik\", pose_estimate, rotation_deltas)\n",
    "    # rendered_images = b.RENDERER.render_multiobject_parallel(jnp.stack([proposals, occ_poses_rot]), jnp.array([0,1]))\n",
    "    # # rendered_images = jax.vmap(b.RENDERER.render_single_object, in_axes=(0, None))(proposals, jnp.int32(0))\n",
    "    # weights_new = b.threedp3_likelihood_parallel(gt_image, rendered_images, 0.05, 0.1, 10**3, 3)\n",
    "    # pose_estimate = proposals[jnp.argmax(weights_new)]\n",
    "\n",
    "    return (pose_memory, T+1), pose_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(p)\n",
    "inference_program = jax.jit(lambda p,x: jax.lax.scan(\n",
    "    update_pose_estimate, \n",
    "    (jnp.tile(p, (x.shape[0]+1,1,1)),1),\n",
    "    x)[1])\n",
    "\n",
    "start = time.time()\n",
    "inferred_poses = inference_program(gt_poses[0], gt_images)\n",
    "end = time.time()\n",
    "print (\"Time elapsed:\", end - start)\n",
    "print (\"FPS:\", gt_poses.shape[0] / (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_images = []\n",
    "max_depth = 10.0\n",
    "\n",
    "# inferred_poses_with_occ = jnp.stack([inferred_poses, occ1_poses], axis = 1)\n",
    "occ_image = b.viz.get_depth_image(b.RENDERER.render(occ1_pose[None,...], jnp.array([1]))[:,:,2])\n",
    "\n",
    "pred_images = b.RENDERER.render_many(inferred_poses[:,None, ...], jnp.array([0]))\n",
    "\n",
    "pred_with_occ_images = [b.overlay_image(b.viz.get_depth_image(pred_images[i,:,:,2]), \n",
    "occ_image, alpha=0.4) for i in range(pred_images.shape[0])]\n",
    "\n",
    "gt_images = b.RENDERER.render_many(gt_poses[:,None, ...], jnp.array([0]))\n",
    "gt_with_occ_images = [b.overlay_image(b.viz.get_depth_image(gt_images[i,:,:,2]), \n",
    "occ_image, alpha=0.5) for i in range(pred_images.shape[0])]\n",
    "\n",
    "viz_images = [\n",
    "    b.viz.multi_panel(\n",
    "        [g, b.viz.get_depth_image(p[:,:,2]), po],\n",
    "        labels = [\"Ground Truth\", \"Reconstruction w/o Occluder\", \"Reconstruction w Occluder\"],\n",
    "        title = \"Scene 11A\",\n",
    "        # bottom_text = \"3DP3 + Physics Prior v1\"\n",
    "    )\n",
    "    for (g, p, po) in zip(gt_with_occ_images, pred_images, pred_with_occ_images)\n",
    "]\n",
    "display_video(viz_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gif(images, filename, fps = 10):\n",
    "    duration = int(1000/fps)\n",
    "    images[0].save(\n",
    "        fp=filename,\n",
    "        format=\"GIF\",\n",
    "        append_images=images,\n",
    "        save_all=True,\n",
    "        duration=duration,\n",
    "        loop=0,\n",
    "    )\n",
    "\n",
    "make_gif(viz_images, \"scene_gifs/scene_demo.gif\", fps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
