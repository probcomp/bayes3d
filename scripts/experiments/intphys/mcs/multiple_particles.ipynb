{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jax\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import genjax\n",
    "import bayes3d as b\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../\")\n",
    "from viz import *\n",
    "from utils import *\n",
    "from mcs_utils import *\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import bayes3d.transforms_3d as t3d\n",
    "from jax.debug import print as jprint\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from genjax._src.core.pytree.utilities import *\n",
    "from genjax.generative_functions.distributions import ExactDensity\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "import jax.tree_util as jtu\n",
    "from genjax._src.core.transforms.incremental import NoChange, UnknownChange, Diff\n",
    "console = genjax.pretty()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and preprocessing all data and renderer\n",
    "SCALE = 0.2\n",
    "cam_pose = CAM_POSE_CV2\n",
    "inverse_cam_pose = jnp.linalg.inv(CAM_POSE_CV2)\n",
    "scene_name = 'passive_physics_validation_object_permanence_0001_02'\n",
    "# with open(f\"/home/arijitdasgupta/bayes3d/scripts/experiments/intphys/mcs/pickled_data/{scene_name}.pkl\", 'rb') as file:\n",
    "#     preprocessed_data = pickle.load(file)\n",
    "observations = load_observations_npz(scene_name)\n",
    "preprocessed_data = preprocess_mcs_physics_scene(observations, MIN_DIST_THRESH=0.6, scale=SCALE)\n",
    "(gt_images, gt_images_bg, gt_images_obj, intrinsics),\\\n",
    "(gt_images_orig, gt_images_bg_orig, gt_images_obj_orig, intrinsics_orig),\\\n",
    "registered_objects, obj_pixels, is_gravity, poses = preprocessed_data\n",
    "\n",
    "# get obj indices padded\n",
    "all_obj_indices = [np.argwhere(gt_images_obj[i,...,2] != intrinsics.far) for i in range(gt_images.shape[0])]\n",
    "max_rows = max(obj_indices.shape[0] for obj_indices in all_obj_indices)\n",
    "def pad_array(array, max_rows):\n",
    "    padding = ((0, max_rows - array.shape[0]), (0, 0))  # Pad rows, not columns\n",
    "    return jnp.pad(array, padding, constant_values=-1)\n",
    "\n",
    "padded_all_obj_indices = jnp.stack([pad_array(array, max_rows) for array in all_obj_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Meshes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:10,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding new mesh for t = {} 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:00<00:02, 25.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding review\n",
      "Review passed, added to init queue\n",
      "Adding new mesh for t = {} 22\n",
      "Adding review\n",
      "Review passed, added to init queue\n",
      "Adding new mesh for t = {} 23\n",
      "Adding review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:01<00:02, 24.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review passed, added to init queue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting downsampled data\n"
     ]
    }
   ],
   "source": [
    "observations = np.load(\"../{}.npz\".format(77242958318),allow_pickle=True)[\"arr_0\"]\n",
    "SCALE =0.2\n",
    "preprocessed_data = preprocess_mcs_physics_scene(observations, MIN_DIST_THRESH=0.6, scale=SCALE)\n",
    "# with open(f\"/home/arijitdasgupta/bayes3d/scripts/experiments/intphys/mcs/pickled_data/{scene_ID}.pkl\", 'rb') as file:\n",
    "#     preprocessed_data = pickle.load(file)\n",
    "\n",
    "(gt_images, gt_images_bg, gt_images_obj, intrinsics),\\\n",
    "(gt_images_orig, gt_images_bg_orig, gt_images_obj_orig, intrinsics_orig),\\\n",
    "registered_objects, obj_pixels, is_gravity, poses, cam_pose = preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m't_init'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "        \u001b[32m'pose'\u001b[0m: \u001b[1;35mArray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       , \u001b[1;36m-1.3350611\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       , \u001b[1;36m-2.914764\u001b[0m \u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m7.8858347\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mfloat32\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m'full_pose'\u001b[0m: \u001b[1;35mArray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       , \u001b[1;36m-1.3347641\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       , \u001b[1;36m-2.6503453\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       ,  \u001b[1;36m7.8808002\u001b[0m\u001b[1m]\u001b[0m,\n",
       "       \u001b[1m[\u001b[0m \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m0\u001b[0m.       ,  \u001b[1;36m1\u001b[0m.       \u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mfloat32\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[32m't_full'\u001b[0m: \u001b[1;36m23\u001b[0m,\n",
       "        \u001b[32m'num_pixels'\u001b[0m: \u001b[1;36m1494\u001b[0m,\n",
       "        \u001b[32m'mesh'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;35mtrimesh.Trimesh\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mvertices.\u001b[0m\u001b[33mshape\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m332\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m, faces.\u001b[0m\u001b[33mshape\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m660\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1m>\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mPIL.Image.Image\u001b[0m\u001b[39m image \u001b[0m\u001b[33mmode\u001b[0m\u001b[39m=\u001b[0m\u001b[35mRGB\u001b[0m\u001b[39m \u001b[0m\u001b[33msize\u001b[0m\u001b[39m=\u001b[0m\u001b[35m60\u001b[0m\u001b[1;36m0x400\u001b[0m\u001b[39m at \u001b[0m\u001b[1;36m0x7FEC2C83D6A0\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.get_depth_image(gt_images_orig[-1,...,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravity_scene_plausible(poses, gt_images_obj_orig, gt_images_bg_orig, intrinsics, registered_objects):\n",
    "\n",
    "    # account for case where support is identified as an object\n",
    "    max_poses_detected = 0\n",
    "    for i in range(len(poses)):\n",
    "        if len(poses[i]) > max_poses_detected:\n",
    "            max_poses_detected = len(poses[i])\n",
    "    new_poses = [[] for _ in range(len(poses))]\n",
    "    if max_poses_detected > 2:\n",
    "        support_pose = poses[0][0]\n",
    "        for i in range(len(poses)):\n",
    "            for j in range(len(poses[i])):\n",
    "                diff = np.linalg.norm(support_pose[:3,3] - poses[i][j][:3,3])\n",
    "                if diff > 0.01:\n",
    "                    new_poses[i].append(poses[i][j])\n",
    "\n",
    "        poses = new_poses\n",
    "\n",
    "    idx = 0\n",
    "    while len(poses[idx]) < 2:\n",
    "        idx += 1\n",
    "    while len(poses[idx]) > 1:\n",
    "        idx += 1\n",
    "\n",
    "    first_checkpt = idx\n",
    "\n",
    "    while len(poses[idx]) < 2:\n",
    "        diff = np.linalg.norm(poses[idx+1][0][:3,3] - poses[idx][0][:3,3])\n",
    "        if diff == 0.0:\n",
    "            break\n",
    "        idx+=1\n",
    "\n",
    "    second_checkpt = idx\n",
    "\n",
    "    ref_pose = poses[idx][0]\n",
    "    ref_depth_obj = gt_images_obj_orig[idx,...,2]\n",
    "    ref_depth_bg = gt_images_bg_orig[idx,...,2]\n",
    "\n",
    "    if max_poses_detected > 2:\n",
    "        ref_depth_bg  = np.where(registered_objects[0]['mask'], ref_depth_obj, ref_depth_bg)\n",
    "        ref_depth_obj = np.where(registered_objects[0]['mask'], intrinsics.far, ref_depth_obj)\n",
    "\n",
    "    obj_indices = np.argwhere(ref_depth_obj != intrinsics.far)\n",
    "    bottom_i = np.max(obj_indices[:,0])\n",
    "\n",
    "    base_pixel_offset = 20\n",
    "    base_depth_delta_thresh = 1\n",
    "    line = ref_depth_bg[bottom_i+base_pixel_offset]\n",
    "    base_j_min = None\n",
    "    base_j_max = None\n",
    "    on_support = False\n",
    "    for j in range(len(line)-1):\n",
    "        if not on_support and line[j+1] < line[j] - base_depth_delta_thresh:\n",
    "            base_j_min = j\n",
    "            on_support = True\n",
    "        if on_support and line[j+1] > line[j] + base_depth_delta_thresh:\n",
    "            base_j_max = j\n",
    "            break\n",
    "        if j == len(line) - 2:\n",
    "            print(\"Error: There is no base for support\")\n",
    "            return True, [True for _ in range(len(poses))], 1e+20\n",
    "        \n",
    "    pixels_stable = np.sum(np.logical_and(obj_indices[:,1] <= base_j_max , obj_indices[:,1] >= base_j_min))\n",
    "    pixels_unstable = np.sum(np.logical_or(obj_indices[:,1] > base_j_max , obj_indices[:,1] < base_j_min))\n",
    "    stable = pixels_stable >= pixels_unstable\n",
    "    ref_height = (cam_pose @ ref_pose)[2,3]\n",
    "    end_height = (cam_pose @ poses[-1][0])[2,3]\n",
    "    # fell = ref_height > end_height + 0.2\n",
    "    fell = np.linalg.norm(ref_pose[:3,3] - poses[-1][0][:3,3]) > 0.1\n",
    "    perc = 100*(np.abs(pixels_unstable - pixels_stable)/(pixels_stable+pixels_unstable))\n",
    "\n",
    "    if perc < 5: # any outcome should be plausible\n",
    "        return True, [True for _ in range(len(poses))], 1e+20\n",
    "\n",
    "    t_violation = 1e+20\n",
    "    if stable and fell:\n",
    "        plausible = False\n",
    "        t_violation = second_checkpt\n",
    "    elif not stable and not fell:\n",
    "        plausible = False\n",
    "        t_violation = first_checkpt\n",
    "    else:\n",
    "        plausible = True\n",
    "\n",
    "    plausibility_list = []\n",
    "    for t in range(len(poses)):\n",
    "        if t >= t_violation:\n",
    "            plausibility_list.append(False)\n",
    "        else:\n",
    "            plausibility_list.append(True)\n",
    "\n",
    "            \n",
    "    return plausible, plausibility_list, t_violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plausible, plausibility_list, _  = gravity_scene_plausible(poses, gt_images_obj_orig, gt_images_bg_orig, intrinsics_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plausible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_depth_bg = gt_images_bg_orig[34,...,2]\n",
    "ref_depth_obj = gt_images_obj_orig[34,...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = np.where(registered_objects[0]['mask'], ref_depth_obj, ref_depth_bg)\n",
    "test2 = np.where(registered_objects[0]['mask'], intrinsics.far, ref_depth_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m150\u001b[0m.\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mfloat32\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test2[registered_objects[0]['mask']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.setup_renderer(intrinsics, num_layers= 1024)\n",
    "for i,registered_obj in enumerate(registered_objects):\n",
    "    b.RENDERER.add_mesh(registered_obj['mesh'])\n",
    "    # f_p = registered_objects[i][\"full_pose\"]\n",
    "    # registered_objects[i][\"full_pose\"] = f_p.at[2,3].set(f_p[2,3] + 0.5*b.RENDERER.model_box_dims[i][2])\n",
    "if len(registered_objects) == 0:\n",
    "    t_start = gt_images.shape[0]-100\n",
    "    registered_objects.append({'t_init' : gt_images.shape[0]-100,\n",
    "                            'pose' : jnp.eye(4).at[:3,3].set([0,0,1e+5]),\n",
    "                            'full_pose' : jnp.eye(4).at[:3,3].set([0,0,1e+5]),\n",
    "                            't_full' : gt_images.shape[0]-100})\n",
    "    b.RENDERER.add_mesh_from_file(os.path.join(b.utils.get_assets_dir(),\"sample_objs/cube.obj\"), scaling_factor = 0.1)\n",
    "else:\n",
    "    t_start = np.min([x[\"t_full\"] for x in registered_objects])\n",
    "# video_from_rendered(gt_images, scale = int(1/SCALE), framerate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model time!\n",
    "\n",
    "def get_height_bounds(i, world_pose):\n",
    "    # Half dimensions to get the corner points relative to the center\n",
    "    rotation_matrix = world_pose[:3,:3]\n",
    "    center = world_pose[:3,3]\n",
    "    dimensions = b.RENDERER.model_box_dims[i]\n",
    "    half_dims = dimensions / 2\n",
    "\n",
    "    # Local corner points of the box in its local coordinate system\n",
    "    local_corners = jnp.array([\n",
    "        [-half_dims[0], -half_dims[1], -half_dims[2]],  # Lower rear left corner\n",
    "        [ half_dims[0], -half_dims[1], -half_dims[2]],  # Lower rear right corner\n",
    "        [-half_dims[0],  half_dims[1], -half_dims[2]],  # Lower front left corner\n",
    "        [ half_dims[0],  half_dims[1], -half_dims[2]],  # Lower front right corner\n",
    "        [-half_dims[0], -half_dims[1],  half_dims[2]],  # Upper rear left corner\n",
    "        [ half_dims[0], -half_dims[1],  half_dims[2]],  # Upper rear right corner\n",
    "        [-half_dims[0],  half_dims[1],  half_dims[2]],  # Upper front left corner\n",
    "        [ half_dims[0],  half_dims[1],  half_dims[2]]   # Upper front right corner\n",
    "    ])\n",
    "\n",
    "    # Apply rotation to each corner point\n",
    "    global_corners = jnp.stack([center + rotation_matrix @ corner for corner in local_corners])\n",
    "\n",
    "    # Find the bottom-most point\n",
    "    bottom_most_point_z = jnp.min(global_corners[:,2])\n",
    "    top_most_point_z = jnp.max(global_corners[:,2])\n",
    "    # distance from centre of bbox to bottom of bbox\n",
    "    center_to_bottom_dist = center[2] - bottom_most_point_z\n",
    "    return bottom_most_point_z,top_most_point_z, center_to_bottom_dist\n",
    "\n",
    "def get_translation_direction(all_poses, t_full, t):\n",
    "    direction = all_poses[t-1][:3,3] - all_poses[t_full+1][:3,3]\n",
    "    direction = cam_pose[:3,:3] @ direction\n",
    "    direction_xy = direction.at[2].set(0)\n",
    "\n",
    "    normalized_direction_xy = jax.lax.cond(jnp.equal(jnp.linalg.norm(direction_xy), 0),\n",
    "                                         lambda: direction_xy,\n",
    "                                         lambda: direction_xy/jnp.linalg.norm(direction_xy))\n",
    "    return normalized_direction_xy\n",
    "\n",
    "\n",
    "# This model has to be recompiled for different # objects for now this is okay\n",
    "@genjax.gen\n",
    "def physics_stepper(all_poses, t, t_full, i, friction, gravity):\n",
    "    # TODO: SAMPLING FRICTION SCHEME --> can be of a hmm style\n",
    "\n",
    "    #################################################################\n",
    "    # First let us consider timestep t-1\n",
    "    #################################################################\n",
    "    # Step 2: find world pose\n",
    "    pose_prev = all_poses[t-1]\n",
    "    pose_prev_world = cam_pose @ pose_prev\n",
    "\n",
    "    # Step 3: check if we are already on the floor\n",
    "    bottom_z, top_z, center_to_bottom = get_height_bounds(i, pose_prev_world)\n",
    "    # within 20% of the object's height in world frame\n",
    "    already_on_floor = jnp.less_equal(bottom_z,0.2 * (top_z - bottom_z))\n",
    "    \n",
    "    # Step 1: Find world velocity\n",
    "    vel_pose_camera = jnp.linalg.solve(all_poses[t-2], all_poses[t-1])\n",
    "    pre_vel_xyz_world = cam_pose[:3,:3] @ vel_pose_camera[:3,3]\n",
    "    mag_xy = jnp.linalg.norm(pre_vel_xyz_world[:2])\n",
    "    \n",
    "    mag_xy_friction = mag_xy - friction * mag_xy\n",
    "\n",
    "    # mag_xy_friction = jax.lax.cond(\n",
    "    #     jnp.less_equal(jnp.abs(mag_xy_friction),3e-3),\n",
    "    #     lambda:0.0,\n",
    "    #     lambda:mag_xy_friction)\n",
    "    \n",
    "    mag_xy, gravity = jax.lax.cond(already_on_floor,lambda:(mag_xy_friction,gravity),lambda:(mag_xy, gravity))\n",
    "\n",
    "    dir_xy_world = get_translation_direction(all_poses, t_full, t)\n",
    "\n",
    "    # Step 7: Determine mag and gravity\n",
    "\n",
    "    vel_xyz_world = mag_xy * dir_xy_world\n",
    "    # Step 6: apply z axis change\n",
    "    vel_xyz_world = vel_xyz_world.at[2].set(pre_vel_xyz_world[2] - gravity * 1./20)\n",
    "\n",
    "    # Step 5: find peturbed velocity (equal to original norm) with random rotation\n",
    "    perturbed_rot_pose = GaussianVMFPoseUntraced()(jnp.eye(4), *(1e-20, 10000.0))  @ \"perturb\"\n",
    "\n",
    "    vel_xyz_world_perturbed = perturbed_rot_pose[:3,:3] @ vel_xyz_world # without friction\n",
    "\n",
    "    vel_xyz_camera = inverse_cam_pose[:3,:3] @ vel_xyz_world_perturbed\n",
    "\n",
    "    # Step 8: Get velocity update in camera frame\n",
    "    vel = pose_prev.at[:3,3].set(vel_xyz_camera)\n",
    "\n",
    "    # Step 9: Identify next pose\n",
    "    next_pose = pose_prev.at[:3,3].set(pose_prev[:3,3] + vel[:3,3]) # trans only, no rot\n",
    "\n",
    "    # Step 10: Ensure new bottom of object is above floor --> ground collision\n",
    "    next_pose_world = cam_pose @ next_pose\n",
    "    bottom_z,_,center_to_bottom = get_height_bounds(i, next_pose_world)\n",
    "    next_pose = jax.lax.cond(\n",
    "        jnp.less_equal(bottom_z,0),\n",
    "        lambda:inverse_cam_pose @ next_pose_world.at[2,3].set(center_to_bottom),\n",
    "        lambda:next_pose\n",
    "    )\n",
    "\n",
    "    return next_pose\n",
    "\n",
    "def threedp3_likelihood_arijit(\n",
    "    observed_xyz: jnp.ndarray,\n",
    "    rendered_xyz: jnp.ndarray,\n",
    "    variance,\n",
    "    outlier_prob,\n",
    "):\n",
    "    distances = jnp.linalg.norm(observed_xyz - rendered_xyz, axis=-1)\n",
    "    probabilities_per_pixel = (distances < variance/2) / variance\n",
    "    average_probability = 1 * probabilities_per_pixel.mean()\n",
    "    return average_probability\n",
    "\n",
    "threedp3_likelihood_arijit_vmap = jax.vmap(threedp3_likelihood_arijit, in_axes=(None,0,None,None))\n",
    "threedp3_likelihood_arijit_double_vmap = jax.vmap(threedp3_likelihood_arijit, in_axes=(0,0,None,None))\n",
    "\n",
    "def outlier_gaussian(\n",
    "    observed_xyz: jnp.ndarray,\n",
    "    rendered_xyz: jnp.ndarray,\n",
    "    variance,\n",
    "    outlier_prob,\n",
    "):\n",
    "    distances = jnp.linalg.norm(observed_xyz - rendered_xyz, axis=-1)\n",
    "    probabilities_per_pixel = jax.scipy.stats.norm.pdf(\n",
    "        distances,\n",
    "        loc=0.0, \n",
    "        scale=variance\n",
    "    )\n",
    "    average_probability = 0.01 * probabilities_per_pixel.sum()\n",
    "    return average_probability\n",
    "\n",
    "outlier_gaussian_double_vmap = jax.vmap(outlier_gaussian, in_axes=(0,0,None,None))\n",
    "outlier_gaussian_vmap = jax.vmap(outlier_gaussian, in_axes=(None,0,None,None))\n",
    "\n",
    "def outlier_gaussian_per_pixel(\n",
    "    observed_xyz: jnp.ndarray,\n",
    "    rendered_xyz: jnp.ndarray,\n",
    "    variance,\n",
    "    outlier_prob,\n",
    "):\n",
    "    distances = jnp.linalg.norm(observed_xyz - rendered_xyz, axis=-1)\n",
    "    probabilities_per_pixel = jax.scipy.stats.norm.pdf(\n",
    "        distances,\n",
    "        loc=0.0, \n",
    "        scale=variance\n",
    "    )\n",
    "    return 0.01 * probabilities_per_pixel\n",
    "\n",
    "outlier_gaussian_per_pixel_vmap = jax.vmap(outlier_gaussian_per_pixel, in_axes=(None,0,None,None))\n",
    "\n",
    "@dataclass\n",
    "class ImageLikelihoodArijit(ExactDensity):\n",
    "    def sample(self, key, img, variance, outlier_prob):\n",
    "        return img\n",
    "\n",
    "    def logpdf(self, observed_image, latent_image, variance, outlier_prob):\n",
    "        # return threedp3_likelihood_arijit(\n",
    "        #     observed_image, latent_image, variance, outlier_prob,\n",
    "        # )        \n",
    "        return outlier_gaussian(\n",
    "            observed_image, latent_image, variance, outlier_prob,\n",
    "        )\n",
    "    \n",
    "@dataclass\n",
    "class GaussianVMFPoseUntraced(ExactDensity):\n",
    "    def sample(self, key, pose_mean, var, concentration, **kwargs):\n",
    "        return b.distributions.gaussian_vmf(key, pose_mean, var, concentration)\n",
    "\n",
    "    def logpdf(self, pose, pose_mean, var, concentration, **kwargs):\n",
    "        return 0\n",
    "\n",
    "@genjax.gen\n",
    "def mcs_model(prev_state, t_inits, t_fulls, init_poses, full_poses, pose_update_params, variance, outlier_prob):\n",
    "    \"\"\"\n",
    "    Single Object Model HMM\n",
    "    \"\"\"\n",
    "\n",
    "    (_, _, poses, all_poses, friction, t, gravity) = prev_state\n",
    "\n",
    "    # jprint(\"t = {}, f = {}\",t, friction)\n",
    "    num_objects = poses.shape[0]\n",
    "    \n",
    "    # for each object\n",
    "    for i in range(num_objects):        \n",
    "\n",
    "        poses = poses.at[i].set(jax.lax.cond(\n",
    "            jnp.equal(t_fulls[i],t), # full pose at the correct time step\n",
    "            lambda:full_poses[i], \n",
    "            lambda:poses[i]))\n",
    "        \n",
    "        poses = poses.at[i].set(jax.lax.cond(\n",
    "            jnp.equal(t_inits[i],t), # init pose at the correct time step\n",
    "            lambda:init_poses[i], \n",
    "            lambda:poses[i]))\n",
    "\n",
    "        physics_prob = jnp.asarray(jax.lax.cond(jnp.greater_equal(t,t_fulls[i]+2),lambda:1,lambda:0), dtype=int)\n",
    "        physics_pose = physics_stepper(all_poses[:,i,...], t, t_fulls[i], i, friction, gravity) @ f\"physics_{i}\"\n",
    "        final_pose, update_params = jax.lax.cond(physics_prob, lambda:(physics_pose, pose_update_params), lambda:(poses[i], (jnp.array([1e20,1e20,1e20]), 0.)))\n",
    "                \n",
    "        updated_pose = b.gaussian_vmf_pose(final_pose, *update_params)  @ f\"pose_{i}\"\n",
    "        poses = poses.at[i].set(updated_pose)\n",
    "        \n",
    "    all_poses = all_poses.at[t].set(poses)\n",
    "    rendered_image_obj = b.RENDERER.render(\n",
    "        poses, jnp.arange(num_objects))[...,:3]\n",
    "\n",
    "    # NOTE: gt_images_bg is a global variable here as it consumes too much memory for the trace\n",
    "    rendered_image = splice_image(rendered_image_obj, gt_images_bg[t])\n",
    "\n",
    "    sampled_image = ImageLikelihoodArijit()(rendered_image, variance, outlier_prob) @ \"depth\"\n",
    "    # sampled_image = b.old_image_likelihood(rendered_image, 0.1, 0.001,1000,None) @ \"depth\"\n",
    "\n",
    "    return (rendered_image, rendered_image_obj, poses, all_poses, friction, t+1, gravity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_update_v5(key, trace_, pose_grid, enumerator):\n",
    "    num_splits = (pose_grid.shape[0] // 400) + 1\n",
    "    all_weights = jnp.array([])\n",
    "    for split_pose_grid in jnp.array_split(pose_grid, num_splits):\n",
    "        weights = enumerator.enumerate_choices_get_scores(trace_, key, split_pose_grid)\n",
    "        all_weights = jnp.hstack([all_weights, weights])\n",
    "    sampled_idx = all_weights.argmax() # jax.random.categorical(key, weights)\n",
    "    # jprint(\"weights = {}\",all_weights)\n",
    "    # jprint(\"weight mix:{}\",jnp.unique(jnp.sort(all_weights), size = 10))\n",
    "    # jprint(\"idx chosen = {}\",sampled_idx)\n",
    "    return *enumerator.update_choices_with_weight(\n",
    "        trace_, key,\n",
    "        pose_grid[sampled_idx]\n",
    "    ), pose_grid[sampled_idx]\n",
    "\n",
    "\n",
    "pose_update_v5_jit = jax.jit(pose_update_v5, static_argnames=(\"enumerator\",))\n",
    "\n",
    "\n",
    "def c2f_pose_update_v5(key, trace_, reference, gridding_schedule, enumerator, obj_id,):\n",
    "    # for each object (TODO: gibbs sampling)\n",
    "    for i in range(len(gridding_schedule)):\n",
    "        updated_grid = jnp.einsum(\"ij,ajk->aik\", reference, gridding_schedule[i])\n",
    "        # Time to check valid poses that dont intersect with the floor\n",
    "        valid = jnp.logical_not(are_bboxes_intersecting_many_jit(\n",
    "                            (100,100,20),\n",
    "                            b.RENDERER.model_box_dims[obj_id],\n",
    "                            jnp.eye(4).at[:3,3].set([0,0,-10.1]),\n",
    "                            jnp.einsum(\"ij,ajk->aik\",cam_pose,updated_grid)\n",
    "                            ))\n",
    "        # if pose is not valid, use the reference pose\n",
    "        valid_grid = jnp.where(valid[:,None,None], updated_grid, reference[None,...])\n",
    "        weight, trace_, reference = pose_update_v5_jit(key, trace_, valid_grid, enumerator)\n",
    "        # jprint(\"ref position is {}\", reference[:3,3])\n",
    "\n",
    "    return weight, trace_\n",
    "\n",
    "c2f_pose_update_v5_vmap_jit = jax.jit(jax.vmap(c2f_pose_update_v5, in_axes=(0,0,None,None,None)),\n",
    "                                    static_argnames=(\"enumerator\", \"obj_id\"))\n",
    "\n",
    "c2f_pose_update_v5_jit = jax.jit(c2f_pose_update_v5,static_argnames=(\"enumerator\", \"obj_id\"))\n",
    "\n",
    "def make_new_keys(key, N_keys):\n",
    "    key, other_key = jax.random.split(key)\n",
    "    new_keys = jax.random.split(other_key, N_keys)\n",
    "    return key, new_keys\n",
    "\n",
    "def update_choice_map_no_unfold(gt_depths, constant_choices, t):\n",
    "    constant_choices['depth'] = gt_depths[t]\n",
    "    return genjax.choice_map(\n",
    "                constant_choices\n",
    "            )\n",
    "\n",
    "def argdiffs_modelv7(trace):\n",
    "    \"\"\"\n",
    "    Argdiffs specific to mcs_single_obejct model with no unfold\n",
    "    \"\"\"\n",
    "    args = trace.get_args()\n",
    "    argdiffs = (\n",
    "        jtu.tree_map(lambda v: Diff(v, UnknownChange), args[0]),\n",
    "        *jtu.tree_map(lambda v: Diff(v, NoChange), args[1:]),\n",
    "    )\n",
    "    return argdiffs\n",
    "\n",
    "def proposal_choice_map_no_unfold(addresses, args, chm_args):\n",
    "    addr = addresses[0] # custom defined\n",
    "    return genjax.choice_map({\n",
    "                        addr: args[0]\n",
    "            })\n",
    "\n",
    "def resampling_priority_fn(particles, all_padded_idxs, t, outlier_variance=0.1):\n",
    "    rendered = particles.get_retval()[0]\n",
    "    padded_idxs = all_padded_idxs[t]\n",
    "    max_rows, _ = padded_idxs.shape\n",
    "\n",
    "    # Create a mask for valid indices (not padded)\n",
    "    valid_mask = padded_idxs[:, 0] != -1  # Assuming -1 is the padding value\n",
    "\n",
    "    # Use the mask to select valid indices, replace invalid indices with a default valid index (e.g., 0)\n",
    "    valid_row_indices = jnp.where(valid_mask, padded_idxs[:, 0], 0)\n",
    "    valid_col_indices = jnp.where(valid_mask, padded_idxs[:, 1], 0)\n",
    "\n",
    "    # Gather rendered and ground truth values\n",
    "    rendered_values_at_indices = rendered[:, valid_row_indices, valid_col_indices, 2]\n",
    "    gt_values_at_indices = gt_images[t, valid_row_indices, valid_col_indices, 2]\n",
    "\n",
    "    # Compute inliers, using the mask to ignore the contributions of invalid indices\n",
    "    inliers = jnp.where(valid_mask, jnp.abs(rendered_values_at_indices - gt_values_at_indices[None, ...]) < outlier_variance, False)\n",
    "    inliers_per_particle = jnp.sum(inliers, axis=1)\n",
    "\n",
    "    log_probs = jnp.log(inliers_per_particle + 1e-9)  # Add a small constant to avoid log(0)\n",
    "\n",
    "    eff_ss = ess(normalize_weights(log_probs))\n",
    "\n",
    "    return eff_ss, log_probs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_approach_G2(model, gt, gridding_schedules, model_args, init_state, key, t_start, constant_choices, T, addr, n_particles):\n",
    "    \"\"\"\n",
    "    Sequential Importance Sampling on the non-unfolded HMM model\n",
    "    with 3D pose enumeration proposal\n",
    "\n",
    "    WITH JUST ONE PARTICLE\n",
    "    \"\"\"\n",
    "    \n",
    "    num_objects = init_state[2].shape[0]\n",
    "\n",
    "    def get_next_state(particle):\n",
    "        return (None,None,*particle.get_retval()[2:])\n",
    "    get_next_state_vmap = jax.vmap(get_next_state, in_axes = (0,))\n",
    "\n",
    "    # sample friction\n",
    "    key, friction_keys = make_new_keys(key, n_particles)\n",
    "    # frictions = jax.vmap(genjax.normal.sample, in_axes = (0,None,None))(friction_keys,*friction_params)\n",
    "    # frictions = jnp.linspace(-0.03,0.07,n_particles)\n",
    "    qs = jnp.linspace(0.05,0.95,n_particles)\n",
    "    frictions = tfp.distributions.Normal(0.02,0.05).quantile(qs)\n",
    "    # frictions = frictions.at[n_particles-1].set(0.5)\n",
    "    gravities = jnp.linspace(0.5,2,n_particles)\n",
    "    # broadcast init_state to number of particles\n",
    "    init_states = jax.vmap(lambda f,g:(*init_state[:4], f, init_state[4], g), in_axes=(0,0))(frictions, gravities)\n",
    "\n",
    "    # define functions for SIS/SMC\n",
    "    init_fn = jax.jit(jax.vmap(model.importance, in_axes=(0,None,0)))\n",
    "    update_fn = jax.jit(model.update)\n",
    "    proposal_fn = c2f_pose_update_v5_jit\n",
    "\n",
    "    def smc_body(carry, t):\n",
    "        # get new keys\n",
    "        print(\"jit compiling\")\n",
    "        # initialize particle based on last time step\n",
    "        jprint(\"t = {}\",t)\n",
    "        \n",
    "        key, log_weights, states,  = carry\n",
    "        key, importance_keys = make_new_keys(key, n_particles)\n",
    "        key, resample_key = jax.random.split(key)\n",
    "        key, proposal_key = jax.random.split(key)\n",
    "\n",
    "        variance = jax.lax.cond(\n",
    "            jnp.less_equal(t, model_args[1][0] + 2),\n",
    "            lambda: 1 * model_args[5],\n",
    "            lambda: model_args[5]\n",
    "        )\n",
    "\n",
    "        modified_model_args = (*model_args[:5], variance, *model_args[6:])\n",
    "\n",
    "        full_args = jax.vmap(lambda x,y:(x, *y), in_axes=(0,None))(states, modified_model_args)\n",
    "\n",
    "        importance_log_weights, particles = init_fn(importance_keys, update_choice_map_no_unfold(gt,constant_choices, t), full_args)\n",
    "\n",
    "        # propose good poses based on proposal\n",
    "        def proposer(carry, p):\n",
    "            key, idx = carry\n",
    "            proposal_log_weight = 0\n",
    "            # argdiff and enumerator\n",
    "            argdiffs = argdiffs_modelv7(p)\n",
    "            enumerators = [b.make_enumerator([(addr + f'_{i}')], \n",
    "                                        chm_builder = proposal_choice_map_no_unfold,\n",
    "                                        argdiff_f=lambda x: argdiffs\n",
    "                                        ) for i in range(num_objects)] \n",
    "            for obj_id in range(num_objects):\n",
    "                key, new_key = jax.random.split(key)\n",
    "                reference = jax.lax.cond(jnp.equal(t,t_start),\n",
    "                                         lambda:model_args[3][obj_id],\n",
    "                                         lambda:states[2][idx][obj_id])\n",
    "                w, p = proposal_fn(new_key, p, reference, gridding_schedules[obj_id], enumerators[obj_id], obj_id)\n",
    "                proposal_log_weight += w\n",
    "            return (new_key, idx + 1), (proposal_log_weight, p)\n",
    "        _, (proposal_log_weights, particles) = jax.lax.scan(proposer, (proposal_key, 0), particles)\n",
    "\n",
    "        eff_ss, priority_fn_log_probs = resampling_priority_fn(particles, padded_all_obj_indices, t)\n",
    "\n",
    "        # jprint(\"t = {}, ess = {}\", t, eff_ss)\n",
    "\n",
    "        # # Resampling when ess is below threshold\n",
    "        indices = jax.lax.cond(eff_ss <= 0.9*n_particles,\n",
    "                               lambda: jax.random.categorical(resample_key, priority_fn_log_probs, shape=(n_particles,)),\n",
    "                               lambda: jnp.arange(n_particles))\n",
    "        particles = jtu.tree_map(lambda v: v[indices], particles)\n",
    "\n",
    "        # get weights of particles\n",
    "        new_log_weight = log_weights + importance_log_weights\n",
    "        next_states = get_next_state_vmap(particles)\n",
    "\n",
    "        return (key, new_log_weight, next_states), (particles, indices)\n",
    "\n",
    "    (_, final_log_weight, _), (particles, indices) = jax.lax.scan(\n",
    "        smc_body, (key, jnp.zeros(n_particles), init_states), jnp.arange(t_start, T))\n",
    "    rendered = particles.get_retval()[0]\n",
    "    rendered_obj = particles.get_retval()[1]\n",
    "    inferred_poses = particles.get_retval()[2]\n",
    "    print(\"SCAN finished\")\n",
    "    return final_log_weight, rendered, rendered_obj, inferred_poses, particles, indices\n",
    "\n",
    "def reset_renderer():\n",
    "    b.RENDERER = None\n",
    "    b.setup_renderer(intrinsics)\n",
    "    for registered_obj in registered_objects:\n",
    "        b.RENDERER.add_mesh(registered_obj['mesh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumeration grid\n",
    "# TODO: ADAPTIVE GRID SIZING\n",
    "# grid_widths = [1, 0.2,0.04,0.008,0.002,0.0004]\n",
    "# # grid_widths = [0.5, 0.1,0.02]\n",
    "# grid_nums = [(7,7,7),(7,7,7),(7,7,7), (7,7,7), (7,7,7), (7,7,7)]\n",
    "# gridding_schedule_trans = make_schedule_translation_3d(grid_widths, grid_nums)\n",
    "# gridding_schedule_rot = [b.utils.make_rotation_grid_enumeration(10, 15, -jnp.pi/12, jnp.pi/12, jnp.pi/12)]\n",
    "# # gridding_schedule = [gridding_schedule_trans[0], gridding_schedule_trans[1], gridding_schedule_trans[2], gridding_schedule_rot[0]]\n",
    "# gridding_schedule = [gridding_schedule_trans[0], gridding_schedule_trans[1], \n",
    "#                      gridding_schedule_trans[2], gridding_schedule_trans[3],\n",
    "#                      gridding_schedule_trans[4], gridding_schedule_trans[5]]\n",
    "\n",
    "\n",
    "gridding_schedules = []\n",
    "for box_dims in b.RENDERER.model_box_dims:\n",
    "    c2fm1 = 2\n",
    "    c2f0 = 1\n",
    "    c2f1 = 0.35 * c2f0\n",
    "    # c2f1 = 0.7 * c2f0\n",
    "    c2f2 = 0.7 * c2f1\n",
    "    c2f3 = 0.2 * c2f2\n",
    "    c2f4 = 0.2 * c2f3\n",
    "    c2f5 = 0.2 * c2f4\n",
    "    c2f6 = 0.2 * c2f5\n",
    "\n",
    "    c2fs = [c2f0,c2f1,c2f2,c2f3,c2f4]#,c2f5,c2f6] #c2fm1\n",
    "    # c2f0 = 1\n",
    "    # c2f1 = 0.15 * c2f0\n",
    "    # c2f2 = 0.05 * c2f1\n",
    "    # c2f3 = 0.05 * c2f2\n",
    "    # c2fs = [c2f0,c2f1,c2f2,c2f3]\n",
    "\n",
    "    x,y,z = box_dims\n",
    "    grid_widths = [[c2f*x, c2f*y, c2f*z] for c2f in c2fs]\n",
    "\n",
    "    grid_nums = [(13,13,13),(7,7,7),(7,7,7),(7,7,7), (7,7,7)]#,(7,7,7),(7,7,7)]\n",
    "    # grid_nums = [(7,7,7),(5,5,5),(5,5,5), (5,5,5), (5,5,5), (3,3,3), (3,3,3)]\n",
    "    # grid_nums = [(5,5,5),(5,5,5),(5,5,5),(5,5,5), (5,5,5), (5,5,5)]#, (5,5,5), (5,5,5)]\n",
    "    # grid_nums = [(15,15,5), (41,5,5), (41,5,5), (41,5,5)]\n",
    "    gridding_schedule_trans = make_schedule_translation_3d_variable_grid(grid_widths, grid_nums)\n",
    "    gridding_schedules.append(gridding_schedule_trans)\n",
    "\n",
    "# Setup for inference\n",
    "T = gt_images.shape[0]\n",
    "num_registered_objects = len(registered_objects)\n",
    "variance = 0.1\n",
    "INIT_STATE = (\n",
    "        None,\n",
    "        None,\n",
    "        jnp.tile(jnp.eye(4).at[2,3].set(1e+5)[None,...],(num_registered_objects,1,1)),\n",
    "        jnp.zeros((T,num_registered_objects,4,4)),\n",
    "        t_start\n",
    ")\n",
    "MODEL_ARGS = (\n",
    "     jnp.array([r['t_init'] for r in registered_objects]),\n",
    "     jnp.array([r['t_full'] for r in registered_objects]),\n",
    "     jnp.array([r['pose'] for r in registered_objects]),\n",
    "     jnp.array([r['full_pose'] for r in registered_objects]),\n",
    "    #  jnp.array([5e-0, 5e-1]),\n",
    "     (jnp.array([1e-0,1e-0,5e-1]), 5e-1),\n",
    "     variance,\n",
    "     None\n",
    ")\n",
    "CONSTANT_CHOICES = {}\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0,2332423432))\n",
    "n_particles = 30\n",
    "model = mcs_model\n",
    "\n",
    "start = time.time()\n",
    "lw, rendered, rendered_obj, inferred_poses, trace, indices = inference_approach_G2(model, gt_images, \n",
    "    gridding_schedules, MODEL_ARGS, INIT_STATE, key, t_start, CONSTANT_CHOICES, T, \"pose\", n_particles)\n",
    "print (\"FPS:\", rendered.shape[0] / (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = trace.score\n",
    "rend_ll = trace.project(genjax.select((\"depth\")))\n",
    "phy_ll = trace.project(genjax.select((\"pose_0\")))\n",
    "worst_rend = outlier_gaussian_double_vmap(gt_images[t_start:], gt_images_bg[t_start:], variance,None)\n",
    "\n",
    "w = trace.project(genjax.select(\"depth\"))\n",
    "offst = 3\n",
    "start = t_start +offst\n",
    "gap = w[offst:].max()-w[offst:].min()\n",
    "\n",
    "max_rend_ll = gt_images.shape[1] * gt_images.shape[2]*jax.scipy.stats.norm.pdf(\n",
    "        0.,\n",
    "        loc=0.0, \n",
    "        scale=variance\n",
    "    ) * 0.01\n",
    "\n",
    "rendering_ll_images = []\n",
    "\n",
    "fig, ax = plt.subplots()  # Using subplots to directly access the figure object\n",
    "lines = []\n",
    "for p_id in range(n_particles):\n",
    "    line = ax.plot(np.array([start]),w[offst,p_id], label = f\"Particle {p_id+1}\")[0]\n",
    "    lines.append(line)\n",
    "line = ax.plot(np.array([start]),worst_rend[offst], label = \"Worst\", linestyle = \"--\")[0]\n",
    "lines.append(line)\n",
    "ax.set_xlim([start,T])\n",
    "ax.set_ylim([worst_rend[offst:].min(),max_rend_ll + 0.1*(max_rend_ll - worst_rend[offst:].min())])\n",
    "# ax.set_ylim([w[offst:].min()-0.1*gap,w[offst:].max()+0.1*gap])\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Log Likelihood\")\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "fig.subplots_adjust(right=0.75)\n",
    "fig.canvas.draw()\n",
    "rendering_ll_img = b.pil_image_from_matplotlib(fig)\n",
    "\n",
    "for _ in tqdm(range(0,start)):\n",
    "    rendering_ll_images.append(rendering_ll_img.copy().resize((600,400)))\n",
    "\n",
    "for t in tqdm(range(start,T)):\n",
    "    for p_id in range(n_particles):\n",
    "        lines[p_id].set_data(np.arange(start,t+1),w[:,p_id][offst:offst+t+1-start])\n",
    "    lines[-1].set_data(np.arange(start,t+1),worst_rend[offst:offst+t+1-start])\n",
    "    fig.canvas.draw()\n",
    "    rendering_ll_img = b.pil_image_from_matplotlib(fig)\n",
    "    rendering_ll_images.append(rendering_ll_img.resize((600,400)))\n",
    "    plt.close()\n",
    "\n",
    "w = trace.project(genjax.select(\"pose_0\"))\n",
    "offst = 3\n",
    "start = t_start+offst\n",
    "gap = w[offst:].max()-w[offst:].min()\n",
    "\n",
    "physics_ll_images = []\n",
    "\n",
    "fig, ax = plt.subplots()  # Using subplots to directly access the figure object\n",
    "lines = []\n",
    "for p_id in range(n_particles):\n",
    "    line = ax.plot(np.array([start]),w[offst,p_id], label = f\"Particle {p_id+1}\")[0]\n",
    "    lines.append(line)\n",
    "    \n",
    "ax.set_xlim([start,T]) \n",
    "ax.set_ylim([-4.66,-4.57])\n",
    "# ax.set_ylim([w[offst:].min()-0.1*gap, w[offst:].max()+0.1*gap])\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Log Likelihood\")\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "fig.subplots_adjust(right=0.75)\n",
    "fig.canvas.draw()\n",
    "physics_ll_img = b.pil_image_from_matplotlib(fig)\n",
    "\n",
    "for _ in tqdm(range(0,start)):\n",
    "    physics_ll_images.append(physics_ll_img.copy().resize((600,400)))\n",
    "\n",
    "for t in tqdm(range(start,T)):\n",
    "    for p_id in range(n_particles):\n",
    "        lines[p_id].set_data(np.arange(start,t+1),w[:,p_id][offst:offst+t+1-start])\n",
    "    fig.canvas.draw()\n",
    "    physics_ll_img = b.pil_image_from_matplotlib(fig)\n",
    "    physics_ll_images.append(physics_ll_img.resize((600,400)))\n",
    "    plt.close()\n",
    "\n",
    "dummy_poses = np.tile(jnp.eye(4).at[2,3].set(-1e5)[None,None,None,...], (t_start,n_particles,num_registered_objects,1,1))\n",
    "concat_inferred_poses = np.concatenate([dummy_poses, inferred_poses])\n",
    "\n",
    "p_images = get_particle_images(intrinsics_orig, concat_inferred_poses, T = T)\n",
    "blended_images = [b.overlay_image(p_images[i],b.get_depth_image(gt_images_orig[i][...,2])) for i in range(len(p_images))]\n",
    "images = []\n",
    "for t in tqdm(range(T)):\n",
    "    images.append(b.scale_image(b.multi_panel([\n",
    "                b.get_depth_image(gt_images_orig[t,...,2]),\n",
    "                # b.scale_image(b.get_depth_image(rendered[t,particle_id,...,2]),scale),\n",
    "                blended_images[t],\n",
    "                physics_ll_images[t],\n",
    "                rendering_ll_images[t]\n",
    "                # b.scale_image(b.get_depth_image(rendered_obj[t,particle_id,...,2]),3)\n",
    "                ],labels = ['gt/observed', 'particles',\n",
    "                            \"physics likelihood\", \"rendering likelihood\"]), 0.4))\n",
    "display_video(images, framerate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = phy_ll[3:]\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(15, 12))  # Adjust figsize as needed\n",
    "\n",
    "# Set global min and max for y-axis\n",
    "ymin, ymax = np.min(data), np.max(data)\n",
    "\n",
    "for i in range(30):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.plot(data[3:, i])\n",
    "    ax.set_ylim([-4.66, -4.57])  # Set the same y-axis limits for all plots\n",
    "    ax.set_title(f\"Plot {i+1}\")\n",
    "    # ax.set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data = rend_ll[3:]\n",
    "\n",
    "fig, axes = plt.subplots(6, 5, figsize=(15, 12))  # Adjust figsize as needed\n",
    "\n",
    "# Set global min and max for y-axis\n",
    "ymin, ymax = np.min(data), np.max(data)\n",
    "\n",
    "for i in range(30):\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.plot(data[3:, i])\n",
    "    ax.set_ylim([94.75, 95.75])  # Set the same y-axis limits for all plots\n",
    "    ax.set_title(f\"Plot {i+1}\")\n",
    "    # ax.set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"rend_ll\":rend_ll, \"phy_ll\":phy_ll, \"all_obj_indices\" :all_obj_indices,\n",
    "        \"inferred_poses\" : concat_inferred_poses,\n",
    "        \"resampled_indices\" : indices, \"heuristic_poses\" : poses, \"worst_rend\":worst_rend,\n",
    "        \"intrinsics\" : intrinsics, \"variance\" : variance}\n",
    "\n",
    "plausible, t_violation, plausibility_list, _ = determine_plausibility(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {}\n",
    "for i,plausibility in enumerate(plausibility_list):\n",
    "    report[i+1] = {\n",
    "        \"rating\": int(plausibility),\n",
    "        \"score\" : float(plausibility),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = {\"rating\": int(plausible), \"score\" : float(plausible), \"report\" : report }\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = data['resampled_indices'].shape[1]\n",
    "resample_bools = np.all(data['resampled_indices'] == np.arange(n_particles), axis = 1)\n",
    "base_indices = np.arange(n_particles)\n",
    "for i in range(data['resampled_indices'].shape[0]):\n",
    "    base_indices = base_indices[data['resampled_indices'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"phy_ll\"][t_violation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_image_double_vmap = jax.vmap(splice_image, in_axes = (0,0))\n",
    "rend_scores = []\n",
    "for p_id in tqdm(range(n_particles)):\n",
    "    # new_rendered_unspliced = b.RENDERER.render_many(inferred_poses[:,p_id,...], jnp.arange(num_registered_objects))[...,:3]\n",
    "    # new_rendered = splice_image_double_vmap(new_rendered_unspliced, gt_images_bg[t_start:])\n",
    "    scores = outlier_gaussian_double_vmap(gt_images[t_start:], gt_images_bg[t_start:], 0.1,None)\n",
    "    rend_scores.append(np.array(scores))\n",
    "rend_scores = np.stack(rend_scores).T\n",
    "rend_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_rend = outlier_gaussian_double_vmap(gt_images[t_start:], gt_images_bg[t_start:], 0.1,None)\n",
    "min_rend = worst_rend.min()\n",
    "for i in range(n_particles):\n",
    "    plt.plot(rend_ll[3:,i])\n",
    "plt.plot(worst_rend[3:], linestyle=\"--\")\n",
    "plt.ylim([min_rend, 95.75]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ registered_objects[0]['full_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ concat_inferred_poses[95,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.RENDERER.model_box_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnge = range(t_start,t_start+40)\n",
    "\n",
    "for p_id in range(n_particles):\n",
    "    if p_id > 28:\n",
    "        xs = np.array([(cam_pose @ concat_inferred_poses[i,p_id,0])[0,3] for i in rnge])\n",
    "        ys = np.array([(cam_pose @ concat_inferred_poses[i,p_id,0])[1,3] for i in rnge])\n",
    "        plt.plot(xs,ys, marker = 'o')\n",
    "        # plt.scatter(xs[-1],ys[-1])\n",
    "\n",
    "nxs = []\n",
    "nys = []\n",
    "for i in rnge:\n",
    "    if len(poses[i]) > 0:\n",
    "        nxs.append((cam_pose @ poses[i][0])[0,3])\n",
    "        nys.append((cam_pose @ poses[i][0])[1,3])\n",
    "# xs = [(cam_pose @ poses[i][0])[0,3] for i in range(134,141)]\n",
    "# ys = [(cam_pose @ poses[i][0])[1,3] for i in range(134,141)]\n",
    "plt.plot(nxs,nys, marker = 'x')\n",
    "# plt.ylim(ys.min(),ys.max())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.RENDERER.model_box_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [(cam_pose @ poses[i][0])[0,3] for i in range(134,141)]\n",
    "ys = [(cam_pose @ poses[i][0])[1,3] for i in range(134,141)]\n",
    "plt.plot(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 204\n",
    "display(rend_ll[tt-134])\n",
    "display(phy_ll[tt-134])\n",
    "display(scs[tt-134])\n",
    "display(rend_ll[tt-134] + phy_ll[tt-134])\n",
    "concat_inferred_poses[tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.scale_image(b.get_depth_image(imm[...,2]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.overlay_image(b.scale_image(b.get_depth_image(imm_unspliced[...,2]),5), b.scale_image(b.get_depth_image(imm[...,2]),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ concat_inferred_poses[134][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ concat_inferred_poses[141][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ debug_physics_stepper(jnp.array(concat_inferred_poses[:,0,0,...]), 142, 134, 0, -0.03, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedp3_likelihood_arijit(gt_images_bg[-1],gt_images[-1],0.1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_images[-1,all_obj_indices[-1][:,0],all_obj_indices[-1][:,1],2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered[-1,0,all_obj_indices[-1][:,0],all_obj_indices[-1][:,1],2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obj_indices[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered[-1,0][25:31,30:40,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_images[-1][25:31,30:40,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.linalg.norm(gt_images[-1][20:35,20:50,:] - rendered[-1,0][20:35,20:50,:], axis=-1) < 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.sum(jnp.linalg.norm(gt_images[123] - gt_images_bg[123], axis=-1) < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def splice_image(rendered_object_image, obs_image_complement, far=150.0):\n",
    "    keep_masks = jnp.logical_or(\n",
    "        jnp.logical_and((rendered_object_image[...,2] <= obs_image_complement[..., 2]) * \n",
    "        rendered_object_image[...,2] > 0.0, (obs_image_complement[...,2] >= far))\n",
    "        ,\n",
    "        (obs_image_complement[...,2] == 0)\n",
    "    )[...,None]\n",
    "    rendered_images = keep_masks * rendered_object_image + (1.0 - keep_masks) * obs_image_complement\n",
    "    return rendered_images, keep_masks\n",
    "imm_unspliced = b.RENDERER.render(registered_objects[0]['full_pose'][None,...], jnp.array([0]))[...,:3]\n",
    "imm,k = splice_image(imm_unspliced, gt_images_bg[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ concat_inferred_poses[123][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pose @ registered_objects[0]['full_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridding_schedules = []\n",
    "for box_dims in b.RENDERER.model_box_dims:\n",
    "    c2fm1 = 2\n",
    "    c2f0 = 1\n",
    "    c2f1 = 0.35 * c2f0\n",
    "    # c2f1 = 0.7 * c2f0\n",
    "    c2f2 = 0.7 * c2f1\n",
    "    c2f3 = 0.2 * c2f2\n",
    "    c2f4 = 0.2 * c2f3\n",
    "    c2f5 = 0.2 * c2f4\n",
    "    c2f6 = 0.2 * c2f5\n",
    "\n",
    "    c2fs = [c2f0,c2f1,c2f2,c2f3,c2f4]#,c2f5,c2f6] #c2fm1\n",
    "    # c2f0 = 1\n",
    "    # c2f1 = 0.15 * c2f0\n",
    "    # c2f2 = 0.05 * c2f1\n",
    "    # c2f3 = 0.05 * c2f2\n",
    "    # c2fs = [c2f0,c2f1,c2f2,c2f3]\n",
    "\n",
    "    x,y,z = box_dims\n",
    "    grid_widths = [[c2f*x, c2f*y, c2f*z] for c2f in c2fs]\n",
    "\n",
    "    grid_nums = [(13,13,7),(7,7,7),(7,7,7),(7,7,7), (7,7,7)]#,(7,7,7),(7,7,7)]\n",
    "    # grid_nums = [(7,7,7),(5,5,5),(5,5,5), (5,5,5), (5,5,5), (3,3,3), (3,3,3)]\n",
    "    # grid_nums = [(5,5,5),(5,5,5),(5,5,5),(5,5,5), (5,5,5), (5,5,5)]#, (5,5,5), (5,5,5)]\n",
    "    # grid_nums = [(15,15,5), (41,5,5), (41,5,5), (41,5,5)]\n",
    "    gridding_schedule_trans = make_schedule_translation_3d_variable_grid(grid_widths, grid_nums)\n",
    "    gridding_schedules.append(gridding_schedule_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_poses = np.tile(jnp.eye(4).at[2,3].set(-1e5)[None,None,None,...], (t_start,n_particles,num_registered_objects,1,1))\n",
    "concat_inferred_poses = np.concatenate([dummy_poses, inferred_poses])\n",
    "\n",
    "# test for bad init\n",
    "tt = 123 # registered_objects[0]['t_full'] + 2\n",
    "reference = concat_inferred_poses[tt][0,0]\n",
    "print(\"original \",(cam_pose @ reference)[:3,3])\n",
    "next_t_step = tt+1\n",
    "obs = gt_images[next_t_step]\n",
    "c2f_imgs = []\n",
    "\n",
    "\n",
    "orig_img = b.RENDERER.render(reference[None,...], jnp.array([0]))[...,:3]\n",
    "xxx_orig = splice_image(orig_img, gt_images_bg[next_t_step])\n",
    "c2f_imgs.append(b.scale_image(b.get_depth_image(xxx_orig[...,2]),5))\n",
    "\n",
    "for i in range(len(gridding_schedules[0])):\n",
    "    updated_grid = jnp.einsum(\"ij,ajk->aik\", reference, gridding_schedules[0][i])\n",
    "\n",
    "    valid = jnp.logical_not(are_bboxes_intersecting_many_jit(\n",
    "                        (100,100,20),\n",
    "                        b.RENDERER.model_box_dims[0],\n",
    "                        jnp.eye(4).at[:3,3].set([0,0,-10.1]),\n",
    "                        jnp.einsum(\"ij,ajk->aik\",cam_pose,updated_grid)\n",
    "                        ))\n",
    "    \n",
    "    # if pose is not valid, use the reference pose\n",
    "    updated_grid = jnp.where(valid[:,None,None], updated_grid, reference[None,...])\n",
    "\n",
    "\n",
    "\n",
    "    imgs = b.RENDERER.render_many(updated_grid[:,None,...], jnp.array([0]))[...,:3]\n",
    "    rendered_images = splice_image_vmap(imgs, gt_images_bg[next_t_step])\n",
    "    scores = outlier_gaussian_vmap(obs, rendered_images, 0.1,None)\n",
    "    # print(scores)\n",
    "    idx = scores.argmax()\n",
    "    reference = updated_grid[idx]\n",
    "    print((cam_pose @ reference)[:3,3])\n",
    "    c2f_imgs.append(b.scale_image(b.get_depth_image(rendered_images[idx,...,2]),5))\n",
    "    print(scores.argmax())\n",
    "for x in c2f_imgs:\n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.RENDERER.model_box_dims/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.array([True]) * jnp.array([False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended_images[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gt_images.shape[1] * gt_images.shape[2]*jax.scipy.stats.norm.pdf(\n",
    "        0,\n",
    "        loc=0.0, \n",
    "        scale=variance\n",
    "    ) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high memory usage !!!####\n",
    "\n",
    "# b.setup_renderer(intrinsics_orig, num_layers= 1024)\n",
    "# for i,registered_obj in enumerate(registered_objects):\n",
    "#     b.RENDERER.add_mesh(registered_obj['mesh'])\n",
    "# if len(registered_objects) == 0:\n",
    "#     b.RENDERER.add_mesh_from_file(os.path.join(b.utils.get_assets_dir(),\"sample_objs/cube.obj\"), scaling_factor = 0.1)\n",
    "\n",
    "# splice_image_double_vmap = jax.vmap(splice_image, in_axes = (0,0))\n",
    "# rend_scores = []\n",
    "# for p_id in tqdm(range(n_particles)):\n",
    "#     new_rendered_unspliced = b.RENDERER.render_many(inferred_poses[:,p_id,...], jnp.arange(num_registered_objects))[...,:3]\n",
    "#     new_rendered = splice_image_double_vmap(new_rendered_unspliced, gt_images_bg_orig[t_start:])\n",
    "#     scores = threedp3_likelihood_arijit_double_vmap(gt_images_orig[t_start:], new_rendered, 0.1,None)\n",
    "#     rend_scores.append(np.array(scores))\n",
    "# rend_scores = np.stack(rend_scores).T\n",
    "# rend_scores.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
