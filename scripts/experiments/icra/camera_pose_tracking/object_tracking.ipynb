{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e0ef8-4473-411e-8908-86620d224bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import bayes3d as b\n",
    "import time\n",
    "from PIL import Image\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import trimesh\n",
    "import os\n",
    "\n",
    "# Can be helpful for debugging:\n",
    "# jax.config.update('jax_enable_checks', True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ab786-2d80-49c3-83a7-ae264401fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.setup_visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f17b0-7aa8-4ea7-8123-a85ca6f3ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pose = b.t3d.transform_from_pos_target_up(\n",
    "    jnp.array([0.0, 1.5, 1.5]),\n",
    "    jnp.array([0.0, 0.0, 0.0]),\n",
    "    jnp.array([0.0, 0.0, 1.0]),\n",
    ")\n",
    "\n",
    "camera_poses = jnp.array([\n",
    "    b.t3d.transform_from_axis_angle(jnp.array([0.0, 0.0, 1.0]), angle) @ camera_pose\n",
    "    for angle in jnp.linspace(0, 2*jnp.pi, 120)]\n",
    ")\n",
    "\n",
    "poses = jnp.linalg.inv(camera_poses)\n",
    "\n",
    "translation_deltas = b.utils.make_translation_grid_enumeration(-0.2, -0.2, -0.2, 0.2, 0.2, 0.2, 11, 11, 11)\n",
    "rotation_deltas = jax.vmap(lambda key: b.distributions.gaussian_vmf_zero_mean(key, 0.00001, 800.0))(\n",
    "    jax.random.split(jax.random.PRNGKey(3), 500)\n",
    ")\n",
    "\n",
    "likelihood = jax.vmap(b.threedp3_likelihood_old, in_axes=(None, 0, None, None, None, None, None))\n",
    "\n",
    "def update_pose_estimate(pose_estimate, gt_image):\n",
    "    proposals = jnp.einsum(\"ij,ajk->aik\", pose_estimate, translation_deltas)\n",
    "    rendered_images = jax.vmap(b.RENDERER.render, in_axes=(0, None))(proposals[:,None, ...], jnp.array([0]))\n",
    "    weights_new = likelihood(gt_image, rendered_images, 0.05, 0.1, 10**3, 0.1, 3)\n",
    "    pose_estimate = proposals[jnp.argmax(weights_new)]\n",
    "\n",
    "    proposals = jnp.einsum(\"ij,ajk->aik\", pose_estimate, rotation_deltas)\n",
    "    rendered_images = jax.vmap(b.RENDERER.render, in_axes=(0, None))(proposals[:, None, ...], jnp.array([0]))\n",
    "    weights_new = likelihood(gt_image, rendered_images, 0.05, 0.1, 10**3, 0.1, 3)\n",
    "    pose_estimate = proposals[jnp.argmax(weights_new)]\n",
    "    return pose_estimate, pose_estimate\n",
    "\n",
    "inference_program = jax.jit(lambda p,x: jax.lax.scan(update_pose_estimate, p,x)[1])\n",
    "\n",
    "original_intrinsics = b.Intrinsics(\n",
    "    height=200,\n",
    "    width=200,\n",
    "    fx=150.0, fy=150.0,\n",
    "    cx=100.0, cy=100.0,\n",
    "    near=0.001, far=6.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fc4ed-0fe3-413f-9bd8-32f1ef59f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_viz_images = [b.get_depth_image(i[:,:,2]) for i in observed_images]\n",
    "# b.make_gif_from_pil_images(input_viz_images, \"input.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d81b1b-1f22-4b2d-82b6-b7f837575cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.utils.ycb_loader.MODEL_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a162459-efdb-4a96-9ef8-b490dfa9872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factors = [1.0, 0.5, 0.25, 0.125]\n",
    "object_ids = [19]\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584c912-371b-4cec-9344-51d824dca37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for OBJECT_ID_IDX in range(len(object_ids)):\n",
    "    print(OBJECT_ID_IDX)\n",
    "    dataa = []\n",
    "    for SCALING_FACTOR_IDX in range(len(scaling_factors)):\n",
    "        print(SCALING_FACTOR_IDX)\n",
    "        intrinsics = b.scale_camera_parameters(original_intrinsics, scaling_factors[SCALING_FACTOR_IDX])\n",
    "        \n",
    "        b.setup_renderer(intrinsics)\n",
    "        model_dir = os.path.join(b.utils.get_assets_dir(),\"bop/ycbv/models\")\n",
    "        mesh_path = os.path.join(model_dir,\"obj_\" + \"{}\".format(object_ids[OBJECT_ID_IDX] + 1).rjust(6, '0') + \".ply\")\n",
    "        b.RENDERER.add_mesh_from_file(mesh_path, scaling_factor=1.0/100.0)\n",
    "        \n",
    "        observed_images = b.RENDERER.render_many(poses[:,None,...],  jnp.array([0]))\n",
    "        print(\"observed_images.shape\", observed_images.shape)\n",
    "        \n",
    "        inferred_poses = inference_program(poses[0], observed_images)\n",
    "        \n",
    "        start = time.time()\n",
    "        pose_estimates_over_time = inference_program(poses[0], observed_images)\n",
    "        end = time.time()\n",
    "        print (\"Time elapsed:\", end - start)\n",
    "        fps =  poses.shape[0] / (end - start)\n",
    "        print (\"FPS:\", poses.shape[0] / (end - start))\n",
    "        dataa.append((scaling_factors[SCALING_FACTOR_IDX], object_ids[OBJECT_ID_IDX], intrinsics.height, fps, pose_estimates_over_time))\n",
    "        \n",
    "        max_depth = 10.0\n",
    "        rerendered_images = b.RENDERER.render_many(pose_estimates_over_time[:, None, ...], jnp.array([0]))\n",
    "        viz_images = []\n",
    "        for (r, d) in zip(rerendered_images, observed_images):\n",
    "            viz_r = b.viz.scale_image(b.viz.get_depth_image(r[:,:,2]), 5.0)\n",
    "            viz_d = b.viz.scale_image(b.viz.get_depth_image(d[:,:,2]), 5.0)\n",
    "            overlay = b.viz.overlay_image(viz_r,viz_d)\n",
    "            viz_images.append(b.viz.multi_panel(\n",
    "                [\n",
    "                    viz_d, viz_r, overlay\n",
    "                ],\n",
    "                [\"Ground Truth\", \"Inferred Reconstruction\", \"Overlay\"],\n",
    "            ))\n",
    "\n",
    "        b.make_gif_from_pil_images(viz_images, \"demo.gif\")\n",
    "    data.append(dataa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064a4a9-8e67-4b7d-a82c-dba4a1d74701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa09f7-8044-49d1-9502-bf9e4cbb5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_between_poses(pose_1, pose_2):\n",
    "    translation_error = jnp.linalg.norm(pose_1[:3,3] - pose_2[:3,3])\n",
    "    error_rotvec = R.from_matrix((pose_1 @ jnp.linalg.inv(pose_2))[:3,:3]).as_rotvec()\n",
    "    rotation_error = jnp.rad2deg(jnp.linalg.norm(error_rotvec))\n",
    "    return translation_error, rotation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c084cd-7e79-42b7-babd-f6cd926a024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_string = \"\"\n",
    "for OBJECT_ID_IDX in range(len(object_ids)):\n",
    "    for SCALING_FACTOR_IDX in range(len(scaling_factors)-1,-1,-1):\n",
    "        scaling_factor, object_id, resolution, fps, poses_inferred = data[OBJECT_ID_IDX][SCALING_FACTOR_IDX]\n",
    "        errors = jnp.array([error_between_poses(p,t) for (p,t) in zip(poses_inferred, poses)])\n",
    "        print(object_id, \" & \",  resolution, \" & \",f\"{fps:0.3f}\", \" & \", f\"{(float(errors[:,0].mean() * 10.0)):0.3f}\", \" & \", f\"{(float(errors[:,1].mean() * 1.0)):0.3f}\", \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7675a6-815f-4225-8a38-2571aa5cd7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a6c66-d0bf-43a6-9795-3a4804b34522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea2feb-1945-4e8d-854a-fa21c76e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "observed_images = b.RENDERER.render_many(poses[:,None,...],  jnp.array([0]))\n",
    "print(\"observed_images.shape\", observed_images.shape)\n",
    "\n",
    "inferred_poses = inference_program(poses[0], observed_images)\n",
    "\n",
    "start = time.time()\n",
    "pose_estimates_over_time = inference_program(poses[0], observed_images)\n",
    "end = time.time()\n",
    "print (\"Time elapsed:\", end - start)\n",
    "print (\"FPS:\", poses.shape[0] / (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31949fbe-add9-480f-9d00-e00a5753da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_depth_image(observed_images[0][:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b71a47-468a-4e3e-baa4-0a0e1e6f6efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801f265-436d-4b12-9bc1-8fcfbb20bd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ea691-0e0f-4980-bf9e-6be777624e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83c091-2507-4c3f-9e95-51a98f90473e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
