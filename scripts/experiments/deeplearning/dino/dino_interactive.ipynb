{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ba743-edd2-48cc-a74d-e1a2138466c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc6924-b057-4425-be33-5fe0608c5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers[torch]\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595be7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bayes3d.neural.dino import Dino\n",
    "import bayes3d as b\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as T\n",
    "import bayes3d.utils.ycb_loader\n",
    "from bayes3d.viz.open3dviz import Open3DVisualizer\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fee45-c8d5-4ba1-b8fd-9555f6e32620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58024f5-a490-4ff8-a850-8dfbb22cdb20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dino = dinov2_vitg14.to(device)  # Same issue with larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024eba14-d79a-4dda-8ba9-835dc932083e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(dinov2_vitg14, rgb):\n",
    "    img = b.get_rgb_image(rgb).convert('RGB')\n",
    "    patch_w, patch_h = np.array(img.size) // 14\n",
    "    transform = T.Compose([\n",
    "        T.GaussianBlur(9, sigma=(0.1, 2.0)),\n",
    "        T.Resize((patch_h * 14, patch_w * 14)),\n",
    "        T.CenterCrop((patch_h * 14, patch_w * 14)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    tensor = transform(img)[:3].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features_dict = dinov2_vitg14.forward_features(tensor)\n",
    "        features = features_dict['x_norm_patchtokens'][0].reshape((patch_h, patch_w, 384)).permute(2, 0, 1).unsqueeze(0)\n",
    "    img_feat_norm = torch.nn.functional.normalize(features, dim=1)\n",
    "    output = jnp.array(img_feat_norm.cpu().detach().numpy())[0]\n",
    "    del img_feat_norm\n",
    "    del features\n",
    "    del tensor\n",
    "    del features_dict\n",
    "    torch.cuda.empty_cache()\n",
    "    return jnp.transpose(output, (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe8ae2-0c5e-429c-a056-c264232c5dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w,h = 1400,1400\n",
    "intrinsics = b.Intrinsics(\n",
    "    height=h,\n",
    "    width=w,\n",
    "    fx=2000.0, fy=2000.0,\n",
    "    cx=w/2.0, cy=h/2.0,\n",
    "    near=0.001, far=6.0\n",
    ")\n",
    "scaled_down_intrinsics = b.camera.scale_camera_parameters(intrinsics, 1.0/14.0)\n",
    "scaled_down_intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57da2bc-b06a-49fd-bde1-92c1d6868c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_renderer(scaled_down_intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d654c9c-259f-425c-8701-e9528e51a289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(b.utils.get_assets_dir(),\"bop/ycbv/models\")\n",
    "mesh_paths = []\n",
    "for idx in range(1,22):\n",
    "    mesh_paths.append(os.path.join(model_dir,\"obj_\" + \"{}\".format(idx).rjust(6, '0') + \".ply\"))\n",
    "SCALING_FACTOR = 1.0/1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5fac5-66c2-4b86-9319-bfefc10ea01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_vmf_parallel = jax.vmap(b.distributions.gaussian_vmf, (0, None, None, None))\n",
    "split_jit = jax.jit(jax.random.split, static_argnums=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05fb32-0f4e-4be9-9693-ca778a17cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Open3DVisualizer(intrinsics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd8d5d-3dc1-4dcf-87e0-3ae8c4c9fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.clear()\n",
    "obj_idx = 2\n",
    "mesh_filename = mesh_paths[obj_idx]\n",
    "mesh = o3d.io.read_triangle_model(mesh_filename)\n",
    "mesh.meshes[0].mesh.scale(SCALING_FACTOR, np.array([0.0, 0.0, 0.0]))\n",
    "viz.render.scene.add_model(f\"1\", mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d15a36-bd5c-43fa-b547-6d71828686c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_poses = jnp.array([b.t3d.inverse_pose(b.t3d.transform_from_pos_target_up(\n",
    "            jnp.array([0.0, 0.6, 0.0]),\n",
    "            jnp.array([0.0, 0.0, 0.0]),\n",
    "            jnp.array([0.0, 0.0, 1.0]),\n",
    "        )) @ b.t3d.transform_from_axis_angle(jnp.array([0.0, 0.0, 1.0]), angle)  for angle in np.linspace(-jnp.pi, jnp.pi, 101)[:-1]])\n",
    "# for (i, pose) in enumerate(object_poses):\n",
    "#     b.show_pose(f\"{i}\", pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d907a0-09bb-4ac2-b535-58ed77a19648",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i, pose in tqdm(enumerate(object_poses)):\n",
    "    # if i > 0:\n",
    "    #     mesh.meshes[0].mesh.transform(b.inv\n",
    "    \n",
    "    # viz.render.scene.add_model(f\"1\", mesh)\n",
    "    rgbd = viz.capture_image(intrinsics, b.t3d.inverse_pose(pose))\n",
    "    images.append(rgbd)\n",
    "\n",
    "# jnp.savez(f\"data_{obj_idx}.npz\",images=images, camera_poses=camera_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebd276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.get_rgb_image(images[0].rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b881e-2fad-4bf5-ae36-99fa4e54a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heatmap(target_embedding, all_embeddings):\n",
    "    dot_products = jnp.einsum(\"i, abi->ab\", target_embedding, all_embeddings)\n",
    "    heatmap = dot_products / 2.0 + 0.5\n",
    "    return heatmap\n",
    "# heatmap = get_heatmap(img1_embedding[45,45], img2_embedding)\n",
    "# scaled_up_heatmap = b.utils.resize(heatmap, img2.shape[0], img2.shape[1])\n",
    "# plt.matshow(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a933cc4-9b31-4534-a88c-342cb112ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bop_ycb_dir = os.path.join(b.utils.get_assets_dir(), \"bop/ycbv\")\n",
    "rgbd, gt_ids, gt_poses, masks = b.utils.ycb_loader.get_test_img('51', '1', bop_ycb_dir)\n",
    "\n",
    "\n",
    "\n",
    "img2 = rgbd.scale_rgbd(3.0).rgb\n",
    "img2_embedding = get_embeddings(dinov2_vitg14, img2)\n",
    "\n",
    "b.get_rgb_image(img2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b36fa-c1b2-4c27-bb76-d8c623948578",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "data = []\n",
    "\n",
    "\n",
    "IDX1 = 70\n",
    "img1 = images[IDX1].rgb\n",
    "img1_embedding = get_embeddings(dinov2_vitg14, img1)\n",
    "img1_resized = images[IDX1].scale_rgbd(1.0/14.0).rgb\n",
    "\n",
    "axes[0].imshow(img1 /255.0)\n",
    "axes[1].imshow(img2 / 255.0)\n",
    "def onclick(event):\n",
    "    print(\"hello\")\n",
    "    # cos = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
    "    x2, y2 = int(np.round(event.xdata / 14.0)), int(np.round(event.ydata / 14.0))\n",
    "    data.append((x,y))\n",
    "\n",
    "    target_embedding = img1_embedding[y2, x2,:]\n",
    "\n",
    "    # dot_products = jnp.einsum(\"i, abi->ab\", target_embedding, img2_embedding)\n",
    "\n",
    "    heatmap = get_heatmap(target_embedding, img2_embedding)\n",
    "    scaled_up_heatmap = b.utils.resize(heatmap, img2.shape[0], img2.shape[1])\n",
    "\n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "    axes[0].imshow(img1 /255.0)\n",
    "    axes[1].imshow(img2 / 255.0)\n",
    "    axes[0].scatter(x, y, c='r', s=10.0)\n",
    "\n",
    "    max_yx = np.unravel_index(scaled_up_heatmap.argmax(), scaled_up_heatmap.shape)\n",
    "    # axes[1].axis('off')\n",
    "    axes[1].imshow(255 * scaled_up_heatmap, alpha=0.45, cmap='viridis')\n",
    "    # axes[1].axis('off')\n",
    "    axes[1].scatter(max_yx[1], max_yx[0], c='r', s=10)\n",
    "    # axes[1].set_title('target image')\n",
    "    # gc.collect()\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47283d6-4464-4757-9495-3672249bf3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2246ea0-b0b9-4ed2-947f-3ad471a4fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 2 * np.pi)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "line, = ax.plot(x, np.sin(x))\n",
    "\n",
    "def update(w = 1.0):\n",
    "    line.set_ydata(np.sin(w * x))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "interact(update);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144508d-93ab-47fe-a3c9-384cfd7c051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = jnp.load(f\"data_{obj_idx}.npz\", allow_pickle=True)\n",
    "# images = data[\"images\"]\n",
    "# camera_poses = data[\"camera_poses\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598cb6ad",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(images)\n",
    "num_training_images = 10\n",
    "training_indices = jnp.arange(0,num_images-1, num_images // num_training_images)\n",
    "# b.hstack_images([\n",
    "#     b.get_rgb_image(images[idx].rgb) for idx in training_indices\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96530e-bf12-4eb0-bfc4-88cdfaafbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoint_coordinates = []\n",
    "keypoint_embeddings = []\n",
    "\n",
    "key = jax.random.PRNGKey(10)\n",
    "sparse_descriptors = []\n",
    "for iteration in range(len(training_indices)):\n",
    "    index = training_indices[iteration]\n",
    "    index_next = training_indices[(iteration+1) % len(training_indices)]\n",
    "    print(index, index_next)\n",
    "    keys = jax.random.split(key)[1]\n",
    "    \n",
    "    training_image = images[index]\n",
    "    object_pose = object_poses[index]\n",
    "    \n",
    "    scaled_down_training_image = training_image.scale_rgbd(1.0/14.0)\n",
    "    embedding_image = get_embeddings(training_image)\n",
    "    embedding_image_next = get_embeddings(images[index_next])\n",
    "    \n",
    "    foreground_mask = (jnp.inf != scaled_down_training_image.depth)\n",
    "    foreground_pixel_coordinates = jnp.transpose(jnp.vstack(jnp.where(foreground_mask)))\n",
    "    \n",
    "    depth = jnp.array(scaled_down_training_image.depth)\n",
    "    depth = depth.at[depth == jnp.inf].set(0.0)\n",
    "    point_cloud_image = b.t3d.unproject_depth(depth, scaled_down_training_image.intrinsics)\n",
    "    point_cloud_image_object_frame = b.t3d.apply_transform(point_cloud_image, b.t3d.inverse_pose(object_pose))\n",
    "    \n",
    "    scaled_down_training_image_next = images[index_next].scale_rgbd(1.0/14.0)\n",
    "    depth = jnp.array(scaled_down_training_image_next.depth)\n",
    "    depth = depth.at[depth == jnp.inf].set(0.0)\n",
    "    point_cloud_image_next = b.t3d.unproject_depth(depth, scaled_down_training_image_next.intrinsics)\n",
    "    point_cloud_image_next_object_frame = b.t3d.apply_transform(point_cloud_image_next, b.t3d.inverse_pose(object_poses[index_next]))\n",
    "    \n",
    "    embeddings_subset = embedding_image[foreground_pixel_coordinates[:,0], foreground_pixel_coordinates[:,1],:]\n",
    "    coordinates_subset = point_cloud_image_object_frame[foreground_pixel_coordinates[:,0], foreground_pixel_coordinates[:,1],:]\n",
    "    similarity_embedding = jnp.einsum(\"abi, ki->abk\", embedding_image_next, embeddings_subset)\n",
    "    best_match = similarity_embedding.argmax(-1)\n",
    "    distance_to_best_match = jnp.linalg.norm(point_cloud_image_next_object_frame - coordinates_subset[best_match,:], axis=-1)\n",
    "    \n",
    "    selected = (distance_to_best_match < 0.01) * (similarity_embedding.max(-1) > 0.9)\n",
    "    subset = jnp.unique(best_match[selected])\n",
    "\n",
    "\n",
    "    _keypoint_embeddings = embedding_image[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "    keypoint_world_coordinates = point_cloud_image[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "    _keypoint_coordinates = b.t3d.apply_transform(keypoint_world_coordinates, b.t3d.inverse_pose(object_pose))\n",
    "\n",
    "    keypoint_coordinates.append(_keypoint_coordinates)\n",
    "    keypoint_embeddings.append(_keypoint_embeddings)\n",
    "    del embedding_image\n",
    "    del embedding_image_next\n",
    "\n",
    "keypoint_coordinates = jnp.concatenate(keypoint_coordinates)\n",
    "keypoint_embeddings = jnp.concatenate(keypoint_embeddings)\n",
    "print(keypoint_coordinates.shape)\n",
    "print(keypoint_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55782678-68e2-49d2-8db4-9462976e567c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keypoint_coordinates = []\n",
    "# keypoint_embeddings = []\n",
    "\n",
    "# key = jax.random.PRNGKey(10)\n",
    "# sparse_descriptors = []\n",
    "\n",
    "# for iteration in range(len(training_indices)):\n",
    "#     index = training_indices[iteration]\n",
    "#     index_next = training_indices[(iteration+1) % len(training_indices)]\n",
    "#     print(index, index_next)\n",
    "#     keys = jax.random.split(key)[1]\n",
    "    \n",
    "#     training_image = images[index]\n",
    "#     object_pose = object_poses[index]\n",
    "    \n",
    "#     scaled_down_training_image = training_image.scale_rgbd(1.0/14.0)\n",
    "#     embedding_image = get_embeddings(training_image)\n",
    "\n",
    "#     # del embeddings\n",
    "#     foreground_mask = (jnp.inf != scaled_down_training_image.depth)\n",
    "#     foreground_pixel_coordinates = jnp.transpose(jnp.vstack(jnp.where(foreground_mask)))\n",
    "\n",
    "#     NUM_KEYPOINTS_TO_SELECT = jnp.min(jnp.array([2000,foreground_pixel_coordinates.shape[0]]))\n",
    "#     subset = jax.random.choice(jax.random.PRNGKey(10),foreground_pixel_coordinates.shape[0], shape=(NUM_KEYPOINTS_TO_SELECT,), replace=False)\n",
    "\n",
    "#     depth = jnp.array(scaled_down_training_image.depth)\n",
    "#     depth = depth.at[depth == jnp.inf].set(0.0)\n",
    "#     point_cloud_image = b.t3d.unproject_depth(depth, scaled_down_training_image.intrinsics)\n",
    "\n",
    "#     keypoint_world_coordinates = point_cloud_image[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "#     _keypoint_coordinates = b.t3d.apply_transform(keypoint_world_coordinates, b.t3d.inverse_pose(object_pose))\n",
    "#     _keypoint_embeddings = embedding_image[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "    \n",
    "#     keypoint_coordinates.append(_keypoint_coordinates)\n",
    "#     keypoint_embeddings.append(_keypoint_embeddings)\n",
    "#     del embedding_image\n",
    "# keypoint_coordinates = jnp.concatenate(keypoint_coordinates)\n",
    "# keypoint_embeddings = jnp.concatenate(keypoint_embeddings)\n",
    "# print(keypoint_coordinates.shape)\n",
    "# print(keypoint_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_embedding_image(pose, keypoint_coordinates, keypoint_embeddings):\n",
    "    point_cloud_img = b.RENDERER.render(pose[None,...], jnp.array([0]))[:,:,:3]\n",
    "    point_cloud_img_in_object_frame = b.t3d.apply_transform(point_cloud_img, b.t3d.inverse_pose(pose))\n",
    "\n",
    "    distances_to_keypoints = (\n",
    "        jnp.linalg.norm(point_cloud_img_in_object_frame[:, :,None,...] - keypoint_coordinates[None, None,:,...],\n",
    "        axis=-1\n",
    "    ))\n",
    "    index_of_nearest_keypoint = distances_to_keypoints.argmin(2)\n",
    "    distance_to_nearest_keypoints = distances_to_keypoints.min(2)\n",
    "\n",
    "    DISTANCE_THRESHOLD = 0.04\n",
    "    valid_match_mask = (distance_to_nearest_keypoints < DISTANCE_THRESHOLD)[...,None]\n",
    "    selected_keypoints = keypoint_coordinates[index_of_nearest_keypoint]\n",
    "    rendered_embeddings_image = keypoint_embeddings[index_of_nearest_keypoint] * valid_match_mask\n",
    "    return point_cloud_img, rendered_embeddings_image\n",
    "\n",
    "vmf_score = lambda q, q_mean, conc: tfp.distributions.VonMisesFisher(\n",
    "    q_mean, conc\n",
    ").log_prob(q)\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "\n",
    "@functools.partial(\n",
    "    jnp.vectorize,\n",
    "    signature='(m),(m)->()',\n",
    "    excluded=(2,),\n",
    ")\n",
    "def vmf_vectorize(\n",
    "    embeddings,\n",
    "    embeddings_mean,\n",
    "    conc\n",
    "):\n",
    "    return vmf_score(embeddings, embeddings_mean, conc)\n",
    "\n",
    "\n",
    "def score_pose(pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings):\n",
    "    _,rendered_embedding_image = render_embedding_image(pose, keypoint_coordinates, keypoint_embeddings)\n",
    "    scores = vmf_vectorize(observed_embeddings, rendered_embedding_image, 1000.0)\n",
    "    return scores\n",
    "\n",
    "def get_pca(embeddings):\n",
    "    features_flat = torch.from_numpy(np.array(embeddings).reshape(-1, embeddings.shape[-1]))\n",
    "    U, S, V = torch.pca_lowrank(features_flat - features_flat.mean(0), niter=10)\n",
    "    proj_PCA = jnp.array(V[:, :3])\n",
    "    return proj_PCA\n",
    "\n",
    "def get_colors(features, proj_V):\n",
    "    features_flat = features.reshape(-1, features.shape[-1])\n",
    "    feat_rgb = features_flat @ proj_V\n",
    "    feat_rgb = (feat_rgb + 1.0) / 2.0\n",
    "    feat_rgb = feat_rgb.reshape(features.shape[:-1] + (3,))\n",
    "    return feat_rgb\n",
    "\n",
    "score_pose_jit = jax.jit(score_pose)\n",
    "score_pose_parallel_jit = jax.jit(jax.vmap(score_pose, in_axes=(0, None, None, None )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4821eef-f7de-498a-9835-753c99087ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import meshcat.geometry as g\n",
    "\n",
    "proj_V = get_pca(keypoint_embeddings)\n",
    "colors = get_colors(keypoint_embeddings, proj_V)\n",
    "b.clear()\n",
    "obj = g.PointCloud(np.transpose(keypoint_coordinates)*10.0, np.transpose(colors), size=0.1)\n",
    "b.meshcatviz.VISUALIZER[\"2\"].set_object(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b319a0d-79bf-4a6c-b771-155b21d035d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_renderer(scaled_down_training_image.intrinsics)\n",
    "b.RENDERER.add_mesh_from_file(mesh_filename, scaling_factor=SCALING_FACTOR)\n",
    "b.setup_renderer(scaled_down_training_image.intrinsics)\n",
    "b.RENDERER.add_mesh_from_file(mesh_filename, scaling_factor=SCALING_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58637e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.RENDERER.render(jnp.eye(4)[None,...], jnp.array([0]));\n",
    "pc_img, rendered_embedding_image = render_embedding_image(object_poses[0], keypoint_coordinates, keypoint_embeddings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb6531-1938-40b3-918b-2e4db3258234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IDX = 15\n",
    "test_rgbd = images[IDX]\n",
    "test_rgbd_scaled = test_rgbd.scale_rgbd(1.0/14.0)\n",
    "observed_embeddings = get_embeddings(test_rgbd)\n",
    "# b.get_rgb_image(test_rgbd.rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1c5d8-898e-419c-9036-6e6e07325b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior = jnp.concatenate([score_pose_parallel_jit(i,  keypoint_coordinates, keypoint_embeddings, observed_embeddings)[:,test_rgbd_scaled.depth != jnp.inf].mean(-1) for i in jnp.array_split(object_poses, 10)])\n",
    "print(posterior.argmax())\n",
    "best_pose = object_poses[posterior.argmax()]\n",
    "print(best_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_poses = []\n",
    "# for IDX in tqdm(range(len(images))):\n",
    "#     test_rgbd = images[IDX]\n",
    "#     test_rgbd_scaled = test_rgbd.scale_rgbd(1.0/14.0)\n",
    "#     observed_embeddings = get_embeddings(test_rgbd)\n",
    "#     posterior = jnp.concatenate([score_pose_parallel_jit(i,  keypoint_coordinates, keypoint_embeddings, observed_embeddings)[:,test_rgbd_scaled.depth != jnp.inf].mean(-1) for i in jnp.array_split(object_poses, 10)])\n",
    "#     print(posterior.argmax())\n",
    "#     best_pose = object_poses[posterior.argmax()]\n",
    "#     print(best_pose)\n",
    "#     predicted_poses.append(best_pose)\n",
    "#     del observed_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd249a6-778c-40ef-a5b1-74bee260be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_embedding_colors = get_colors(observed_embeddings, proj_V)\n",
    "observed_embeddings_image_viz = b.get_rgb_image(observed_embedding_colors * 255.0)\n",
    "\n",
    "pc_img, rendered_embedding_image = render_embedding_image(best_pose, keypoint_coordinates, keypoint_embeddings)\n",
    "colors = get_colors(rendered_embedding_image, proj_V)\n",
    "rgba = jnp.array(b.get_rgb_image(colors * 255.0))\n",
    "# rgba = rgba.at[pc_img[:,:,2] > intrinsics.far - 0.01, :3].set(255.0)\n",
    "rerendered_embeddings_viz = b.get_rgb_image(rgba)\n",
    "\n",
    "b.multi_panel([\n",
    "    b.get_rgb_image(test_rgbd.rgb), \n",
    "    b.scale_image(observed_embeddings_image_viz, 14.0),\n",
    "    b.scale_image(rerendered_embeddings_viz, 14.0)\n",
    "],labels=[\n",
    "    \"Observed RGB\",\n",
    "    \"Embeddings\",\n",
    "    \"Reconstruction\"\n",
    "],label_fontsize=50\n",
    ").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3705f6-58bf-49d2-915b-58957269c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pose = b.transform_from_pos(jnp.array([0.0, 0.0, 0.6])) @ b.distributions.vmf_jit(jax.random.PRNGKey(40), 0.001)\n",
    "test_rgbd = viz.capture_image(intrinsics, b.t3d.inverse_pose(random_pose))\n",
    "test_rgbd_scaled = test_rgbd.scale_rgbd(1.0/14.0)\n",
    "observed_embeddings = get_embeddings(test_rgbd)\n",
    "b.get_rgb_image(test_rgbd.scale_rgbd(0.2).rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca175b-9d2e-42a9-8fd9-420b307fb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_embedding_colors = get_colors(observed_embeddings, proj_V)\n",
    "observed_embeddings_image_viz = b.get_rgb_image(observed_embedding_colors * 255.0)\n",
    "observed_embeddings_image_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dbd24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a23b52-7b2b-4457-977d-c740f51ce15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_scores = jnp.einsum(\"abk,ck\",observed_embeddings, keypoint_embeddings)\n",
    "top_match = match_scores.max(-1)\n",
    "top_match_idx = match_scores.argmax(-1)\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "match_mask = (top_match > THRESHOLD) * (test_rgbd_scaled.depth < test_rgbd_scaled.intrinsics.far)\n",
    "print(match_mask.sum())\n",
    "b.get_depth_image(1.0 * match_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b240d-d87f-49e8-a3bc-69bca58b1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_point_cloud_image = b.unproject_depth_jit(test_rgbd_scaled.depth, test_rgbd_scaled.intrinsics)\n",
    "\n",
    "observed_match_coordinates = observed_point_cloud_image[match_mask,:]\n",
    "model_coordinates = keypoint_coordinates[top_match_idx[match_mask],:]\n",
    "\n",
    "b.clear()\n",
    "b.show_cloud(\"1\", observed_match_coordinates.reshape(-1,3))\n",
    "b.show_cloud(\"2\", model_coordinates.reshape(-1,3), color=b.RED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff97c1-7335-4928-beb0-979262f03634",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.clear()\n",
    "estimated_pose = b.estimate_transform_between_clouds(model_coordinates, observed_match_coordinates)\n",
    "estimated_pose = b.distributions.gaussian_vmf_jit(keys[10],random_pose, 0.1, 10.0)\n",
    "b.show_trimesh(\"mesh\", b.RENDERER.meshes[0])\n",
    "b.set_pose(\"mesh\", estimated_pose)\n",
    "b.show_cloud(\"obs\", observed_point_cloud_image.reshape(-1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ed246",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_pose(random_pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings)[test_rgbd_scaled.depth != jnp.inf].mean(-1))\n",
    "print(score_pose(estimated_pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings)[test_rgbd_scaled.depth != jnp.inf].mean(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c960f5e-2451-4113-a5be-ac60c6a156cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = split_jit(jax.random.PRNGKey(10), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde0448-d0b1-48bf-9e7b-aeac8f4d274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    potential_poses = gaussian_vmf_parallel(keys,estimated_pose, 0.01, 20000.0)\n",
    "    current_score = score_pose(estimated_pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings)[test_rgbd_scaled.depth != jnp.inf].mean(-1)\n",
    "    scores = score_pose_parallel_jit(potential_poses,  keypoint_coordinates, keypoint_embeddings, observed_embeddings)[:,test_rgbd_scaled.depth != jnp.inf].mean(-1)\n",
    "    if scores.max() > current_score:\n",
    "        estimated_pose = potential_poses[scores.argmax()]\n",
    "    keys = split_jit(keys[0], 100)\n",
    "    print(scores.max(), current_score)\n",
    "    b.show_trimesh(\"mesh\", b.RENDERER.meshes[0])\n",
    "    b.set_pose(\"mesh\", estimated_pose)\n",
    "    b.show_cloud(\"obs\", observed_point_cloud_image.reshape(-1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8b244-4417-46f6-afeb-3d37df5d6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_pose(random_pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings)[test_rgbd_scaled.depth != jnp.inf].mean(-1))\n",
    "print(score_pose(estimated_pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings)[test_rgbd_scaled.depth != jnp.inf].mean(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30af6-66ad-4566-85d3-286c73e7d7ed",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d26e3-4ed9-463e-bffb-014d5147e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_embedding_colors = get_colors(observed_embeddings, proj_V)\n",
    "observed_embeddings_image_viz = b.get_rgb_image(observed_embedding_colors * 255.0)\n",
    "\n",
    "pc_img, rendered_embedding_image = render_embedding_image(estimated_pose, keypoint_coordinates, keypoint_embeddings)\n",
    "colors = get_colors(rendered_embedding_image, proj_V)\n",
    "rgba = jnp.array(b.get_rgb_image(colors * 255.0))\n",
    "# rgba = rgba.at[pc_img[:,:,2] > intrinsics.far - 0.01, :3].set(255.0)\n",
    "rerendered_embeddings_viz = b.get_rgb_image(rgba)\n",
    "\n",
    "b.multi_panel([\n",
    "    b.get_rgb_image(test_rgbd.rgb), \n",
    "    b.scale_image(observed_embeddings_image_viz, 14.0),\n",
    "    b.scale_image(rerendered_embeddings_viz, 14.0)\n",
    "],labels=[\n",
    "    \"Observed RGB\",\n",
    "    \"Embeddings\",\n",
    "    \"Reconstruction\"\n",
    "],label_fontsize=50\n",
    ").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50d3ff-f95f-4e94-b102-948a2bc18614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.clear()\n",
    "# b.show_trimesh(\"mesh\", b.RENDERER.meshes[obj_idx])\n",
    "b.show_cloud(\"obs\", observed_point_cloud_image.reshape(-1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289f933-ac74-484d-b3ff-d49e4a2d5009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8fa8d-1c15-4a39-b7db-53d3b6d924b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
