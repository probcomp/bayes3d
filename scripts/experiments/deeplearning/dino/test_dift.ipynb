{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595be7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bayes3d.dino import Dino\n",
    "import bayes3d as b\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as T\n",
    "import bayes3d.ycb_loader\n",
    "import bayes3d.o3d_viz\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2414edd-04aa-4e16-a522-950d0db70877",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade diffusers[torch]\n",
    "!pip install transformers\n",
    "!pip install -U xformers\n",
    "!pip install ipympl\n",
    "!pip install timm\n",
    "!pip install torch==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cacef-d747-4e25-8dbc-a06a340abebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import PILToTensor\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fee45-c8d5-4ba1-b8fd-9555f6e32620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_visualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ccf22-fcc9-4bff-a06f-4401f84b4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        up_ft_indices,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        timestep_cond: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n",
    "            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n",
    "            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n",
    "            cross_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
    "        \"\"\"\n",
    "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
    "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
    "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
    "        # on the fly if necessary.\n",
    "        default_overall_up_factor = 2**self.num_upsamplers\n",
    "\n",
    "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
    "        forward_upsample_size = False\n",
    "        upsample_size = None\n",
    "\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        # prepare attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "            )\n",
    "\n",
    "        # 5. up\n",
    "        up_ft = {}\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "\n",
    "            if i > np.max(up_ft_indices):\n",
    "                break\n",
    "\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # if we have not reached the final block and need to forward the\n",
    "            # upsample size, we do it here\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
    "                )\n",
    "\n",
    "            if i in up_ft_indices:\n",
    "                up_ft[i] = sample.detach()\n",
    "\n",
    "        output = {}\n",
    "        output['up_ft'] = up_ft\n",
    "        return output\n",
    "\n",
    "class OneStepSDPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_tensor,\n",
    "        t,\n",
    "        up_ft_indices,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "\n",
    "        device = self._execution_device\n",
    "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "        unet_output = self.unet(latents_noisy,\n",
    "                               t,\n",
    "                               up_ft_indices,\n",
    "                               encoder_hidden_states=prompt_embeds,\n",
    "                               cross_attention_kwargs=cross_attention_kwargs)\n",
    "        return unet_output\n",
    "\n",
    "\n",
    "class SDFeaturizer:\n",
    "    def __init__(self, sd_id='stabilityai/stable-diffusion-2-1'):\n",
    "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\")\n",
    "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None)\n",
    "        onestep_pipe.vae.decoder = None\n",
    "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\")\n",
    "        gc.collect()\n",
    "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
    "        onestep_pipe.enable_attention_slicing()\n",
    "        onestep_pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe = onestep_pipe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,\n",
    "                img_tensor, # single image, [1,c,h,w]\n",
    "                prompt,\n",
    "                t=261,\n",
    "                up_ft_index=1,\n",
    "                ensemble_size=8):\n",
    "        img_tensor = img_tensor.repeat(ensemble_size, 1, 1, 1).cuda() # ensem, c, h, w\n",
    "        prompt_embeds = self.pipe._encode_prompt(\n",
    "            prompt=prompt,\n",
    "            device='cuda',\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False) # [1, 77, dim]\n",
    "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1)\n",
    "        unet_ft_all = self.pipe(\n",
    "            img_tensor=img_tensor,\n",
    "            t=t,\n",
    "            up_ft_indices=[up_ft_index],\n",
    "            prompt_embeds=prompt_embeds)\n",
    "        unet_ft = unet_ft_all['up_ft'][up_ft_index] # ensem, c, h, w\n",
    "        unet_ft = unet_ft.mean(0, keepdim=True) # 1,c,h,w\n",
    "        return unet_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c114ea-8222-441b-9057-4d7dca10be85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bop_ycb_dir = os.path.join(b.utils.get_assets_dir(), \"bop/ycbv\")\n",
    "rgbd, gt_ids, gt_poses, masks = b.ycb_loader.get_test_img('49', '1', bop_ycb_dir)\n",
    "img1 = b.get_rgb_image(rgbd.rgb)\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d682319",
   "metadata": {},
   "outputs": [],
   "source": [
    "bop_ycb_dir = os.path.join(b.utils.get_assets_dir(), \"bop/ycbv\")\n",
    "rgbd, gt_ids, gt_poses, masks = b.ycb_loader.get_test_img('51', '1', bop_ycb_dir)\n",
    "img2 = b.get_rgb_image(rgbd.rgb)\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fe673-5252-4dd1-b564-18df2a6720d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dift = SDFeaturizer(sd_id='stabilityai/stable-diffusion-2-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85e2af-6614-4d94-9505-a72b7da871c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(img):\n",
    "    img = img.convert('RGB')\n",
    "    img_tensor = (PILToTensor()(img) / 255.0 - 0.5) * 2\n",
    "    output_tensor = dift.forward(img_tensor, prompt=\"object\", ensemble_size=2)\n",
    "    output = jnp.transpose(jnp.array(output_tensor.cpu().detach().numpy())[0], (1,2,0))\n",
    "    del img_tensor\n",
    "    del output_tensor\n",
    "    torch.cuda.empty_cache()\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_embeddings_from_rgbd(rgbd):\n",
    "    img = b.get_rgb_image(rgbd.rgb).convert('RGB')\n",
    "    return get_embeddings(img)\n",
    "\n",
    "\n",
    "embeddings1 = jax.image.resize(get_embeddings(img1), (img1.height, img1.width, embeddings.shape[-1]), \"bilinear\")\n",
    "embeddings2 = jax.image.resize(get_embeddings(img2), (img1.height, img1.width, embeddings.shape[-1]), \"bilinear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo:\n",
    "    def __init__(self, imgs, ft, img_size):\n",
    "        self.ft = ft # NCHW\n",
    "        self.imgs = imgs\n",
    "        self.num_imgs = len(imgs)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def plot_img_pairs(self, fig_size=3, alpha=0.45, scatter_size=70):\n",
    "\n",
    "        fig, axes = plt.subplots(1, self.num_imgs, figsize=(fig_size*self.num_imgs, fig_size))\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        for i in range(self.num_imgs):\n",
    "            axes[i].imshow(self.imgs[i])\n",
    "            axes[i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[i].set_title('source image')\n",
    "            else:\n",
    "                axes[i].set_title('target image')\n",
    "\n",
    "        num_channel = self.ft.size(1)\n",
    "        cos = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "        def onclick(event):\n",
    "            if event.inaxes == axes[0]:\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
    "\n",
    "                    src_ft = self.ft[0].unsqueeze(0)\n",
    "                    src_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(src_ft)\n",
    "                    src_vec = src_ft[0, :, y, x].view(1, num_channel, 1, 1)  # 1, C, 1, 1\n",
    "                    del src_ft\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    trg_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(self.ft[1:])\n",
    "                    cos_map = cos(src_vec, trg_ft).cpu().numpy()  # N, H, W\n",
    "                    \n",
    "                    del trg_ft\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    axes[0].clear()\n",
    "                    axes[0].imshow(self.imgs[0])\n",
    "                    axes[0].axis('off')\n",
    "                    axes[0].scatter(x, y, c='r', s=scatter_size)\n",
    "                    axes[0].set_title('source image')\n",
    "\n",
    "                    for i in range(1, self.num_imgs):\n",
    "                        max_yx = np.unravel_index(cos_map[i-1].argmax(), cos_map[i-1].shape)\n",
    "                        axes[i].clear()\n",
    "\n",
    "                        heatmap = cos_map[i-1]\n",
    "                        heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))  # Normalize to [0, 1]\n",
    "                        axes[i].imshow(self.imgs[i])\n",
    "                        axes[i].imshow(255 * heatmap, alpha=alpha, cmap='viridis')\n",
    "                        axes[i].axis('off')\n",
    "                        axes[i].scatter(max_yx[1].item(), max_yx[0].item(), c='r', s=scatter_size)\n",
    "                        axes[i].set_title('target image')\n",
    "\n",
    "                    del cos_map\n",
    "                    del heatmap\n",
    "                    gc.collect()\n",
    "\n",
    "        fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94316a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d90150-0d54-4256-ad30-7cd0536df029",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor= o.shape[0] / training_images[0].rgb.shape[0]\n",
    "scaled_down_intrinsics = b.camera.scale_camera_parameters(intrinsics, scaling_factor)\n",
    "scaled_down_intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55782678-68e2-49d2-8db4-9462976e567c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images = len(training_images)\n",
    "training_indices = jnp.arange(0,num_images-1, num_images//4)\n",
    "\n",
    "keypoint_coordinates = []\n",
    "keypoint_embeddings = []\n",
    "\n",
    "for idx in tqdm(training_indices):\n",
    "    angle = training_angles[idx]\n",
    "    training_image = training_images[idx]\n",
    "    pose = b.t3d.inverse_pose(b.t3d.transform_from_pos_target_up(\n",
    "        jnp.array([0.0, 0.6, 0.0]),\n",
    "        jnp.array([0.0, 0.0, 0.0]),\n",
    "        jnp.array([0.0, 0.0, 1.0]),\n",
    "    )) @ b.t3d.transform_from_axis_angle(jnp.array([0.0, 0.0, 1.0]), angle)\n",
    "\n",
    "    scaled_down_training_image = b.scale_rgbd(training_image, scaling_factor)\n",
    "    embeddings = get_embeddings(training_image)\n",
    "    # del embeddings\n",
    "    foreground_mask = (jnp.inf != scaled_down_training_image.depth)\n",
    "    foreground_pixel_coordinates = jnp.transpose(jnp.vstack(jnp.where(foreground_mask)))\n",
    "\n",
    "    NUM_KEYPOINTS_TO_SELECT = jnp.min(jnp.array([2000,foreground_pixel_coordinates.shape[0]]))\n",
    "    subset = jax.random.choice(jax.random.PRNGKey(10),foreground_pixel_coordinates.shape[0], shape=(NUM_KEYPOINTS_TO_SELECT,), replace=False)\n",
    "\n",
    "    depth = jnp.array(scaled_down_training_image.depth)\n",
    "    depth = depth.at[depth == jnp.inf].set(0.0)\n",
    "    point_cloud_image = b.t3d.unproject_depth(depth, scaled_down_training_image.intrinsics)\n",
    "\n",
    "    keypoint_world_coordinates = point_cloud_image[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "    _keypoint_coordinates = b.t3d.apply_transform(keypoint_world_coordinates, b.t3d.inverse_pose(pose))\n",
    "    _keypoint_embeddings = embeddings[foreground_pixel_coordinates[subset,0], foreground_pixel_coordinates[subset,1],:]\n",
    "    \n",
    "    keypoint_coordinates.append(_keypoint_coordinates)\n",
    "    keypoint_embeddings.append(_keypoint_embeddings)\n",
    "    del embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4821eef-f7de-498a-9835-753c99087ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keypoint_coordinates = jnp.concatenate(keypoint_coordinates)\n",
    "keypoint_embeddings = jnp.concatenate(keypoint_embeddings)\n",
    "print(keypoint_coordinates.shape)\n",
    "print(keypoint_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b319a0d-79bf-4a6c-b771-155b21d035d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_renderer(scaled_down_training_image.intrinsics)\n",
    "b.RENDERER.add_mesh_from_file(mesh_filename, scaling_factor=SCALING_FACTOR)\n",
    "b.setup_renderer(scaled_down_training_image.intrinsics)\n",
    "b.RENDERER.add_mesh_from_file(mesh_filename, scaling_factor=SCALING_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830046f-f1d5-4e37-9865-842621d68c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def render_embedding_image(pose, keypoint_coordinates, keypoint_embeddings):\n",
    "    point_cloud_img = b.RENDERER.render_single_object(pose, jnp.int32(0))[:,:,:3]\n",
    "    point_cloud_img_in_object_frame = b.t3d.apply_transform(point_cloud_img, b.t3d.inverse_pose(pose))\n",
    "\n",
    "    distances_to_keypoints = (\n",
    "        jnp.linalg.norm(point_cloud_img_in_object_frame[:, :,None,...] - keypoint_coordinates[None, None,:,...],\n",
    "        axis=-1\n",
    "    ))\n",
    "    index_of_nearest_keypoint = distances_to_keypoints.argmin(2)\n",
    "    distance_to_nearest_keypoints = distances_to_keypoints.min(2)\n",
    "\n",
    "    DISTANCE_THRESHOLD = 0.04\n",
    "    valid_match_mask = (distance_to_nearest_keypoints < DISTANCE_THRESHOLD)[...,None]\n",
    "    selected_keypoints = keypoint_coordinates[index_of_nearest_keypoint]\n",
    "    rendered_embeddings_image = keypoint_embeddings[index_of_nearest_keypoint] * valid_match_mask\n",
    "    return point_cloud_img, rendered_embeddings_image\n",
    "\n",
    "def score_pose(pose, keypoint_coordinates, keypoint_embeddings, observed_embeddings):\n",
    "    _,rendered_embedding_image = render_embedding_image(pose, keypoint_coordinates, keypoint_embeddings)\n",
    "    dot_products = jnp.einsum(\"abi,abi->ab\", rendered_embedding_image, observed_embeddings)\n",
    "    return dot_products.mean()\n",
    "\n",
    "def get_pca(embeddings):\n",
    "    features_flat = torch.from_numpy(np.array(embeddings).reshape(-1, embeddings.shape[-1]))\n",
    "    U, S, V = torch.pca_lowrank(features_flat - features_flat.mean(0), niter=10)\n",
    "    proj_PCA = jnp.array(V[:, :3])\n",
    "    return proj_PCA\n",
    "\n",
    "def get_colors(features, proj_V):\n",
    "    features_flat = features.reshape(-1, features.shape[-1])\n",
    "    feat_rgb = features_flat @ proj_V\n",
    "    feat_rgb = (feat_rgb + 1.0) / 2.0\n",
    "    feat_rgb = feat_rgb.reshape(features.shape[:-1] + (3,))\n",
    "    return feat_rgb\n",
    "\n",
    "score_pose_jit = jax.jit(score_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf416e-d6ed-4b2a-a64c-ae0df2fb052f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "angles = jnp.linspace(-jnp.pi, jnp.pi, 300)\n",
    "angle_to_pose = lambda angle : b.t3d.inverse_pose(b.t3d.transform_from_pos_target_up(\n",
    "        jnp.array([0.0, 0.6, 0.0]),\n",
    "        jnp.array([0.0, 0.0, 0.0]),\n",
    "        jnp.array([0.0, 0.0, 1.0]),\n",
    "    )) @ b.t3d.transform_from_axis_angle(jnp.array([0.0, 0.0, 1.0]), angle)\n",
    "scorer = lambda angle, observed_embeddings: score_pose(\n",
    "    angle_to_pose(angle),\n",
    "    keypoint_coordinates, keypoint_embeddings, observed_embeddings\n",
    ")\n",
    "scorer_jit = jax.jit(scorer)\n",
    "scorer_parallel_jit = jax.jit(jax.vmap(scorer, in_axes=(0,None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b4c5b-51df-4d69-a02c-007912b9282a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "proj_V = get_pca(keypoint_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb6531-1938-40b3-918b-2e4db3258234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IDX = 3\n",
    "test_rgbd = training_images[IDX]\n",
    "observed_embeddings = get_embeddings(test_rgbd)\n",
    "training_angles[IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d1c5d8-898e-419c-9036-6e6e07325b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posterior = jnp.concatenate([scorer_parallel_jit(i, observed_embeddings) for i in jnp.array_split(angles, 10)])\n",
    "best_angle = angles[posterior.argmax()]\n",
    "print(best_angle)\n",
    "best_pose = angle_to_pose(best_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13b86e-1f0a-468d-a51f-c44ca53aed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = get_colors(observed_embeddings, proj_V)\n",
    "embedding_image = b.scale_image(b.get_rgb_image(colors * 255.0),14.0)\n",
    "embedding_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee1bce-9a27-4ffa-b805-025a0e77e22d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pc_img, img = render_embedding_image(angle_to_pose(best_angle), keypoint_coordinates, keypoint_embeddings)\n",
    "colors = get_colors(observed_embeddings, proj_V)\n",
    "rgba = jnp.array(b.get_rgb_image(colors * 255.0))\n",
    "rgba = rgba.at[pc_img[:,:,2] > intrinsics.far - 0.01, :3].set(255.0)\n",
    "rerendered_embeddings = b.scale_image(b.get_rgb_image(rgba),14.0)\n",
    "rerendered_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3bb38-273b-4fac-98d9-a61ba6131aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.multi_panel([\n",
    "    b.get_rgb_image(test_rgbd.rgb),\n",
    "    embedding_image,\n",
    "    rerendered_embeddings\n",
    "]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25a065-b3c4-46bb-8a01-5af2710714ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import meshcat.geometry as g\n",
    "\n",
    "proj_V = get_pca(keypoint_embeddings)\n",
    "colors = get_colors(keypoint_embeddings, proj_V)\n",
    "b.clear()\n",
    "obj = g.PointCloud(np.transpose(keypoint_coordinates)*30.0, np.transpose(colors), size=0.1)\n",
    "b.meshcatviz.VISUALIZER[\"2\"].set_object(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22264b20-d514-42fe-89de-6b406f5c457a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.setup_renderer(test_rgbd.intrinsics)\n",
    "# b.RENDERER.add_mesh_from_file(mesh_filename, scaling_factor=SCALING_FACTOR)\n",
    "point_cloud_img = b.RENDERER.render_single_object(best_pose, jnp.int32(0))[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89285af-e1c3-4724-a88e-8180376f1946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = (test_rgbd.intrinsics.near < point_cloud_img[:,:,2]) * (point_cloud_img[:,:,2] < test_rgbd.intrinsics.far)\n",
    "print(point_cloud_img[:,:,2][mask].min(), point_cloud_img[:,:,2][mask].max())\n",
    "b.get_depth_image(1.0 * mask)\n",
    "img = jnp.array(b.get_depth_image(point_cloud_img[:,:,2], min=0.46, max=0.65))\n",
    "img = img.at[jnp.invert(mask) , :3].set(255.0)\n",
    "b.get_rgb_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880d415-e21a-484a-82c5-487e945259b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b.overlay_image(b.get_rgb_image(test_rgbd.rgb), b.get_rgb_image(img), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3524f2-19ff-4398-b4b7-cc5134049a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pose = best_pose\n",
    "point_cloud_img = b.RENDERER.render_single_object(pose, jnp.int32(0))[:,:,:3]\n",
    "point_cloud_img_in_object_frame = b.t3d.apply_transform(point_cloud_img, b.t3d.inverse_pose(pose))\n",
    "\n",
    "distances_to_keypoints = (\n",
    "    jnp.linalg.norm(point_cloud_img_in_object_frame[:, :,None,...] - keypoint_coordinates[None, None,:,...],\n",
    "    axis=-1\n",
    "))\n",
    "index_of_nearest_keypoint = distances_to_keypoints.argmin(2)\n",
    "distance_to_nearest_keypoints = distances_to_keypoints.min(2)\n",
    "\n",
    "DISTANCE_THRESHOLD = 0.2\n",
    "valid_match_mask = (distance_to_nearest_keypoints < DISTANCE_THRESHOLD)[...,None]\n",
    "selected_keypoints = keypoint_coordinates[index_of_nearest_keypoint]\n",
    "rendered_embeddings_image = keypoint_embeddings[index_of_nearest_keypoint] * valid_match_mask\n",
    "\n",
    "colors = get_colors(rendered_embeddings_image, proj_V)\n",
    "b.get_rgb_image(colors * 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7a678-573f-4415-b3d2-dbf93cd4c4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
