{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fef143-f8d4-4078-9386-deeff2fd80c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import bayes3d as b\n",
    "from tqdm import tqdm\n",
    "import pytorch3d.transforms\n",
    "import jax.numpy as jnp\n",
    "import nvdiffrast.torch as dr\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae0d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter           = 10000\n",
    "repeats            = 1\n",
    "log_interval       = 10\n",
    "display_interval   = None\n",
    "display_res        = 512\n",
    "lr_base            = 1e-3\n",
    "lr_falloff         = 1.0\n",
    "nr_base            = 1.0\n",
    "nr_falloff         = 1e-4\n",
    "grad_phase_start   = 0.5\n",
    "resolution         = 128\n",
    "out_dir            = None\n",
    "log_fn             = None\n",
    "mp4save_interval   = None\n",
    "mp4save_fn         = None\n",
    "use_opengl         = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ce3703-36df-4d82-bebc-751bb974c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(x=0.1, n=1.0, f=50.0):\n",
    "    return np.array([[n/x,    0,            0,              0],\n",
    "                     [  0,  n/x,            0,              0],\n",
    "                     [  0,    0, -(f+n)/(f-n), -(2*f*n)/(f-n)],\n",
    "                     [  0,    0,           -1,              0]]).astype(np.float32)\n",
    "def translate(x, y, z):\n",
    "    return np.array([[1, 0, 0, x],\n",
    "                     [0, 1, 0, y],\n",
    "                     [0, 0, 1, z],\n",
    "                     [0, 0, 0, 1]]).astype(np.float32)\n",
    "glctx = dr.RasterizeGLContext() #if use_opengl else dr.RasterizeCudaContext()\n",
    "mvp = torch.tensor(np.matmul(projection(x=0.4), translate(0, 0, 0.0)).astype(np.float32), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fdf559-094d-46e5-ae8b-b75122891ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_to_matrix(poses: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert rotations given as quaternions to rotation matrices.\n",
    "\n",
    "    Args:\n",
    "        quaternions: quaternions with real part first,\n",
    "            as tensor of shape (..., 4).\n",
    "\n",
    "    Returns:\n",
    "        Rotation matrices as tensor of shape (..., 3, 3).\n",
    "    \"\"\"\n",
    "    positions = poses[...,:3]\n",
    "    quaternions = poses[...,3:]\n",
    "    r, i, j, k = torch.unbind(quaternions, -1)\n",
    "    x, y, z = torch.unbind(positions, -1)\n",
    "    # pyre-fixme[58]: `/` is not supported for operand types `float` and `Tensor`.\n",
    "    two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "    o = torch.stack(\n",
    "        (\n",
    "            1 - two_s * (j * j + k * k),\n",
    "            two_s * (i * j - k * r),\n",
    "            two_s * (i * k + j * r),\n",
    "            x,\n",
    "            two_s * (i * j + k * r),\n",
    "            1 - two_s * (i * i + k * k),\n",
    "            two_s * (j * k - i * r),\n",
    "            y,\n",
    "            two_s * (i * k - j * r),\n",
    "            two_s * (j * k + i * r),\n",
    "            1 - two_s * (i * i + j * j),\n",
    "            z,\n",
    "            0.0 * x,\n",
    "            0.0 * x,\n",
    "            0.0 * x,\n",
    "            0.0 * x + 1.0,\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    rotation_matrices = o.reshape(quaternions.shape[:-1] + (4, 4))\n",
    "    return rotation_matrices\n",
    "\n",
    "# Transform vertex positions to clip space\n",
    "def transform_pos(mtx, pos):\n",
    "    t_mtx = torch.from_numpy(mtx).cuda() if isinstance(mtx, np.ndarray) else mtx\n",
    "    # (x,y,z) -> (x,y,z,1)\n",
    "    posw = torch.cat([pos, torch.ones([pos.shape[0], 1]).cuda()], axis=1)\n",
    "    return torch.matmul(posw, t_mtx.t())[None, ...]\n",
    "\n",
    "def render(glctx, mtx, pos, pos_idx, resolution: int):\n",
    "    # Setup TF graph for reference.\n",
    "    depth_ = pos[..., 2:3]\n",
    "    depth = torch.tensor([[[(z_val/1)] for z_val in depth_.squeeze()]], dtype=torch.float32).cuda()\n",
    "    pos_clip    = transform_pos(mtx, pos)\n",
    "    rast_out, _ = dr.rasterize(glctx, pos_clip, pos_idx, resolution=[resolution, resolution])\n",
    "    color   , _ = dr.interpolate(depth, rast_out, pos_idx)\n",
    "    # color       = dr.antialias(color, rast_out, pos_clip, pos_idx)\n",
    "    return color\n",
    "    # return rast_out[:,:,:,2:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fc02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1000, 0.2000],\n",
      "        [0.0000, 0.1000, 0.2000],\n",
      "        [0.0000, 0.1000, 0.2000],\n",
      "        [0.0000, 0.1000, 0.2000],\n",
      "        [0.0000, 0.1000, 0.2000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def posevec_to_matrix(position, quat):\n",
    "    return torch.cat(\n",
    "        (\n",
    "            torch.cat((pytorch3d.transforms.quaternion_to_matrix(quat), position.unsqueeze(1)), 1),\n",
    "            torch.tensor([[0.0, 0.0, 0.0, 1.0]],device=device),\n",
    "        ),\n",
    "        0,\n",
    "    )\n",
    "def apply_transform(points, transform):\n",
    "    rels_ = torch.cat(\n",
    "        (\n",
    "            points,\n",
    "            torch.ones((points.shape[0], 1),  device=device),\n",
    "        ),\n",
    "        1,\n",
    "    )\n",
    "    return torch.einsum(\"ij, aj -> ai\", transform, rels_)[...,:3]\n",
    "position = torch.tensor([0.0, 0.1, 0.2], device=device)\n",
    "quat = torch.tensor([1.0, 0.1, 0.2, 0.3],device=device)\n",
    "points = torch.zeros((5,3), device = device)\n",
    "print(apply_transform(points, posevec_to_matrix(position, quat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9c2c03-8f55-4625-982d-32479eebfa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15728, 3]) torch.Size([16762, 3]) torch.Size([16762, 3]) torch.Size([1, 3])\n",
      "tensor([[-0.4248, -0.2584, -0.3477],\n",
      "        [-0.4334, -0.2500, -0.3395],\n",
      "        [-0.4441, -0.2338, -0.3527],\n",
      "        ...,\n",
      "        [ 0.4904, -0.0145,  0.2256],\n",
      "        [ 0.3056, -0.0512,  0.2582],\n",
      "        [ 0.3417, -0.0483,  0.3824]], device='cuda:0') tensor([[1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(b.utils.get_assets_dir(),\"bop/ycbv/models\")\n",
    "idx = 14\n",
    "mesh_path = os.path.join(model_dir,\"obj_\" + \"{}\".format(idx).rjust(6, '0') + \".ply\")\n",
    "m = b.utils.load_mesh(mesh_path)\n",
    "m = b.utils.scale_mesh(m, 1.0/100.0)\n",
    "\n",
    "# m = b.utils.make_cuboid_mesh(jnp.array([0.5, 0.5, 0.2]))\n",
    "\n",
    "vtx_pos = torch.from_numpy(m.vertices.astype(np.float32)).cuda()\n",
    "pos_idx = torch.from_numpy(m.faces.astype(np.int32)).cuda()\n",
    "col_idx = torch.from_numpy(np.zeros((vtx_pos.shape[0],3)).astype(np.int32)).cuda()\n",
    "vtx_col = torch.from_numpy(np.ones((1,3)).astype(np.float32)).cuda()\n",
    "# print(\"Mesh has %d triangles and %d vertices.\" % (pos_idx.shape[0], pos.shape[0]))\n",
    "print(pos_idx.shape, vtx_pos.shape, col_idx.shape, vtx_col.shape)\n",
    "print(vtx_pos, vtx_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb5415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_wrapper(pos,quat):\n",
    "    rast_target = render(glctx, torch.matmul(mvp, posevec_to_matrix(pos, quat)), vtx_pos, pos_idx,  resolution)\n",
    "    return rast_target\n",
    "\n",
    "def get_viz(rast_target):\n",
    "    img_target  = rast_target[0].detach().cpu().numpy()\n",
    "    viz = b.get_depth_image(img_target[:,:,0]* 255.0)\n",
    "    return viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6f93c7-f7e8-427f-8ea5-365370a9560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_218609/2544377672.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  quat =  torch.tensor(torch.rand(4,device=device) - 0.5,device=device)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.tensor([0.0, 0.0, -2.5],device=device)\n",
    "quat =  torch.tensor(torch.rand(4,device=device) - 0.5,device=device)\n",
    "\n",
    "ground_truth_image = render_wrapper(pos,quat)\n",
    "viz_gt = get_viz(ground_truth_image)\n",
    "viz_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.tensor([0.0, 0.0, -2.2],device=device, requires_grad=True)\n",
    "quat =  torch.tensor(torch.rand(4,device=device) - 0.5,device=device, requires_grad=True)\n",
    "rendered_image = render_wrapper(pos,quat)\n",
    "viz = get_viz(rendered_image)\n",
    "b.hstack_images([viz, viz_gt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5a306-73ca-4981-aca2-1c041ae57f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "    {'params': [pos], 'lr': 0.001, \"name\": \"pos\"},\n",
    "    {'params': [quat], 'lr': 1.0, \"name\": \"quat\"},\n",
    "], lr=0.0, eps=1e-15)\n",
    "print(quat)\n",
    "\n",
    "pbar = tqdm(range(100))\n",
    "for _ in pbar:\n",
    "    rendered_image =  render_wrapper(pos, quat)\n",
    "    loss = torch.abs(ground_truth_image - rendered_image).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(f\"{loss.item()}\")\n",
    "viz = get_viz(rendered_image)\n",
    "print(quat)\n",
    "\n",
    "b.hstack_images([viz, viz_gt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (rast_opt - rast_target)**2 # L2 norm.\n",
    "diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98ecb0-6e55-4280-af4f-6639e5d69f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([pose_opt],  lr=0.00001)\n",
    "images = []\n",
    "\n",
    "for _ in tqdm(range(200)):    \n",
    "    rast_opt = render(glctx, torch.matmul(mvp, quaternion_to_matrix(pose_opt)), vtx_pos, pos_idx,  resolution)\n",
    "\n",
    "    diff = (rast_opt - rast_target)**2 # L2 norm.\n",
    "    loss = torch.mean(diff)\n",
    "    loss_val = float(loss)\n",
    "    \n",
    "    if (loss_val < loss_best) and (loss_val > 0.0):\n",
    "        loss_best = loss_val\n",
    "                \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)    \n",
    "    with torch.no_grad():\n",
    "        pose_opt /= torch.sum(pose_opt**2)**0.5\n",
    "    \n",
    "    img_opt  = rast_opt[0].detach().cpu().numpy()\n",
    "    images.append(\n",
    "        b.hstack_images([\n",
    "            b.get_depth_image(img_opt[:,:,0]* 255.0) ,\n",
    "            b.get_depth_image(img_target[:,:,0]* 255.0) ,\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651d8eb-75c3-4453-b58c-09e46ce2b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.vstack_images([images[0],images[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1f6dc-31e1-4c33-97c3-d13fa8e5f34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921bcb5-4d8b-4b0f-8b9f-f3d70ca43522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f58562-0672-4e28-810b-2e30e7c849e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dc7cd-0aef-44c7-a821-368b512c9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
